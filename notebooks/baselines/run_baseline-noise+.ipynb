{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f35404f-67eb-4c03-9494-3a60b0452c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................Data Overview................................\n",
      "Train Data Shape: (34149, 29)\n",
      "Test Data Shape: (14636, 29)\n",
      "Train Data Labels [0]: [0]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Number of samples after adding noise: 3888\n",
      "Number of features: 28\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model KNN\n",
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 92.22068736244613\n",
      "AUCPR: 97.89870685262869\n",
      "Accuracy: 82.08\n",
      "MCC: 0.5105\n",
      "F1 Score: 0.8887\n",
      "Precision: 0.9830\n",
      "Recall: 0.8110\n",
      "Results saved for data_ToNIoT.csv with model KNN\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model LUNAR\n",
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 94.48575378572211\n",
      "AUCPR: 98.70856809471587\n",
      "Accuracy: 89.59\n",
      "MCC: 0.6406\n",
      "F1 Score: 0.9382\n",
      "Precision: 0.9848\n",
      "Recall: 0.8959\n",
      "Results saved for data_ToNIoT.csv with model LUNAR\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model NeuTraLAD\n",
      "Epoch 10/200, Loss: 0.144473\n",
      "Epoch 20/200, Loss: 0.137093\n",
      "Epoch 30/200, Loss: 0.135592\n",
      "Epoch 40/200, Loss: 0.135027\n",
      "Epoch 50/200, Loss: 0.134747\n",
      "Epoch 60/200, Loss: 0.134554\n",
      "Epoch 70/200, Loss: 0.134454\n",
      "Epoch 80/200, Loss: 0.134370\n",
      "Epoch 90/200, Loss: 0.134326\n",
      "Epoch 100/200, Loss: 0.134321\n",
      "Epoch 110/200, Loss: 0.134261\n",
      "Epoch 120/200, Loss: 0.134231\n",
      "Epoch 130/200, Loss: 0.134221\n",
      "Epoch 140/200, Loss: 0.134197\n",
      "Epoch 150/200, Loss: 0.134188\n",
      "Epoch 160/200, Loss: 0.134226\n",
      "Epoch 170/200, Loss: 0.134184\n",
      "Epoch 180/200, Loss: 0.134177\n",
      "Epoch 190/200, Loss: 0.134182\n",
      "Epoch 200/200, Loss: 0.134133\n",
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 83.53246708304789\n",
      "AUCPR: 96.23401098056299\n",
      "Accuracy: 58.94\n",
      "MCC: 0.2777\n",
      "F1 Score: 0.7030\n",
      "Precision: 0.9720\n",
      "Recall: 0.5506\n",
      "Results saved for data_ToNIoT.csv with model NeuTraLAD\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model LOF\n",
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 84.98958280952245\n",
      "AUCPR: 96.49319026596923\n",
      "Accuracy: 52.46\n",
      "MCC: 0.2367\n",
      "F1 Score: 0.6388\n",
      "Precision: 0.9697\n",
      "Recall: 0.4762\n",
      "Results saved for data_ToNIoT.csv with model LOF\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model AutoEncoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:02<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 71.03186554538243\n",
      "AUCPR: 93.15560545011878\n",
      "Accuracy: 28.76\n",
      "MCC: 0.0816\n",
      "F1 Score: 0.3390\n",
      "Precision: 0.9359\n",
      "Recall: 0.2070\n",
      "Results saved for data_ToNIoT.csv with model AutoEncoder\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model CBLOF\n",
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 64.66074131789648\n",
      "AUCPR: 90.55086962612545\n",
      "Accuracy: 17.67\n",
      "MCC: -0.0325\n",
      "F1 Score: 0.1493\n",
      "Precision: 0.8483\n",
      "Recall: 0.0818\n",
      "Results saved for data_ToNIoT.csv with model CBLOF\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model HBOS\n",
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 82.44037657789865\n",
      "AUCPR: 96.8782853575566\n",
      "Accuracy: 67.01\n",
      "MCC: 0.3400\n",
      "F1 Score: 0.7746\n",
      "Precision: 0.9755\n",
      "Recall: 0.6423\n",
      "Results saved for data_ToNIoT.csv with model HBOS\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model DASVDD\n",
      "Tuning gamma...\n",
      "Gamma: 0.9367\n",
      "Epoch 10/100, Loss: 1.002211\n",
      "Epoch 20/100, Loss: 0.447058\n",
      "Epoch 30/100, Loss: 0.341232\n",
      "Epoch 40/100, Loss: 0.315647\n",
      "Epoch 50/100, Loss: 0.134478\n",
      "Epoch 60/100, Loss: 0.093704\n",
      "Epoch 70/100, Loss: 0.108226\n",
      "Epoch 80/100, Loss: 0.049473\n",
      "Epoch 90/100, Loss: 0.063533\n",
      "Epoch 100/100, Loss: 0.047884\n",
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 59.60014236842006\n",
      "AUCPR: 90.5009093906426\n",
      "Accuracy: 52.83\n",
      "MCC: 0.0879\n",
      "F1 Score: 0.6588\n",
      "Precision: 0.9109\n",
      "Recall: 0.5160\n",
      "Results saved for data_ToNIoT.csv with model DASVDD\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model PCA\n",
      "Error with dataset data_ToNIoT.csv, model PCA: Input X contains infinity or a value too large for dtype('float64').\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model AE1SVM\n",
      "Epoch 10/50, Loss: 2.2471093584279545\n",
      "Epoch 20/50, Loss: 1.6362366803356858\n",
      "Epoch 30/50, Loss: 1.4873176197536657\n",
      "Epoch 40/50, Loss: 1.239896570561362\n",
      "Epoch 50/50, Loss: 1.2016807513647392\n",
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 52.247574519026706\n",
      "AUCPR: 88.85540644759335\n",
      "Accuracy: 17.25\n",
      "MCC: -0.0327\n",
      "F1 Score: 0.1398\n",
      "Precision: 0.8468\n",
      "Recall: 0.0762\n",
      "Results saved for data_ToNIoT.csv with model AE1SVM\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model DevNet\n",
      "Original training size: 3888, No. outliers: 0\n",
      "3888 0 3888 432\n",
      "Epoch 1, Loss: -72.35830688476562\n",
      "Epoch 2, Loss: -213.81248474121094\n",
      "Epoch 3, Loss: -393.1815185546875\n",
      "Epoch 4, Loss: -605.34814453125\n",
      "Epoch 5, Loss: -823.5523681640625\n",
      "Epoch 6, Loss: -1012.0157470703125\n",
      "Epoch 7, Loss: -1302.2169189453125\n",
      "Epoch 8, Loss: -1523.8612060546875\n",
      "Epoch 9, Loss: -1720.7119140625\n",
      "Epoch 10, Loss: -1977.8916015625\n",
      "Epoch 11, Loss: -2219.1396484375\n",
      "Epoch 12, Loss: -2476.06787109375\n",
      "Epoch 13, Loss: -2720.6201171875\n",
      "Epoch 14, Loss: -3107.35595703125\n",
      "Epoch 15, Loss: -3327.739990234375\n",
      "Epoch 16, Loss: -3377.779541015625\n",
      "Epoch 17, Loss: -3677.98681640625\n",
      "Epoch 18, Loss: -4012.3857421875\n",
      "Epoch 19, Loss: -4321.32763671875\n",
      "Epoch 20, Loss: -4494.64892578125\n",
      "Epoch 21, Loss: -4970.0009765625\n",
      "Epoch 22, Loss: -5383.34228515625\n",
      "Epoch 23, Loss: -5745.85546875\n",
      "Epoch 24, Loss: -5789.44873046875\n",
      "Epoch 25, Loss: -6229.96533203125\n",
      "Epoch 26, Loss: -6489.24560546875\n",
      "Epoch 27, Loss: -7021.509765625\n",
      "Epoch 28, Loss: -7135.08935546875\n",
      "Epoch 29, Loss: -7179.478515625\n",
      "Epoch 30, Loss: -7789.814453125\n",
      "Epoch 31, Loss: -8024.1474609375\n",
      "Epoch 32, Loss: -8809.134765625\n",
      "Epoch 33, Loss: -8738.623046875\n",
      "Epoch 34, Loss: -9262.408203125\n",
      "Epoch 35, Loss: -9546.212890625\n",
      "Epoch 36, Loss: -10285.0419921875\n",
      "Epoch 37, Loss: -10081.21875\n",
      "Epoch 38, Loss: -10672.8603515625\n",
      "Epoch 39, Loss: -10962.796875\n",
      "Epoch 40, Loss: -11373.640625\n",
      "Epoch 41, Loss: -11881.5556640625\n",
      "Epoch 42, Loss: -11931.9873046875\n",
      "Epoch 43, Loss: -12699.4228515625\n",
      "Epoch 44, Loss: -12602.482421875\n",
      "Epoch 45, Loss: -13534.689453125\n",
      "Epoch 46, Loss: -13981.1259765625\n",
      "Epoch 47, Loss: -13723.36328125\n",
      "Epoch 48, Loss: -15141.6484375\n",
      "Epoch 49, Loss: -15453.77734375\n",
      "Epoch 50, Loss: -15528.44140625\n",
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 52.05217841345815\n",
      "AUCPR: 88.5358683325676\n",
      "Accuracy: 25.99\n",
      "MCC: 0.0641\n",
      "F1 Score: 0.2943\n",
      "Precision: 0.9289\n",
      "Recall: 0.1749\n",
      "Results saved for data_ToNIoT.csv with model DevNet\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model DeepSVDD\n",
      "Epoch 1/100, Loss: 69.35584722459316\n",
      "Epoch 2/100, Loss: 69.4396775662899\n",
      "Epoch 3/100, Loss: 69.42405050992966\n",
      "Epoch 4/100, Loss: 69.34043869376183\n",
      "Epoch 5/100, Loss: 69.42091996967793\n",
      "Epoch 6/100, Loss: 69.32165877521038\n",
      "Epoch 7/100, Loss: 69.39959055185318\n",
      "Epoch 8/100, Loss: 69.33632662892342\n",
      "Epoch 9/100, Loss: 69.3859646320343\n",
      "Epoch 10/100, Loss: 69.33226352930069\n",
      "Epoch 11/100, Loss: 69.38435533642769\n",
      "Epoch 12/100, Loss: 69.43262514472008\n",
      "Epoch 13/100, Loss: 69.34274405241013\n",
      "Epoch 14/100, Loss: 69.33687207102776\n",
      "Epoch 15/100, Loss: 69.30928795039654\n",
      "Epoch 16/100, Loss: 69.37983936071396\n",
      "Epoch 17/100, Loss: 69.42766770720482\n",
      "Epoch 18/100, Loss: 69.37444287538528\n",
      "Epoch 19/100, Loss: 69.36875787377357\n",
      "Epoch 20/100, Loss: 69.35085052251816\n",
      "Epoch 21/100, Loss: 69.35095642507076\n",
      "Epoch 22/100, Loss: 69.32784447073936\n",
      "Epoch 23/100, Loss: 69.35759773850441\n",
      "Epoch 24/100, Loss: 69.39413768053055\n",
      "Epoch 25/100, Loss: 69.39390054345131\n",
      "Epoch 26/100, Loss: 69.40056484937668\n",
      "Epoch 27/100, Loss: 69.36326986551285\n",
      "Epoch 28/100, Loss: 69.49974551796913\n",
      "Epoch 29/100, Loss: 69.41085377335548\n",
      "Epoch 30/100, Loss: 69.36003628373146\n",
      "Epoch 31/100, Loss: 69.43326887488365\n",
      "Epoch 32/100, Loss: 69.3409990221262\n",
      "Epoch 33/100, Loss: 69.31818053126335\n",
      "Epoch 34/100, Loss: 69.38830208778381\n",
      "Epoch 35/100, Loss: 69.37191650271416\n",
      "Epoch 36/100, Loss: 69.36528345942497\n",
      "Epoch 37/100, Loss: 69.34726630151272\n",
      "Epoch 38/100, Loss: 73.07180446386337\n",
      "Epoch 39/100, Loss: 69.33142790198326\n",
      "Epoch 40/100, Loss: 69.3456366956234\n",
      "Epoch 41/100, Loss: 69.33009645342827\n",
      "Epoch 42/100, Loss: 69.40008369088173\n",
      "Epoch 43/100, Loss: 69.3371117413044\n",
      "Epoch 44/100, Loss: 69.38307479023933\n",
      "Epoch 45/100, Loss: 69.37246942520142\n",
      "Epoch 46/100, Loss: 69.36677154898643\n",
      "Epoch 47/100, Loss: 71.08119197189808\n",
      "Epoch 48/100, Loss: 69.44996038079262\n",
      "Epoch 49/100, Loss: 69.3621529340744\n",
      "Epoch 50/100, Loss: 70.11934295296669\n",
      "Epoch 51/100, Loss: 69.34226477146149\n",
      "Epoch 52/100, Loss: 69.35708311200142\n",
      "Epoch 53/100, Loss: 69.41942718625069\n",
      "Epoch 54/100, Loss: 69.41817671060562\n",
      "Epoch 55/100, Loss: 69.35935613512993\n",
      "Epoch 56/100, Loss: 69.37957811355591\n",
      "Epoch 57/100, Loss: 69.35932964086533\n",
      "Epoch 58/100, Loss: 69.44548133015633\n",
      "Epoch 59/100, Loss: 69.36059615015984\n",
      "Epoch 60/100, Loss: 69.34362411499023\n",
      "Epoch 61/100, Loss: 69.40858340263367\n",
      "Epoch 62/100, Loss: 69.42218294739723\n",
      "Epoch 63/100, Loss: 69.35269585251808\n",
      "Epoch 64/100, Loss: 69.39911231398582\n",
      "Epoch 65/100, Loss: 69.38908115029335\n",
      "Epoch 66/100, Loss: 69.34096100926399\n",
      "Epoch 67/100, Loss: 73.1261673271656\n",
      "Epoch 68/100, Loss: 69.36133667826653\n",
      "Epoch 69/100, Loss: 69.30104434490204\n",
      "Epoch 70/100, Loss: 69.36253798007965\n",
      "Epoch 71/100, Loss: 69.3538291156292\n",
      "Epoch 72/100, Loss: 69.39308679103851\n",
      "Epoch 73/100, Loss: 69.34454874694347\n",
      "Epoch 74/100, Loss: 69.38086864352226\n",
      "Epoch 75/100, Loss: 69.4042454212904\n",
      "Epoch 76/100, Loss: 69.38837295770645\n",
      "Epoch 77/100, Loss: 69.43964383006096\n",
      "Epoch 78/100, Loss: 69.36699099838734\n",
      "Epoch 79/100, Loss: 69.96223372220993\n",
      "Epoch 80/100, Loss: 69.36273831129074\n",
      "Epoch 81/100, Loss: 69.36174464225769\n",
      "Epoch 82/100, Loss: 69.42347133159637\n",
      "Epoch 83/100, Loss: 69.39873918890953\n",
      "Epoch 84/100, Loss: 69.37339758872986\n",
      "Epoch 85/100, Loss: 73.5073059797287\n",
      "Epoch 86/100, Loss: 69.98787742853165\n",
      "Epoch 87/100, Loss: 69.36108309030533\n",
      "Epoch 88/100, Loss: 69.39100921154022\n",
      "Epoch 89/100, Loss: 69.4285196363926\n",
      "Epoch 90/100, Loss: 69.43237382173538\n",
      "Epoch 91/100, Loss: 69.35382416844368\n",
      "Epoch 92/100, Loss: 69.40293607115746\n",
      "Epoch 93/100, Loss: 69.336824670434\n",
      "Epoch 94/100, Loss: 69.35496124625206\n",
      "Epoch 95/100, Loss: 69.3464842736721\n",
      "Epoch 96/100, Loss: 69.29897990822792\n",
      "Epoch 97/100, Loss: 69.39767959713936\n",
      "Epoch 98/100, Loss: 69.39079397916794\n",
      "Epoch 99/100, Loss: 69.43010532855988\n",
      "Epoch 100/100, Loss: 69.36426623165607\n",
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 43.8285389654922\n",
      "AUCPR: 85.86814888247402\n",
      "Accuracy: 15.46\n",
      "MCC: -0.0664\n",
      "F1 Score: 0.1050\n",
      "Precision: 0.7996\n",
      "Recall: 0.0562\n",
      "Results saved for data_ToNIoT.csv with model DeepSVDD\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model IForest\n",
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 79.59121708147083\n",
      "AUCPR: 94.29151956296818\n",
      "Accuracy: 30.57\n",
      "MCC: 0.1001\n",
      "F1 Score: 0.3655\n",
      "Precision: 0.9448\n",
      "Recall: 0.2266\n",
      "Results saved for data_ToNIoT.csv with model IForest\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model OCSVM\n",
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 30.26246102171105\n",
      "AUCPR: 83.77876474773336\n",
      "Accuracy: 18.44\n",
      "MCC: -0.0232\n",
      "F1 Score: 0.1643\n",
      "Precision: 0.8593\n",
      "Recall: 0.0908\n",
      "Results saved for data_ToNIoT.csv with model OCSVM\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model LODA\n",
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 45.61397935684944\n",
      "AUCPR: 83.89142173664538\n",
      "Accuracy: 12.01\n",
      "MCC: -0.1577\n",
      "F1 Score: 0.0292\n",
      "Precision: 0.5575\n",
      "Recall: 0.0150\n",
      "Results saved for data_ToNIoT.csv with model LODA\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model MO_GAAL\n",
      "Epoch 1 of 60\n",
      "Epoch 2 of 60\n",
      "Epoch 3 of 60\n",
      "Epoch 4 of 60\n",
      "Epoch 5 of 60\n",
      "Epoch 6 of 60\n",
      "Epoch 7 of 60\n",
      "Epoch 8 of 60\n",
      "Epoch 9 of 60\n",
      "Epoch 10 of 60\n",
      "Epoch 11 of 60\n",
      "Epoch 12 of 60\n",
      "Epoch 13 of 60\n",
      "Epoch 14 of 60\n",
      "Epoch 15 of 60\n",
      "Epoch 16 of 60\n",
      "Epoch 17 of 60\n",
      "Epoch 18 of 60\n",
      "Epoch 19 of 60\n",
      "Epoch 20 of 60\n",
      "Epoch 21 of 60\n",
      "Epoch 22 of 60\n",
      "Epoch 23 of 60\n",
      "Epoch 24 of 60\n",
      "Epoch 25 of 60\n",
      "Epoch 26 of 60\n",
      "Epoch 27 of 60\n",
      "Epoch 28 of 60\n",
      "Epoch 29 of 60\n",
      "Epoch 30 of 60\n",
      "Epoch 31 of 60\n",
      "Epoch 32 of 60\n",
      "Epoch 33 of 60\n",
      "Epoch 34 of 60\n",
      "Epoch 35 of 60\n",
      "Epoch 36 of 60\n",
      "Epoch 37 of 60\n",
      "Epoch 38 of 60\n",
      "Epoch 39 of 60\n",
      "Epoch 40 of 60\n",
      "Epoch 41 of 60\n",
      "Epoch 42 of 60\n",
      "Epoch 43 of 60\n",
      "Epoch 44 of 60\n",
      "Epoch 45 of 60\n",
      "Epoch 46 of 60\n",
      "Epoch 47 of 60\n",
      "Epoch 48 of 60\n",
      "Epoch 49 of 60\n",
      "Epoch 50 of 60\n",
      "Epoch 51 of 60\n",
      "Epoch 52 of 60\n",
      "Epoch 53 of 60\n",
      "Epoch 54 of 60\n",
      "Epoch 55 of 60\n",
      "Epoch 56 of 60\n",
      "Epoch 57 of 60\n",
      "Epoch 58 of 60\n",
      "Epoch 59 of 60\n",
      "Epoch 60 of 60\n",
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 49.308792669297034\n",
      "AUCPR: 87.08456073500858\n",
      "Accuracy: 22.06\n",
      "MCC: 0.0377\n",
      "F1 Score: 0.2259\n",
      "Precision: 0.9148\n",
      "Recall: 0.1289\n",
      "Results saved for data_ToNIoT.csv with model MO_GAAL\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model SUOD\n",
      "Split among workers default: [5] []\n",
      "Parallel Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Scheduling Total Train Time: 0.2705235481262207\n",
      "Split among workers default: [5] []\n",
      "Split among workers default: [5] []\n",
      "Parallel score prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel Score Prediction without Approximators Total Time: 0.32795119285583496\n",
      "Split among workers default: [5] []\n",
      "Parallel score prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel Score Prediction without Approximators Total Time: 0.3305246829986572\n",
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 50.909315425804856\n",
      "AUCPR: 87.19517372025676\n",
      "Accuracy: 24.92\n",
      "MCC: -0.0839\n",
      "F1 Score: 0.3069\n",
      "Precision: 0.8287\n",
      "Recall: 0.1883\n",
      "Results saved for data_ToNIoT.csv with model SUOD\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model DIF\n",
      "network additional parameters: {'n_hidden': [500, 100], 'n_emb': 20, 'skip_connection': None, 'dropout': None, 'activation': 'tanh', 'be_size': 50}\n",
      "network additional parameters: {'n_hidden': [500, 100], 'n_emb': 20, 'skip_connection': None, 'dropout': None, 'activation': 'tanh', 'be_size': 50}\n",
      "training done, time: 0.8\n",
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 46.43284334373115\n",
      "AUCPR: 88.5167003163932\n",
      "Accuracy: 45.71\n",
      "MCC: -0.1333\n",
      "F1 Score: 0.6073\n",
      "Precision: 0.8397\n",
      "Recall: 0.4757\n",
      "Results saved for data_ToNIoT.csv with model DIF\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model ALAD\n",
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 46.047131534974525\n",
      "AUCPR: 86.47491215768204\n",
      "Accuracy: 16.05\n",
      "MCC: -0.0466\n",
      "F1 Score: 0.1151\n",
      "Precision: 0.8263\n",
      "Recall: 0.0619\n",
      "Results saved for data_ToNIoT.csv with model ALAD\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model COPOD\n",
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 22.39722312190028\n",
      "AUCPR: 79.55632579494127\n",
      "Accuracy: 20.32\n",
      "MCC: -0.1982\n",
      "F1 Score: 0.2470\n",
      "Precision: 0.7444\n",
      "Recall: 0.1481\n",
      "Results saved for data_ToNIoT.csv with model COPOD\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model ECOD\n",
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 19.598320182413254\n",
      "AUCPR: 78.75328046475127\n",
      "Accuracy: 20.08\n",
      "MCC: -0.2385\n",
      "F1 Score: 0.2530\n",
      "Precision: 0.7227\n",
      "Recall: 0.1534\n",
      "Results saved for data_ToNIoT.csv with model ECOD\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model VAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [00:17<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 57.459278828884116\n",
      "AUCPR: 90.54968838712419\n",
      "Accuracy: 24.57\n",
      "MCC: 0.0376\n",
      "F1 Score: 0.2739\n",
      "Precision: 0.9108\n",
      "Recall: 0.1612\n",
      "Results saved for data_ToNIoT.csv with model VAE\n",
      "\n",
      "Running dataset data_ToNIoT.csv with model SO_GAAL\n",
      "Epoch 1 of 60\n",
      "Epoch 2 of 60\n",
      "Epoch 3 of 60\n",
      "Epoch 4 of 60\n",
      "Epoch 5 of 60\n",
      "Epoch 6 of 60\n",
      "Epoch 7 of 60\n",
      "Epoch 8 of 60\n",
      "Epoch 9 of 60\n",
      "Epoch 10 of 60\n",
      "Epoch 11 of 60\n",
      "Epoch 12 of 60\n",
      "Epoch 13 of 60\n",
      "Epoch 14 of 60\n",
      "Epoch 15 of 60\n",
      "Epoch 16 of 60\n",
      "Epoch 17 of 60\n",
      "Epoch 18 of 60\n",
      "Epoch 19 of 60\n",
      "Epoch 20 of 60\n",
      "Epoch 21 of 60\n",
      "Epoch 22 of 60\n",
      "Epoch 23 of 60\n",
      "Epoch 24 of 60\n",
      "Epoch 25 of 60\n",
      "Epoch 26 of 60\n",
      "Epoch 27 of 60\n",
      "Epoch 28 of 60\n",
      "Epoch 29 of 60\n",
      "Epoch 30 of 60\n",
      "Epoch 31 of 60\n",
      "Epoch 32 of 60\n",
      "Epoch 33 of 60\n",
      "Epoch 34 of 60\n",
      "Epoch 35 of 60\n",
      "Epoch 36 of 60\n",
      "Epoch 37 of 60\n",
      "Epoch 38 of 60\n",
      "Epoch 39 of 60\n",
      "Epoch 40 of 60\n",
      "Epoch 41 of 60\n",
      "Epoch 42 of 60\n",
      "Epoch 43 of 60\n",
      "Epoch 44 of 60\n",
      "Epoch 45 of 60\n",
      "Epoch 46 of 60\n",
      "Epoch 47 of 60\n",
      "Epoch 48 of 60\n",
      "Epoch 49 of 60\n",
      "Epoch 50 of 60\n",
      "Epoch 51 of 60\n",
      "Epoch 52 of 60\n",
      "Epoch 53 of 60\n",
      "Epoch 54 of 60\n",
      "Epoch 55 of 60\n",
      "Epoch 56 of 60\n",
      "Epoch 57 of 60\n",
      "Epoch 58 of 60\n",
      "Epoch 59 of 60\n",
      "Epoch 60 of 60\n",
      "..............................Evaluation Metrics...............................\n",
      "AUCROC: 40.02339643576091\n",
      "AUCPR: 81.95912580742272\n",
      "Accuracy: 11.57\n",
      "MCC: -0.1824\n",
      "F1 Score: 0.0185\n",
      "Precision: 0.4535\n",
      "Recall: 0.0094\n",
      "Results saved for data_ToNIoT.csv with model SO_GAAL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    matthews_corrcoef, f1_score, precision_score, recall_score,\n",
    "    accuracy_score, roc_auc_score, average_precision_score\n",
    ")\n",
    "\n",
    "from pyod.models.ae1svm import AE1SVM\n",
    "from pyod.models.devnet import DevNet\n",
    "from pyod.models.lunar import LUNAR\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.deep_svdd import DeepSVDD\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.pca import PCA\n",
    "from pyod.models.sod import SOD\n",
    "from pyod.models.cof import COF\n",
    "from pyod.models.loda import LODA\n",
    "from pyod.models.alad import ALAD\n",
    "from pyod.models.ecod import ECOD\n",
    "from pyod.models.copod import COPOD\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.models.vae import VAE\n",
    "from pyod.models.so_gaal import SO_GAAL  # đúng: \"sogaal\" (không phải \"so_gaal\")\n",
    "from pyod.models.mo_gaal import MO_GAAL  # đúng: \"mogaal\" (không phải \"mo_gaal\")\n",
    "from pyod.models.suod import SUOD\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from baseline_model.dasvdd_wrapper import DASVDD\n",
    "from baseline_model.neutralad_wrapper import NeuTraLAD\n",
    "from baseline_model.dif_wrapper import DIF\n",
    "\n",
    "models_list = [\n",
    "    \"KNN\",\n",
    "    \"LUNAR\",\n",
    "    \"NeuTraLAD\",     \n",
    "    \"LOF\",\n",
    "    \"AutoEncoder\",\n",
    "    \"CBLOF\",\n",
    "    \"HBOS\",\n",
    "    \"DASVDD\", \n",
    "    \"PCA\",\n",
    "    \"AE1SVM\",\n",
    "    \"DevNet\",\n",
    "    \"DeepSVDD\",\n",
    "    \"IForest\",\n",
    "    \"OCSVM\",\n",
    "    \"LODA\",\n",
    "    \"MO_GAAL\", \n",
    "    \"SUOD\", \n",
    "    \"DIF\", \n",
    "    \"ALAD\",\n",
    "    \"COPOD\",\n",
    "    \"ECOD\",\n",
    "    \"VAE\", \n",
    "    \"SO_GAAL\",\n",
    "]\n",
    "\n",
    "output_file = f\"additionalBaselineNoise.csv\"\n",
    "# output_file = f\"../../Results/additionalBaselineNoise.csv\"\n",
    "columns = [\"Dataset\",\"Model\",  \"scaled\", \"noise_percentage\", \"AUCROC\", \"AUCPR\", \"Accuracy\", \"MCC\", \"F1 Score\",\n",
    "           \"Precision\", \"Recall\", \"Time Train\", \"Time Test\"]\n",
    "\n",
    "def preprocess_data_noise(train_data, test_data, noise_percentage=10):\n",
    "    \"\"\"\n",
    "    Preprocess the training and testing data by separating features and labels,\n",
    "    and return the preprocessed feature sets and labels with added noise.\n",
    "    \n",
    "    Args:\n",
    "        train_data (DataFrame): Training data containing features and labels.\n",
    "        test_data (DataFrame): Testing data containing features and labels.\n",
    "        noise_percentage (float): The percentage of training samples to add noise to.\n",
    "        \n",
    "    Returns:\n",
    "        X_train (ndarray): Preprocessed training features with added noise.\n",
    "        y_train (ndarray): Preprocessed training labels with added noise.\n",
    "        X_test (ndarray): Preprocessed testing features.\n",
    "        y_test (ndarray): Preprocessed testing labels.\n",
    "    \"\"\"\n",
    "    print(\"..............................Data Overview................................\")\n",
    "    print(\"Train Data Shape:\", train_data.shape)\n",
    "    print(\"Test Data Shape:\", test_data.shape)\n",
    "    \n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    X_train_total = train_data.iloc[:, :-1].to_numpy()\n",
    "    y_train_total = train_data.iloc[:, -1].to_numpy()\n",
    "\n",
    "    # Separate the samples with label 0\n",
    "    X_train = X_train_total[y_train_total == 0]\n",
    "    y_train = y_train_total[y_train_total == 0]\n",
    "\n",
    "    print(\"Train Data Labels [0]:\", np.unique(y_train))\n",
    "\n",
    "    # Calculate how many samples to add noise to based on the provided percentage\n",
    "    n_samples = X_train.shape[0]\n",
    "    noise_samples_count = int(n_samples * (noise_percentage / 100))\n",
    "\n",
    "    # Get the samples with label 1 (for generating noise)\n",
    "    X_train_noise = X_train_total[y_train_total == 1]\n",
    "    \n",
    "    # Randomly select noise_samples_count from X_train_noise\n",
    "    noisy_indices = np.random.choice(X_train_noise.shape[0], size=noise_samples_count, replace=False)\n",
    "    X_train_noise = X_train_noise[noisy_indices]\n",
    "    \n",
    "    # Add the noisy samples to the training set\n",
    "    X_train = np.vstack((X_train, X_train_noise))\n",
    "    # y_train = np.concatenate((y_train, np.ones(X_train_noise.shape[0])))\n",
    "    y_train = np.concatenate((y_train, np.zeros(X_train_noise.shape[0])))\n",
    "\n",
    "    print(y_train) \n",
    "    # Prepare test data\n",
    "    X_test = test_data.iloc[:, :-1].to_numpy()\n",
    "    y_test = test_data.iloc[:, -1].to_numpy()\n",
    "\n",
    "    # Print the new size of training data\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_features = X_train.shape[1]\n",
    "    print(\"Number of samples after adding noise:\", n_samples)\n",
    "    print(\"Number of features:\", n_features)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred, y_scores=None, y_probabilities=None):\n",
    "    \"\"\"\n",
    "    Evaluates the model using multiple metrics and prints the results.\n",
    "    This version includes AUCROC and AUCPR using predicted probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true (np.ndarray): True labels of the test data.\n",
    "    - y_pred (np.ndarray): Predicted labels of the test data.\n",
    "    - y_scores (np.ndarray, optional): Scores used to compute AUCROC and AUCPR.\n",
    "    - y_probabilities (np.ndarray, optional): Predicted probabilities for each class.\n",
    "\n",
    "    Returns:\n",
    "    - metrics (list): A list containing AUCROC, AUCPR, accuracy, MCC, F1 score, precision, and recall.\n",
    "    \"\"\"\n",
    "    print(\"..............................Evaluation Metrics...............................\")\n",
    "\n",
    "    # Calculate standard metrics\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)  # Matthew's correlation coefficient\n",
    "    f1 = f1_score(y_true, y_pred)  # F1 score\n",
    "    precision = precision_score(y_true, y_pred)  # Precision\n",
    "    recall = recall_score(y_true, y_pred)  # Recall\n",
    "    accuracy = accuracy_score(y_true, y_pred)  # Accuracy\n",
    "\n",
    "    # ROC and PR curve scores using predicted probabilities\n",
    "    auc_roc, auc_pr = None, None\n",
    "    if y_probabilities is not None:\n",
    "        auc_roc = roc_auc_score(y_true, y_probabilities[:, 1])  # Probabilities for class 1 (outliers)\n",
    "        auc_pr = average_precision_score(y_true, y_probabilities[:, 1])  # AUCPR for class 1\n",
    "\n",
    "    # Display metrics\n",
    "    print(f\"AUCROC: {auc_roc * 100 if auc_roc else 'N/A'}\")\n",
    "    print(f\"AUCPR: {auc_pr * 100 if auc_pr else 'N/A'}\")\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}\")\n",
    "    print(f\"MCC: {mcc:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "    # Return the evaluation metrics as a list\n",
    "    return [auc_roc * 100 if auc_roc else None, auc_pr * 100 if auc_pr else None,\n",
    "            accuracy * 100, mcc, f1, precision, recall]\n",
    "\n",
    "\n",
    "def get_model(name, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns the appropriate PyOD model based on the provided name.\n",
    "    \n",
    "    Parameters:\n",
    "    - name (str): The name of the anomaly detection model (e.g., 'CBLOF', 'KNN').\n",
    "    - kwargs (dict): Additional parameters to initialize the model.\n",
    "\n",
    "    Returns:\n",
    "    - model (object): An instance of the specified anomaly detection model.\n",
    "    \"\"\"\n",
    "    model_dict = {\n",
    "        \"CBLOF\": CBLOF,\n",
    "        \"KNN\": KNN,\n",
    "        \"IForest\": IForest,\n",
    "        \"OCSVM\": OCSVM,\n",
    "        \"LOF\": LOF,\n",
    "        \"DeepSVDD\": DeepSVDD,\n",
    "        \"HBOS\": HBOS,\n",
    "        \"SOD\": SOD,\n",
    "        \"COF\": COF,\n",
    "        \"LODA\": LODA,\n",
    "        \"PCA\": PCA,\n",
    "        \"ECOD\": ECOD,\n",
    "        \"COPOD\": COPOD,\n",
    "        \"AutoEncoder\": AutoEncoder,\n",
    "        \"DevNet\": DevNet,\n",
    "        \"LUNAR\": LUNAR,\n",
    "        \"AE1SVM\": AE1SVM,\n",
    "        \"ALAD\": ALAD,\n",
    "        \"VAE\": VAE,\n",
    "        \"SO_GAAL\": SO_GAAL,\n",
    "        \"MO_GAAL\": MO_GAAL,\n",
    "        \"DASVDD\": DASVDD, \n",
    "        \"SUOD\": SUOD, \n",
    "        \"NeuTraLAD\": NeuTraLAD,\n",
    "        \"DIF\": DIF, \n",
    "    }\n",
    "    \n",
    "    # Fetch the model class based on the provided model name\n",
    "    model_class = model_dict.get(name)\n",
    "    \n",
    "    # Raise an error if the model name is not found\n",
    "    if model_class is None:\n",
    "        raise ValueError(f\"Model {name} not found.\")\n",
    "    \n",
    "    # Return the model initialized with the provided parameters\n",
    "    return model_class(**kwargs)\n",
    "\n",
    "\n",
    "# Initialize output CSV file\n",
    "if not os.path.exists(output_file):\n",
    "    pd.DataFrame(columns=columns).to_csv(output_file, index=False)\n",
    "\n",
    "# Define a function to run the model training and evaluation\n",
    "def run_experiment(X_train, y_train, X_test, y_test, name, noise_percentage, scaler ):\n",
    "    for model_name in models_list:\n",
    "        try:\n",
    "            print(f\"\\nRunning dataset {name} with model {model_name}\")\n",
    "            \n",
    "            # train_data = pd.concat([X_train, y_train], axis=1, ignore_index=True)\n",
    "            # test_data = pd.concat([X_test, y_test], axis=1, ignore_index=True)\n",
    "\n",
    "            # # Preprocess data (assuming preprocess_data is defined elsewhere)\n",
    "            # X_train, y_train, X_test, y_test = preprocess_data(train_data, test_data)\n",
    "            \n",
    "            if model_name == 'DeepSVDD':\n",
    "                start_time = time.time()\n",
    "                n_features = X_train.shape[1]\n",
    "                model = DeepSVDD(n_features=n_features)\n",
    "                model.fit(X_train)\n",
    "            elif model_name == 'DASVDD':\n",
    "                start_time = time.time()\n",
    "                model = DASVDD(\n",
    "                    code_size=32,\n",
    "                    num_epochs=100,\n",
    "                    batch_size=128,\n",
    "                    lr=1e-3,\n",
    "                    K=0.9,\n",
    "                    T=10,\n",
    "                    verbose=1\n",
    "                )\n",
    "                model.fit(X_train)\n",
    "            elif model_name == \"DIF\": \n",
    "                start_time = time.time()\n",
    "                model = DIF(n_ensemble=50, n_estimators=6)\n",
    "\n",
    "                # Hoặc với custom config\n",
    "                model = DIF(\n",
    "                    n_ensemble=50,\n",
    "                    n_estimators=6,\n",
    "                    max_samples=256,\n",
    "                    hidden_dim=[500, 100],\n",
    "                    rep_dim=20,\n",
    "                    activation='tanh',\n",
    "                    batch_size=64,\n",
    "                    device='cuda',\n",
    "                    verbose=1\n",
    "                )\n",
    "                model.fit(X_train)\n",
    "                \n",
    "            elif model_name == \"NeuTraLAD\": \n",
    "                start_time = time.time()\n",
    "                model = NeuTraLAD(\n",
    "                    latent_dim=32,      # Kích thước latent space\n",
    "                    enc_hdim=32,        # Hidden dim encoder\n",
    "                    num_trans=11,       # Số transformations\n",
    "                    num_epochs=200,     # Số epochs\n",
    "                    batch_size=128,\n",
    "                    lr=0.001,\n",
    "                    device='cuda',      # hoặc 'cpu'\n",
    "                    verbose=1           # 1 để hiển thị progress\n",
    "                )\n",
    "                # Fit model (chỉ dùng normal data)\n",
    "                model.fit(X_train)\n",
    "            elif model_name == \"SUOD\": \n",
    "                start_time = time.time()\n",
    "                \n",
    "                base_estimators = [\n",
    "                    HBOS(n_bins=10),\n",
    "                    COPOD(),\n",
    "                    ECOD(),\n",
    "                    IForest(n_estimators=50, max_samples=128, random_state=42),\n",
    "                    PCA(n_components=10, random_state=42),\n",
    "                ]\n",
    "                \n",
    "                model = SUOD(\n",
    "                    base_estimators=base_estimators,\n",
    "                    n_jobs=1,                    # BẮT BUỘC = 1 để tránh hoàn toàn bug joblib\n",
    "                    rp_flag_global=True,         # vẫn tăng tốc\n",
    "                    bps_flag=False,              # tắt BPS → nguyên nhân chính của lỗi\n",
    "                    contamination=0.1,\n",
    "                    verbose=True                 # in chi tiết để bạn thấy nó chạy\n",
    "                )\n",
    "                \n",
    "                model.fit(X_train)  # fit all models with X\n",
    "            else:\n",
    "                model = get_model(model_name)\n",
    "                start_time = time.time()\n",
    "                if model_name == 'DevNet':\n",
    "                    model.fit(X_train, y_train)\n",
    "                else:\n",
    "                    model.fit(X_train)\n",
    "                    \n",
    "            train_time = time.time() - start_time\n",
    "\n",
    "           #  columns = [\"Dataset\", \"Model\", \"outlier_mode\", \"scaler\", \"AUCROC\", \"AUCPR\", \"Accuracy\", \"MCC\", \"F1 Score\",\n",
    "           # \"Precision\", \"Recall\", \"Time Train\", \"Time Test\"]\n",
    "            # Test the model\n",
    "            start_time = time.time()\n",
    "            y_pred = model.predict(X_test)\n",
    "            # Check if the model supports predict_proba and calculate probabilities\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                y_probabilities = model.predict_proba(X_test)  # Class probabilities for each instance\n",
    "            else:\n",
    "                y_probabilities = None  # Some models do not have predict_proba method\n",
    "\n",
    "            test_time = time.time() - start_time\n",
    "\n",
    "           #  columns = [\"Dataset\",\"Model\",  \"scaled\", \"noise_percentage\", \"AUCROC\", \"AUCPR\", \"Accuracy\", \"MCC\", \"F1 Score\",\n",
    "           # \"Precision\", \"Recall\", \"Time Train\", \"Time Test\"]\n",
    "            \n",
    "            # Evaluate the model using the appropriate probabilities\n",
    "            metrics = evaluate_model(y_test, y_pred, y_scores=None, y_probabilities=y_probabilities)\n",
    "            result = [name, model_name] + [scaler, noise_percentage]+ metrics + [train_time, test_time]\n",
    "            result_df = pd.DataFrame([result], columns=columns)\n",
    "            result_df.to_csv(output_file, mode='a', header=False, index=False)\n",
    "\n",
    "            print(f\"Results saved for {name} with model {model_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with dataset {name}, model {model_name}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    \n",
    "    dataset_prefixes =  ['data_ToNIoT.csv'] # , 'data_N_BaIoT.csv' , 'data_CICIoT2023.csv', 'data_BoTIoT.csv']\n",
    "    \n",
    "    # scaler_names = ['MinMaxScaler'] 'StandardScaler', \n",
    "    scaler_names = ['QuantileTransformer' ] # ,'MinMaxScaler','Normalizer','RobustScaler']\n",
    "    \n",
    "    # scaler_names = ['QuantileTransformer']\n",
    "    \n",
    "    for prefix in dataset_prefixes:\n",
    "        \n",
    "        for scaler in scaler_names: \n",
    "         \n",
    "            # Construct file paths for train and test datasets with 'Train_' and 'Test_' prefixes\n",
    "            train_file = f'../../Datascaled/NoiseOCData/Train_{scaler}_{prefix}'\n",
    "            test_file = f'../../Datascaled/NoiseOCData/Test_{scaler}_{prefix}'\n",
    "            \n",
    "            # Load the CSV files\n",
    "            df_train = pd.read_csv(train_file)\n",
    "            df_test = pd.read_csv(test_file)\n",
    "\n",
    "            \n",
    "            df_train = df_train.dropna() \n",
    "            df_test = df_test.dropna() \n",
    "            \n",
    "            # Nối lại thành 1 DataFrame\n",
    "            df_full = pd.concat([df_train, df_test], ignore_index=True)\n",
    "            \n",
    "            # Chia theo tỉ lệ 70% train, 30% test\n",
    "            df_train_new, df_test_new = train_test_split(df_full, test_size=0.3, random_state=42)\n",
    "\n",
    "            # print(f\"Số lượng hàng có NaN: {num_rows_with_nan}\")\n",
    "\n",
    "            for noise in [0] : # 1, 3, 5]: \n",
    "                \n",
    "                # Step 1: Preprocess data\n",
    "                X_train, y_train, X_test, y_test = preprocess_data_noise(df_train_new, df_test_new, noise)\n",
    "                # Run the experiments for each dataset type\n",
    "                run_experiment(X_train, y_train, X_test, y_test, prefix, noise, scaler ) \n",
    "               \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
