{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (roc_auc_score, precision_score, average_precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error, roc_curve, auc, classification_report,auc,confusion_matrix,matthews_corrcoef)\n",
    "from sklearn import logger\n",
    "from sklearn.datasets import make_blobs,make_multilabel_classification\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import KernelCenterer,LabelEncoder, MinMaxScaler, Normalizer, QuantileTransformer, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import scipy as sp\n",
    "from scipy.linalg import svd,null_space\n",
    "import os\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.cluster import KMeans,AgglomerativeClustering,SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.sparse import csr_matrix as sp\n",
    "import math\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.spatial.distance import cdist\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Import 3D plotting tools\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.datasets import make_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hàm\tĐộ phức tạp tính toán\tBộ nhớ cần\n",
    "# preprocess_data_OC\tO(n⋅d²)\tO(n⋅d²)\n",
    "# clusterr\tO(n⋅d⋅k⋅iters)\tO(n⋅d)\n",
    "# nullspace\tO(d³)\tO(d²)\n",
    "# gram_schmidt\tO(d³)\tO(d²)\n",
    "# minimum_distance\tO(n⋅m⋅d)\tO(n⋅m)\n",
    "# distance_vector\tO(n⋅m⋅d)\tO(n⋅m)\n",
    "# calculate_NPD\tO(d³)\tO(d²)\n",
    "# learn\tO(n²⋅d)\tO(n²)\n",
    "\n",
    "\n",
    "\n",
    "alpha = 0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data_noise(train_data, noise_percentage=10):\n",
    "  \n",
    "    print(\"..............................Data Overview................................\")\n",
    "    # print(\"Train Data Shape:\", train_data.shape) \n",
    "    \n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    X_train_total = train_data[:, :-1]\n",
    "    y_train_total = train_data[:, -1]\n",
    "\n",
    "    # Separate the samples with label 0\n",
    "    X_train = X_train_total[y_train_total == 0]\n",
    "    y_train = y_train_total[y_train_total == 0]\n",
    "\n",
    "    # print(\"Train Data Labels [0]:\", np.unique(y_train))\n",
    "\n",
    "    # Calculate how many samples to add noise to based on the provided percentage\n",
    "    n_samples = X_train.shape[0]\n",
    "    noise_samples_count = int(n_samples * (noise_percentage / 100))\n",
    "\n",
    "    # Get the samples with label 1 (for generating noise)\n",
    "    X_train_noise = X_train_total[y_train_total == 1]\n",
    "    \n",
    "    # Randomly select noise_samples_count from X_train_noise\n",
    "    noisy_indices = np.random.choice(X_train_noise.shape[0], size=noise_samples_count, replace=False)\n",
    "    X_train_noise = X_train_noise[noisy_indices]\n",
    "    \n",
    "    # Add the noisy samples to the training set\n",
    "    X_train = np.vstack((X_train, X_train_noise))\n",
    "    y_train = np.concatenate((y_train, np.ones(X_train_noise.shape[0])))\n",
    "    # print(y_train) \n",
    "    \n",
    "    # Print the new size of training data\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_features = X_train.shape[1]\n",
    "    # print(\"Number of samples after adding noise:\", n_samples)\n",
    "    # print(\"Number of features:\", n_features)\n",
    "\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def cluster_kmeans(data, initial_k):\n",
    "    print(\"Starting K-Means clustering...\")\n",
    "\n",
    "    # Thực hiện K-Means clustering với số cụm initial_k\n",
    "    kmeans = KMeans(n_clusters=initial_k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(data)\n",
    "\n",
    "    # Gán nhãn cụm vào biến labels\n",
    "    labels = cluster_labels\n",
    "\n",
    "    # Sắp xếp dữ liệu theo nhãn cụm\n",
    "    sorted_indices = np.argsort(labels)\n",
    "    sorted_data = data[sorted_indices]\n",
    "    sorted_labels = labels[sorted_indices]\n",
    "\n",
    "    print(\"Final number of clusters:\", len(np.unique(sorted_labels)))\n",
    "    return sorted_data, sorted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nullspace(A):\n",
    "    _, s, vh = np.linalg.svd(A)\n",
    "    null_mask = np.isclose(s, 0)\n",
    "    null_space = vh[null_mask].T\n",
    "    return null_space\n",
    "\n",
    "\n",
    "def minimum_distance( A, B):\n",
    "        \"\"\"\n",
    "        Compute the minimum Euclidean distance from each point in A to all points in B.\n",
    "        \n",
    "        Parameters:\n",
    "        - A (ndarray): Set of points (N_A, d).\n",
    "        - B (ndarray): Set of points (N_B, d).\n",
    "        \n",
    "        Returns:\n",
    "        - min_distances (ndarray): Minimum distances from each point in A to the nearest point in B.\n",
    "        \"\"\"\n",
    "        A = np.asarray(A)\n",
    "        B = np.asarray(B)\n",
    "        \n",
    "        # Initialize an array for storing minimum distances\n",
    "        min_distances = np.empty(A.shape[0], dtype=np.float64)\n",
    "        \n",
    "        # Iterate over each point in A and calculate minimum distance to points in B\n",
    "        for i, a in enumerate(A):\n",
    "            distances = cdist([a], B, metric='euclidean')  # Compute all pairwise distances\n",
    "            min_distances[i] = np.min(distances)  # Store minimum distance\n",
    "        \n",
    "        return min_distances   \n",
    "\n",
    "def distance_vector(point_X, point_Y):\n",
    "    \"\"\"\n",
    "    Calculate pairwise Euclidean distance between two sets of points.\n",
    "    \n",
    "    Args:\n",
    "        point_X (ndarray): Array of shape (N_train, d) where N_train is the number of training samples and d is the number of features.\n",
    "        point_Y (ndarray): Array of shape (N_test, d) where N_test is the number of test samples and d is the number of features.\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: Distance matrix of shape (N_test, N_train) containing Euclidean distances between each pair of points.\n",
    "    \"\"\"\n",
    "    print( 'Complexity of calculate: ', point_X )\n",
    "    # Compute squared norms for each point\n",
    "    norm_X = np.sum(point_X**2, axis=1)  # (N_train,)\n",
    "    norm_Y = np.sum(point_Y**2, axis=1)  # (N_test,)\n",
    "    \n",
    "    # Compute the dot product between the two sets of points\n",
    "    dot_product = np.dot(point_Y, point_X.T)  # (N_test, N_train)\n",
    "    \n",
    "    # Apply Euclidean distance formula\n",
    "    distance = np.sqrt(abs(norm_Y[:, np.newaxis] + norm_X[np.newaxis, :] - 2 * dot_product))\n",
    "    return distance\n",
    "\n",
    "\n",
    "\n",
    "def calculate_NPD(X,y):\n",
    "    print(\"Begin calculating NPD --------------\")\n",
    "    X = X.T\n",
    "    print('shape of X', X.shape)\n",
    "    c = len(np.unique(y))  # Số lớp\n",
    "    d, N = X.shape  # Số đặc trưng và số mẫu\n",
    "    # Tính trung bình toàn cục và tạo ma trận P_t với zero-mean\n",
    "    mean_total = np.mean(X, axis=1, keepdims=True)\n",
    "    P_t = X - mean_total  # P_t là ma trận zero-mean có kích thước d x N\n",
    "    \n",
    "    # Tính P_w cho từng lớp\n",
    "    P_w = np.zeros_like(X)\n",
    "    for i in range(c):\n",
    "        class_mean = np.mean(X[:, y == i], axis=1, keepdims=True)\n",
    "        P_w[:, y == i] = X[:, y == i] - class_mean  # Tạo ma trận P_w có kích thước d x N\n",
    "        \n",
    "    # Tính ma trận phương sai S_w và S_t\n",
    "    S_w = np.dot(P_w, P_w.T) / N  # S_w là d x d\n",
    "    S_t = np.dot(P_t, P_t.T) / N  # S_t là d x d\n",
    "\n",
    "    print( \"rank sw st :\", np.linalg.matrix_rank(S_t) , np.linalg.matrix_rank(S_w)) \n",
    "    print( \"rank sw st :\", np.linalg.matrix_rank(P_t) , np.linalg.matrix_rank(P_w)) \n",
    "\n",
    "    \n",
    "    U, S, Vt = np.linalg.svd(P_t, full_matrices=False)\n",
    "    Q = U\n",
    "    B = nullspace(Q.T @ S_w @ Q)\n",
    "    W = Q @ B  # W có kích thước d x (c - 1)\n",
    "\n",
    "    # In kết quả\n",
    "    print(\"N =\", N,  \"  d =\", d, \"   c =\", c )\n",
    "    print(\"P_w : d x N =\", P_w.shape)  # d x N\n",
    "    print(\"P_t : d x N =\", P_t.shape)  # d x N\n",
    "    print(\"S_w : d x d =\", S_w.shape)  # d x d\n",
    "    print(\"S_t : d x d =\", S_t.shape)  # d x d\n",
    "    print(\"Q   : d x N =\", Q.shape)     # d x r\n",
    "    print(\"B   : N x L =\", B.shape)     # r x L\n",
    "    print(\"W   : d x L =\", W.shape)     # d x L\n",
    "    return W    \n",
    "\n",
    "    \n",
    "\n",
    "def BruteForce_Threshold(y_true, y_prob, minth=0.0, maxth=1.0, num_thresholds=1000):\n",
    "    \"\"\"\n",
    "    Finds the best classification threshold for a binary model using brute force search.\n",
    "\n",
    "    Args:\n",
    "        y_true (ndarray): True labels (0 or 1), shape (n_samples,).\n",
    "        y_prob (ndarray): Predicted probabilities for class 1, shape (n_samples,).\n",
    "        minth (float, optional): Minimum threshold value. Default is 0.0.\n",
    "        maxth (float, optional): Maximum threshold value. Default is 1.0.\n",
    "        num_thresholds (int, optional): Number of threshold values to search. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the best threshold for each evaluation metric.\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(minth, maxth, num_thresholds)  # Generate candidate thresholds\n",
    "\n",
    "    # Initialize best metrics\n",
    "    best_results = {\n",
    "        \"accuracy\": (0, 0),  # (best_threshold, best_score)\n",
    "        \"f1\": (0, 0),\n",
    "        \"mcc\": (0, 0),\n",
    "        \"auc_roc\": (0, 0),\n",
    "        \"auc_pr\": (0, 0)\n",
    "    }\n",
    "\n",
    "    return best_results \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob[:,1] >= threshold).astype(int)\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "        auc_roc = roc_auc_score(y_true, y_prob[:, 1])\n",
    "        auc_pr = average_precision_score(y_true, y_prob[:, 1])\n",
    "\n",
    "        # Update best threshold for each metric\n",
    "        if acc > best_results[\"accuracy\"][1]:\n",
    "            best_results[\"accuracy\"] = (threshold, acc)\n",
    "        if f1 > best_results[\"f1\"][1]:\n",
    "            best_results[\"f1\"] = (threshold, f1)\n",
    "        if mcc > best_results[\"mcc\"][1]:\n",
    "            best_results[\"mcc\"] = (threshold, mcc)\n",
    "        if auc_roc > best_results[\"auc_roc\"][1]:\n",
    "            best_results[\"auc_roc\"] = (threshold, auc_roc)\n",
    "        if auc_pr > best_results[\"auc_pr\"][1]:\n",
    "            best_results[\"auc_pr\"] = (threshold, auc_pr)\n",
    "    return best_results\n",
    "\n",
    "\n",
    "def learn( npd, X_train, y_train , X_test, t0):\n",
    "    '''\n",
    "    X_train n1, d \n",
    "\n",
    "    C: n * n * d \n",
    "    '''\n",
    "    t1 = time.time()\n",
    "    null_point_X = (sp(X_train).dot(sp(npd))).toarray()\n",
    "    null_point_X_test = (sp(X_test).dot(sp(npd))).toarray()  \n",
    "\n",
    "    train_score_tmp = distance_vector(null_point_X, null_point_X)\n",
    "    for i in range(len(train_score_tmp)):\n",
    "        train_score_tmp[i , i] = 1e9                                                      \n",
    "    train_score = np.amin(train_score_tmp, axis=1)\n",
    "    \n",
    "    y_score = minimum_distance(null_point_X_test, null_point_X)\n",
    "    y_proba = np.zeros((len(y_score), 2))\n",
    "    y_proba[:, 1] = np.minimum(y_score / np.max(train_score), 1)                         \n",
    "    y_proba[:, 0] = 1 - y_proba[:, 1]                                                     # Probability for class 0\n",
    "    y_proba = np.nan_to_num(y_proba, nan=1.0)\n",
    "    y_predict = (y_proba[:, 1] > 0.001).astype(int) \n",
    "    t2 = time.time()\n",
    "    print(\"...............................Timing Model................................\")\n",
    "    print(\"Time train:\", t1-t0)\n",
    "    print(\"Time test:\" , t2-t1)\n",
    "    return y_proba, t1, t2  \n",
    "    \n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "def Model_evaluating(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    Evaluate the model using threshold derived from ROC curve (Youden’s J statistic).\n",
    "    \n",
    "    Args:\n",
    "        y_true (ndarray): True labels (binary: 0 or 1).\n",
    "        y_scores (ndarray): Predicted probabilities for each class (2D array).\n",
    "        \n",
    "    Returns:\n",
    "        list: List of evaluation metrics [AUC, AUCPR, Accuracy, MCC, F1, Precision, Recall].\n",
    "    \"\"\"\n",
    "    print(\"..............................Report Parameter...............................\")\n",
    "    \n",
    "    # Lấy xác suất cho lớp dương (lớp 1)\n",
    "    y_prob = y_scores[:, 1]\n",
    "    \n",
    "    # Tính ROC và threshold tối ưu theo Youden’s J statistic\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    j_scores = tpr - fpr\n",
    "    optimal_idx = np.argmax(j_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    print(\"Optimal threshold (Youden's J):\", optimal_threshold)\n",
    "\n",
    "    # Dự đoán nhãn với threshold tối ưu\n",
    "    y_predict_optimal = (y_prob >= optimal_threshold).astype(int)\n",
    "\n",
    "    # Tính các chỉ số đánh giá\n",
    "    mcc = matthews_corrcoef(y_true, y_predict_optimal)\n",
    "    f1 = f1_score(y_true, y_predict_optimal)\n",
    "    ppv = precision_score(y_true, y_predict_optimal)\n",
    "    recall = recall_score(y_true, y_predict_optimal)\n",
    "    accuracy = accuracy_score(y_true, y_predict_optimal)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    aucpr = average_precision_score(y_true, y_prob)\n",
    "    \n",
    "    # In ra các kết quả\n",
    "    print(\"AUCROC:\", auc * 100)\n",
    "    print(\"AUCPR:\", aucpr * 100)\n",
    "    print(\"Accuracy:\", accuracy * 100)\n",
    "    print(\"MCC:\", mcc)\n",
    "    print(\"F1 score:\", f1)\n",
    "    print(\"PPV (Precision):\", ppv)\n",
    "    print(\"TPR (Recall):\", recall)\n",
    "\n",
    "    return y_predict_optimal  # [auc * 100, aucpr * 100, accuracy * 100, mcc, f1, ppv, recall]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train, X_test, y_test = preprocess_data_OC(df2, df1)\n",
    "\n",
    "# X_total = np.vstack((X_train, X_test))\n",
    "# unique_rows = np.unique(X_total, axis=0)\n",
    "\n",
    "# print(\"Số dòng trong X_total:\", X_total.shape[0])   # Output: 5\n",
    "# print(\"Số dòng khác nhau:\", unique_rows.shape[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "columns = [\"scaler\",\"nCluster\", 'noise_percentage', \"AUCROC\", \"AUCPR\", \"Accuracy\", \"MCC\", \"F1 Score\",\n",
    "           \"Precision\", \"Recall\", \"Time Train\", \"Time Test\"]\n",
    "\n",
    "import cProfile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def function(df1, df2, scaler, noise, ncluster):\n",
    "    X_train0, y_train0 = preprocess_data_noise(df1, noise)\n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"mean\") \n",
    "    X_train0[np.isinf(X_train0)] = np.nan  \n",
    "    X_train0 = imputer.fit_transform(X_train0)\n",
    "\n",
    "    X_train, y_train = cluster_kmeans(X_train0, ncluster)\n",
    "\n",
    "\n",
    "    t5 = time.time()     \n",
    "    npd = calculate_NPD(X_train, y_train)\n",
    "    t4 = time.time() \n",
    "\n",
    "    print(\" time to calculate NPD: \" , t4 - t5)  \n",
    "\n",
    "    return 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating synthetic data for 1000 samples and 10 features...\n",
      "<class 'numpy.ndarray'>\n",
      "..............................Data Overview................................\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "Begin calculating NPD --------------\n",
      "shape of X (10, 346)\n",
      "rank sw st : 9 9\n",
      "rank sw st : 9 9\n",
      "N = 346   d = 10    c = 1\n",
      "P_w : d x N = (10, 346)\n",
      "P_t : d x N = (10, 346)\n",
      "S_w : d x d = (10, 10)\n",
      "S_t : d x d = (10, 10)\n",
      "Q   : d x N = (10, 10)\n",
      "B   : N x L = (10, 1)\n",
      "W   : d x L = (10, 1)\n",
      " time to calculate NPD:  0.0012509822845458984\n",
      "Done!\n",
      "\n",
      "Generating synthetic data for 1000 samples and 100 features...\n",
      "<class 'numpy.ndarray'>\n",
      "..............................Data Overview................................\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "Begin calculating NPD --------------\n",
      "shape of X (100, 343)\n",
      "rank sw st : 90 90\n",
      "rank sw st : 90 90\n",
      "N = 343   d = 100    c = 1\n",
      "P_w : d x N = (100, 343)\n",
      "P_t : d x N = (100, 343)\n",
      "S_w : d x d = (100, 100)\n",
      "S_t : d x d = (100, 100)\n",
      "Q   : d x N = (100, 100)\n",
      "B   : N x L = (100, 10)\n",
      "W   : d x L = (100, 10)\n",
      " time to calculate NPD:  0.1366736888885498\n",
      "Done!\n",
      "\n",
      "Generating synthetic data for 1000 samples and 1000 features...\n",
      "<class 'numpy.ndarray'>\n",
      "..............................Data Overview................................\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "Begin calculating NPD --------------\n",
      "shape of X (1000, 346)\n",
      "rank sw st : 345 345\n",
      "rank sw st : 345 345\n",
      "N = 346   d = 1000    c = 1\n",
      "P_w : d x N = (1000, 346)\n",
      "P_t : d x N = (1000, 346)\n",
      "S_w : d x d = (1000, 1000)\n",
      "S_t : d x d = (1000, 1000)\n",
      "Q   : d x N = (1000, 346)\n",
      "B   : N x L = (346, 1)\n",
      "W   : d x L = (1000, 1)\n",
      " time to calculate NPD:  1.1709060668945312\n",
      "Done!\n",
      "\n",
      "Generating synthetic data for 1000 samples and 10000 features...\n",
      "<class 'numpy.ndarray'>\n",
      "..............................Data Overview................................\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "Begin calculating NPD --------------\n",
      "shape of X (10000, 358)\n",
      "rank sw st : 357 357\n",
      "rank sw st : 357 357\n",
      "N = 358   d = 10000    c = 1\n",
      "P_w : d x N = (10000, 358)\n",
      "P_t : d x N = (10000, 358)\n",
      "S_w : d x d = (10000, 10000)\n",
      "S_t : d x d = (10000, 10000)\n",
      "Q   : d x N = (10000, 358)\n",
      "B   : N x L = (358, 1)\n",
      "W   : d x L = (10000, 1)\n",
      " time to calculate NPD:  645.3335394859314\n",
      "Done!\n",
      "\n",
      "Generating synthetic data for 10000 samples and 10 features...\n",
      "<class 'numpy.ndarray'>\n",
      "..............................Data Overview................................\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "Begin calculating NPD --------------\n",
      "shape of X (10, 3498)\n",
      "rank sw st : 9 9\n",
      "rank sw st : 9 9\n",
      "N = 3498   d = 10    c = 1\n",
      "P_w : d x N = (10, 3498)\n",
      "P_t : d x N = (10, 3498)\n",
      "S_w : d x d = (10, 10)\n",
      "S_t : d x d = (10, 10)\n",
      "Q   : d x N = (10, 10)\n",
      "B   : N x L = (10, 1)\n",
      "W   : d x L = (10, 1)\n",
      " time to calculate NPD:  0.009410381317138672\n",
      "Done!\n",
      "\n",
      "Generating synthetic data for 10000 samples and 100 features...\n",
      "<class 'numpy.ndarray'>\n",
      "..............................Data Overview................................\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "Begin calculating NPD --------------\n",
      "shape of X (100, 3501)\n",
      "rank sw st : 90 90\n",
      "rank sw st : 90 90\n",
      "N = 3501   d = 100    c = 1\n",
      "P_w : d x N = (100, 3501)\n",
      "P_t : d x N = (100, 3501)\n",
      "S_w : d x d = (100, 100)\n",
      "S_t : d x d = (100, 100)\n",
      "Q   : d x N = (100, 100)\n",
      "B   : N x L = (100, 10)\n",
      "W   : d x L = (100, 10)\n",
      " time to calculate NPD:  0.23290395736694336\n",
      "Done!\n",
      "\n",
      "Generating synthetic data for 10000 samples and 1000 features...\n",
      "<class 'numpy.ndarray'>\n",
      "..............................Data Overview................................\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "Begin calculating NPD --------------\n",
      "shape of X (1000, 3522)\n",
      "rank sw st : 900 900\n",
      "rank sw st : 900 900\n",
      "N = 3522   d = 1000    c = 1\n",
      "P_w : d x N = (1000, 3522)\n",
      "P_t : d x N = (1000, 3522)\n",
      "S_w : d x d = (1000, 1000)\n",
      "S_t : d x d = (1000, 1000)\n",
      "Q   : d x N = (1000, 1000)\n",
      "B   : N x L = (1000, 100)\n",
      "W   : d x L = (1000, 100)\n",
      " time to calculate NPD:  3.3218934535980225\n",
      "Done!\n",
      "\n",
      "Generating synthetic data for 10000 samples and 10000 features...\n",
      "<class 'numpy.ndarray'>\n",
      "..............................Data Overview................................\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "Begin calculating NPD --------------\n",
      "shape of X (10000, 3492)\n",
      "rank sw st : 3491 3491\n",
      "rank sw st : 3491 3491\n",
      "N = 3492   d = 10000    c = 1\n",
      "P_w : d x N = (10000, 3492)\n",
      "P_t : d x N = (10000, 3492)\n",
      "S_w : d x d = (10000, 10000)\n",
      "S_t : d x d = (10000, 10000)\n",
      "Q   : d x N = (10000, 3492)\n",
      "B   : N x L = (3492, 1)\n",
      "W   : d x L = (10000, 1)\n",
      " time to calculate NPD:  732.6733946800232\n",
      "Done!\n",
      "\n",
      "Generating synthetic data for 100000 samples and 10 features...\n",
      "<class 'numpy.ndarray'>\n",
      "..............................Data Overview................................\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "Begin calculating NPD --------------\n",
      "shape of X (10, 35083)\n",
      "rank sw st : 9 9\n",
      "rank sw st : 9 9\n",
      "N = 35083   d = 10    c = 1\n",
      "P_w : d x N = (10, 35083)\n",
      "P_t : d x N = (10, 35083)\n",
      "S_w : d x d = (10, 10)\n",
      "S_t : d x d = (10, 10)\n",
      "Q   : d x N = (10, 10)\n",
      "B   : N x L = (10, 1)\n",
      "W   : d x L = (10, 1)\n",
      " time to calculate NPD:  0.0335230827331543\n",
      "Done!\n",
      "\n",
      "Generating synthetic data for 100000 samples and 100 features...\n",
      "<class 'numpy.ndarray'>\n",
      "..............................Data Overview................................\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "Begin calculating NPD --------------\n",
      "shape of X (100, 34910)\n",
      "rank sw st : 90 90\n",
      "rank sw st : 90 90\n",
      "N = 34910   d = 100    c = 1\n",
      "P_w : d x N = (100, 34910)\n",
      "P_t : d x N = (100, 34910)\n",
      "S_w : d x d = (100, 100)\n",
      "S_t : d x d = (100, 100)\n",
      "Q   : d x N = (100, 100)\n",
      "B   : N x L = (100, 10)\n",
      "W   : d x L = (100, 10)\n",
      " time to calculate NPD:  1.4780604839324951\n",
      "Done!\n",
      "\n",
      "Generating synthetic data for 100000 samples and 1000 features...\n",
      "<class 'numpy.ndarray'>\n",
      "..............................Data Overview................................\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "Begin calculating NPD --------------\n",
      "shape of X (1000, 34878)\n",
      "rank sw st : 900 900\n",
      "rank sw st : 900 900\n",
      "N = 34878   d = 1000    c = 1\n",
      "P_w : d x N = (1000, 34878)\n",
      "P_t : d x N = (1000, 34878)\n",
      "S_w : d x d = (1000, 1000)\n",
      "S_t : d x d = (1000, 1000)\n",
      "Q   : d x N = (1000, 1000)\n",
      "B   : N x L = (1000, 100)\n",
      "W   : d x L = (1000, 100)\n",
      " time to calculate NPD:  19.21705198287964\n",
      "Done!\n",
      "\n",
      "Generating synthetic data for 100000 samples and 10000 features...\n",
      "<class 'numpy.ndarray'>\n",
      "..............................Data Overview................................\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "Begin calculating NPD --------------\n",
      "shape of X (10000, 35000)\n",
      "rank sw st : 9000 9000\n",
      "rank sw st : 9000 9000\n",
      "N = 35000   d = 10000    c = 1\n",
      "P_w : d x N = (10000, 35000)\n",
      "P_t : d x N = (10000, 35000)\n",
      "S_w : d x d = (10000, 10000)\n",
      "S_t : d x d = (10000, 10000)\n",
      "Q   : d x N = (10000, 10000)\n",
      "B   : N x L = (10000, 1000)\n",
      "W   : d x L = (10000, 1000)\n",
      " time to calculate NPD:  2959.124404191971\n",
      "Done!\n",
      "\n",
      "Generating synthetic data for 1000000 samples and 10 features...\n",
      "<class 'numpy.ndarray'>\n",
      "..............................Data Overview................................\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "Begin calculating NPD --------------\n",
      "shape of X (10, 349930)\n",
      "rank sw st : 9 9\n",
      "rank sw st : 9 9\n",
      "N = 349930   d = 10    c = 1\n",
      "P_w : d x N = (10, 349930)\n",
      "P_t : d x N = (10, 349930)\n",
      "S_w : d x d = (10, 10)\n",
      "S_t : d x d = (10, 10)\n",
      "Q   : d x N = (10, 10)\n",
      "B   : N x L = (10, 1)\n",
      "W   : d x L = (10, 1)\n",
      " time to calculate NPD:  0.49156761169433594\n",
      "Done!\n",
      "\n",
      "Generating synthetic data for 1000000 samples and 100 features...\n",
      "<class 'numpy.ndarray'>\n",
      "..............................Data Overview................................\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "Begin calculating NPD --------------\n",
      "shape of X (100, 349934)\n",
      "rank sw st : 90 90\n",
      "rank sw st : 90 90\n",
      "N = 349934   d = 100    c = 1\n",
      "P_w : d x N = (100, 349934)\n",
      "P_t : d x N = (100, 349934)\n",
      "S_w : d x d = (100, 100)\n",
      "S_t : d x d = (100, 100)\n",
      "Q   : d x N = (100, 100)\n",
      "B   : N x L = (100, 10)\n",
      "W   : d x L = (100, 10)\n",
      " time to calculate NPD:  18.848007202148438\n",
      "Done!\n",
      "\n",
      "Generating synthetic data for 1000000 samples and 1000 features...\n",
      "<class 'numpy.ndarray'>\n",
      "..............................Data Overview................................\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "Begin calculating NPD --------------\n",
      "shape of X (1000, 349558)\n",
      "rank sw st : 900 900\n",
      "rank sw st : 900 900\n",
      "N = 349558   d = 1000    c = 1\n",
      "P_w : d x N = (1000, 349558)\n",
      "P_t : d x N = (1000, 349558)\n",
      "S_w : d x d = (1000, 1000)\n",
      "S_t : d x d = (1000, 1000)\n",
      "Q   : d x N = (1000, 1000)\n",
      "B   : N x L = (1000, 100)\n",
      "W   : d x L = (1000, 100)\n",
      " time to calculate NPD:  232.3245677947998\n",
      "Done!\n",
      "\n",
      "Generating synthetic data for 1000000 samples and 10000 features...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 74.5 GiB for an array with shape (1000000, 10000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m weights \u001b[38;5;241m=\u001b[39m [ratio_0, ratio_1]\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Sinh dữ liệu\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mmake_classification\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_informative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# phần đặc trưng thực sự\u001b[39;49;00m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_redundant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_repeated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflip_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_sep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[1;32m     57\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# df_synthetic = pd.DataFrame(X)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# df_synthetic['label'] = y\u001b[39;00m\n\u001b[1;32m     62\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/datasets/_samples_generator.py:249\u001b[0m, in \u001b[0;36mmake_classification\u001b[0;34m(n_samples, n_features, n_informative, n_redundant, n_repeated, n_classes, n_clusters_per_class, weights, flip_y, class_sep, hypercube, shift, scale, shuffle, random_state)\u001b[0m\n\u001b[1;32m    246\u001b[0m     n_samples_per_cluster[i \u001b[38;5;241m%\u001b[39m n_clusters] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Initialize X and y\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(n_samples, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Build the polytope whose vertices become cluster centroids\u001b[39;00m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 74.5 GiB for an array with shape (1000000, 10000) and data type float64"
     ]
    }
   ],
   "source": [
    "# import cProfile\n",
    "import pstats    \n",
    "\n",
    "# dataset_prefixes =  ['N_BaIoT_dataloader.csv']\n",
    "dataset_prefixes =  [ 'BoTIoT', 'CICIoT2023','N_BaIoT', 'ToNIoT' ]\n",
    "\n",
    "sample_sizes = [10**3, 10**4, 10**5, 10**6]\n",
    "feature_sizes = [10, 100, 1000, 10000]\n",
    "\n",
    "setting = { # C means the number of cluster , Sc is scaler   \n",
    "    \"BoTIoT\": { \n",
    "        \"C\" : 1, \n",
    "        \"Sc\" : \"Normalizer\" \n",
    "    }, \n",
    "    \"CICIoT2023\": {\n",
    "        \"C\" : 1, \n",
    "        \"Sc\" : \"MinMaxScaler\" \n",
    "    }, \n",
    "    \"N_BaIoT\": {\n",
    "        \"C\": 1, \n",
    "        \"Sc\": \"MinMaxScaler\"\n",
    "    },\n",
    "    \"ToNIoT\": {\n",
    "        \"C\": 1, \n",
    "        \"Sc\": \"QuantileTransformer\" \n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "for n_samples in sample_sizes:\n",
    "    for n_features in feature_sizes:\n",
    "        print(f\"\\nGenerating synthetic data for {n_samples} samples and {n_features} features...\")\n",
    "\n",
    "        train_file = f'Datascaled/NoiseOCData/Train_Normalizer_data_BoTIoT.csv'\n",
    "        df_train = pd.read_csv(train_file).dropna()\n",
    "        \n",
    "        # Lấy tỷ lệ nhãn từ df_train\n",
    "        # label_ratio = df_train['label'].value_counts(normalize=True)\n",
    "        label_ratio = df_train.iloc[:, :-1].value_counts(normalize=True)\n",
    "        ratio_0 = label_ratio.get(0, 0.5)\n",
    "        ratio_1 = label_ratio.get(1, 0.5)\n",
    "\n",
    "        weights = [ratio_0, ratio_1]\n",
    "\n",
    "        # Sinh dữ liệu\n",
    "        X, y = make_classification(\n",
    "            n_samples=n_samples,\n",
    "            n_features=n_features,\n",
    "            n_informative=int(n_features * 0.6),  # phần đặc trưng thực sự\n",
    "            n_redundant=int(n_features * 0.1),\n",
    "            n_repeated=0,\n",
    "            n_classes=2,\n",
    "            weights=weights,\n",
    "            flip_y=0.01,\n",
    "            class_sep=1.0,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # df_synthetic = pd.DataFrame(X)\n",
    "        # df_synthetic['label'] = y\n",
    "\n",
    "        y = y.reshape(-1, 1)\n",
    "\n",
    "        df_synthetic = np.hstack((X, y))  # shape sẽ là (n_samples, n_features + 1)\n",
    "        df_train_new, df_test_new = train_test_split(df_synthetic, test_size=0.3, random_state=42)\n",
    "\n",
    "        print( type(df_train_new) ) \n",
    "        try:\n",
    "            function(df_train_new, df_test_new, \"Normalizer\" , noise=0, ncluster=1)\n",
    "            print(\"Done!\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed on this setting:\", e)\n",
    "            \n",
    "# for prefix in dataset_prefixes:\n",
    "#     print(f\"\\n{'-'*50}\\nRunning for {prefix}\\n{'-'*50}\")\n",
    "    \n",
    "#     scaler = Normalizer\n",
    "#     ncluster = 1\n",
    "    \n",
    "\n",
    "#     # Tải dữ liệu\n",
    "#     train_file = f'Datascaled/NoiseOCData/Train_{scaler}_data_{prefix}.csv'\n",
    "#     test_file = f'Datascaled/NoiseOCData/Test_{scaler}_data_{prefix}.csv'\n",
    "\n",
    "#     df_train = pd.read_csv(train_file).dropna()\n",
    "#     df_test = pd.read_csv(test_file).dropna()\n",
    "\n",
    "#     df_full = pd.concat([df_train, df_test], ignore_index=True)\n",
    "#     df_train_new, df_test_new = train_test_split(df_full, test_size=0.3, random_state=42)\n",
    "\n",
    "#     y_true, y_pred = function(df_train_new, df_test_new, scaler, noise=0, ncluster=ncluster)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
