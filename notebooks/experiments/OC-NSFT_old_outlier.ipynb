{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (roc_auc_score, precision_score, average_precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error, roc_curve, auc, classification_report,auc,confusion_matrix,matthews_corrcoef)\n",
    "from sklearn import logger\n",
    "from sklearn.datasets import make_blobs,make_multilabel_classification\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import KernelCenterer,LabelEncoder, MinMaxScaler, Normalizer, QuantileTransformer, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import scipy as sp\n",
    "from scipy.linalg import svd,null_space\n",
    "import os\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.cluster import KMeans,AgglomerativeClustering,SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.sparse import csr_matrix as sp\n",
    "import math\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.spatial.distance import cdist\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Import 3D plotting tools\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hàm\tĐộ phức tạp tính toán\tBộ nhớ cần\n",
    "# preprocess_data_OC\tO(n⋅d²)\tO(n⋅d²)\n",
    "# clusterr\tO(n⋅d⋅k⋅iters)\tO(n⋅d)\n",
    "# nullspace\tO(d³)\tO(d²)\n",
    "# gram_schmidt\tO(d³)\tO(d²)\n",
    "# minimum_distance\tO(n⋅m⋅d)\tO(n⋅m)\n",
    "# distance_vector\tO(n⋅m⋅d)\tO(n⋅m)\n",
    "# calculate_NPD\tO(d³)\tO(d²)\n",
    "# learn\tO(n²⋅d)\tO(n²)\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "eps = 1e-9\n",
    "alpha = 0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data_noise(train_data, test_data, noise_percentage=0):\n",
    "  \n",
    "    print(\"..............................Data Overview................................\")\n",
    "    print(\"Train Data Shape:\", train_data.shape)\n",
    "    print(\"Test Data Shape:\", test_data.shape)\n",
    "    \n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    X_train_total = train_data.iloc[:, :-1].to_numpy()\n",
    "    y_train_total = train_data.iloc[:, -1].to_numpy()\n",
    "\n",
    "    # Separate the samples with label 0\n",
    "    X_train = X_train_total[y_train_total == 0]\n",
    "    y_train = y_train_total[y_train_total == 0]\n",
    "\n",
    "    print(\"Train Data Labels [0]:\", np.unique(y_train))\n",
    "\n",
    "    # Calculate how many samples to add noise to based on the provided percentage\n",
    "    n_samples = X_train.shape[0]\n",
    "    noise_samples_count = int(n_samples * (noise_percentage / 100))\n",
    "\n",
    "    # Get the samples with label 1 (for generating noise)\n",
    "    X_train_noise = X_train_total[y_train_total == 1]\n",
    "    \n",
    "    # Randomly select noise_samples_count from X_train_noise\n",
    "    noisy_indices = np.random.choice(X_train_noise.shape[0], size=noise_samples_count, replace=False)\n",
    "    X_train_noise = X_train_noise[noisy_indices]\n",
    "    \n",
    "    # Add the noisy samples to the training set\n",
    "    X_train = np.vstack((X_train, X_train_noise))\n",
    "    y_train = np.concatenate((y_train, np.ones(X_train_noise.shape[0])))\n",
    "    print(y_train) \n",
    "    \n",
    "    # Prepare test data\n",
    "    X_test = test_data.iloc[:, :-1].to_numpy()\n",
    "    y_test = test_data.iloc[:, -1].to_numpy()\n",
    "\n",
    "    # Print the new size of training data\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_features = X_train.shape[1]\n",
    "    print(\"Number of samples after adding noise:\", n_samples)\n",
    "    print(\"Number of features:\", n_features)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def cluster_kmeans(data, initial_k):\n",
    "    print(\"Starting K-Means clustering...\")\n",
    "\n",
    "    # Thực hiện K-Means clustering với số cụm initial_k\n",
    "    kmeans = KMeans(n_clusters=initial_k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(data)\n",
    "\n",
    "    # Gán nhãn cụm vào biến labels\n",
    "    labels = cluster_labels\n",
    "\n",
    "    # Sắp xếp dữ liệu theo nhãn cụm\n",
    "    sorted_indices = np.argsort(labels)\n",
    "    sorted_data = data[sorted_indices]\n",
    "    sorted_labels = labels[sorted_indices]\n",
    "\n",
    "    print(\"Final number of clusters:\", len(np.unique(sorted_labels)))\n",
    "    return sorted_data, sorted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nullspace(A):\n",
    "    _, s, vh = np.linalg.svd(A)\n",
    "    null_mask = np.isclose(s, 0)\n",
    "    null_space = vh[null_mask].T\n",
    "    return null_space\n",
    "\n",
    "\n",
    "def minimum_distance( A, B):\n",
    "        \"\"\"\n",
    "        Compute the minimum Euclidean distance from each point in A to all points in B.\n",
    "        \n",
    "        Parameters:\n",
    "        - A (ndarray): Set of points (N_A, d).\n",
    "        - B (ndarray): Set of points (N_B, d).\n",
    "        \n",
    "        Returns:\n",
    "        - min_distances (ndarray): Minimum distances from each point in A to the nearest point in B.\n",
    "        \"\"\"\n",
    "        A = np.asarray(A)\n",
    "        B = np.asarray(B)\n",
    "        \n",
    "        # Initialize an array for storing minimum distances\n",
    "        min_distances = np.empty(A.shape[0], dtype=np.float64)\n",
    "        \n",
    "        # Iterate over each point in A and calculate minimum distance to points in B\n",
    "        for i, a in enumerate(A):\n",
    "            distances = cdist([a], B, metric='euclidean')  # Compute all pairwise distances\n",
    "            min_distances[i] = np.min(distances)  # Store minimum distance\n",
    "        \n",
    "        return min_distances   \n",
    "\n",
    "def distance_vector(point_X, point_Y):\n",
    "    \"\"\"\n",
    "    Calculate pairwise Euclidean distance between two sets of points.\n",
    "    \n",
    "    Args:\n",
    "        point_X (ndarray): Array of shape (N_train, d) where N_train is the number of training samples and d is the number of features.\n",
    "        point_Y (ndarray): Array of shape (N_test, d) where N_test is the number of test samples and d is the number of features.\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: Distance matrix of shape (N_test, N_train) containing Euclidean distances between each pair of points.\n",
    "    \"\"\"\n",
    "    print( 'Complexity of calculate: ', point_X )\n",
    "    # Compute squared norms for each point\n",
    "    norm_X = np.sum(point_X**2, axis=1)  # (N_train,)\n",
    "    norm_Y = np.sum(point_Y**2, axis=1)  # (N_test,)\n",
    "    \n",
    "    # Compute the dot product between the two sets of points\n",
    "    dot_product = np.dot(point_Y, point_X.T)  # (N_test, N_train)\n",
    "    \n",
    "    # Apply Euclidean distance formula\n",
    "    distance = np.sqrt(abs(norm_Y[:, np.newaxis] + norm_X[np.newaxis, :] - 2 * dot_product))\n",
    "    return distance\n",
    "\n",
    "\n",
    "\n",
    "def calculate_NPD(X,y):\n",
    "    print(\"Begin calculating NPD --------------\")\n",
    "    X = X.T\n",
    "    print('shape of X', X.shape)\n",
    "    c = len(np.unique(y))  # Số lớp\n",
    "    d, N = X.shape  # Số đặc trưng và số mẫu\n",
    "    # Tính trung bình toàn cục và tạo ma trận P_t với zero-mean\n",
    "    mean_total = np.mean(X, axis=1, keepdims=True)\n",
    "    P_t = X - mean_total  # P_t là ma trận zero-mean có kích thước d x N\n",
    "    \n",
    "    # Tính P_w cho từng lớp\n",
    "    P_w = np.zeros_like(X)\n",
    "    for i in range(c):\n",
    "        class_mean = np.mean(X[:, y == i], axis=1, keepdims=True)\n",
    "        P_w[:, y == i] = X[:, y == i] - class_mean  # Tạo ma trận P_w có kích thước d x N\n",
    "        \n",
    "    # Tính ma trận phương sai S_w và S_t\n",
    "    S_w = np.dot(P_w, P_w.T) / N  # S_w là d x d\n",
    "    S_t = np.dot(P_t, P_t.T) / N  # S_t là d x d\n",
    "\n",
    "    U, S, Vt = np.linalg.svd(P_t, full_matrices=False)\n",
    "    Q = U\n",
    "    B = nullspace(Q.T @ S_w @ Q)\n",
    "    W = Q @ B  # W có kích thước d x (c - 1)\n",
    "\n",
    "    # In kết quả\n",
    "    print(\"N =\", N,  \"  d =\", d, \"   c =\", c )\n",
    "    print(\"P_w : d x N =\", P_w.shape)  # d x N\n",
    "    print(\"P_t : d x N =\", P_t.shape)  # d x N\n",
    "    print(\"S_w : d x d =\", S_w.shape)  # d x d\n",
    "    print(\"S_t : d x d =\", S_t.shape)  # d x d\n",
    "    print(\"Q   : d x N =\", Q.shape)     # d x r\n",
    "    print(\"B   : N x L =\", B.shape)     # r x L\n",
    "    print(\"W   : d x L =\", W.shape)     # d x L\n",
    "    return W    \n",
    "\n",
    "    \n",
    "\n",
    "def BruteForce_Threshold(y_true, y_prob, minth=0.0, maxth=1.0, num_thresholds=1000):\n",
    "    \"\"\"\n",
    "    Finds the best classification threshold for a binary model using brute force search.\n",
    "\n",
    "    Args:\n",
    "        y_true (ndarray): True labels (0 or 1), shape (n_samples,).\n",
    "        y_prob (ndarray): Predicted probabilities for class 1, shape (n_samples,).\n",
    "        minth (float, optional): Minimum threshold value. Default is 0.0.\n",
    "        maxth (float, optional): Maximum threshold value. Default is 1.0.\n",
    "        num_thresholds (int, optional): Number of threshold values to search. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the best threshold for each evaluation metric.\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(minth, maxth, num_thresholds)  # Generate candidate thresholds\n",
    "\n",
    "    # Initialize best metrics\n",
    "    best_results = {\n",
    "        \"accuracy\": (0, 0),  # (best_threshold, best_score)\n",
    "        \"f1\": (0, 0),\n",
    "        \"mcc\": (0, 0),\n",
    "        \"auc_roc\": (0, 0),\n",
    "        \"auc_pr\": (0, 0)\n",
    "    }\n",
    "\n",
    "    return best_results \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob[:,1] >= threshold).astype(int)\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "        auc_roc = roc_auc_score(y_true, y_prob[:, 1])\n",
    "        auc_pr = average_precision_score(y_true, y_prob[:, 1])\n",
    "\n",
    "        # Update best threshold for each metric\n",
    "        if acc > best_results[\"accuracy\"][1]:\n",
    "            best_results[\"accuracy\"] = (threshold, acc)\n",
    "        if f1 > best_results[\"f1\"][1]:\n",
    "            best_results[\"f1\"] = (threshold, f1)\n",
    "        if mcc > best_results[\"mcc\"][1]:\n",
    "            best_results[\"mcc\"] = (threshold, mcc)\n",
    "        if auc_roc > best_results[\"auc_roc\"][1]:\n",
    "            best_results[\"auc_roc\"] = (threshold, auc_roc)\n",
    "        if auc_pr > best_results[\"auc_pr\"][1]:\n",
    "            best_results[\"auc_pr\"] = (threshold, auc_pr)\n",
    "\n",
    "    return best_results\n",
    "\n",
    "\n",
    "def learn( npd, X_train, y_train , X_test, t0):\n",
    "    '''\n",
    "    X_train n1, d \n",
    "\n",
    "    C: n * n * d \n",
    "    '''\n",
    "    print( \"npd ::: \" , npd.shape ) \n",
    "    t1 = time.time()\n",
    "    null_point_X = (sp(X_train).dot(sp(npd))).toarray()\n",
    "    null_point_X_test = (sp(X_test).dot(sp(npd))).toarray()  \n",
    "\n",
    "    plot_data_2D(null_point_X, y_train, \"Du lieu sau projection\") \n",
    "\n",
    "    train_score_tmp = distance_vector(null_point_X, null_point_X)\n",
    "    for i in range(len(train_score_tmp)):\n",
    "        train_score_tmp[i , i] = 1e9                                                      \n",
    "    train_score = np.amin(train_score_tmp, axis=1)\n",
    "    \n",
    "    y_score = minimum_distance(null_point_X_test, null_point_X)\n",
    "    y_proba = np.zeros((len(y_score), 2))\n",
    "    y_proba[:, 1] = np.minimum(y_score / np.max(train_score), 1)                         \n",
    "    y_proba[:, 0] = 1 - y_proba[:, 1]                                                     # Probability for class 0\n",
    "    \n",
    "    y_proba = np.nan_to_num(y_proba, nan=1.0)\n",
    "\n",
    "    y_predict = (y_proba[:, 1] > 0.2).astype(int) \n",
    "    \n",
    "    \n",
    "    \n",
    "    t2 = time.time()\n",
    "    print(\"...............................Timing Model................................\")\n",
    "    print(\"Time train:\", t1-t0)\n",
    "    print(\"Time test:\" , t2-t1)\n",
    "    return y_proba, y_predict, t1, t2  \n",
    "\n",
    "\n",
    "def Model_evaluating(y_true, y_predict, y_scores):\n",
    "    \"\"\"\n",
    "    Evaluate the model using various metrics such as AUC, AUCPR, Accuracy, MCC, F1-score, Precision, and Recall.\n",
    "    \n",
    "    Args:\n",
    "        y_true (ndarray): True labels.\n",
    "        y_predict (ndarray): Predicted labels.\n",
    "        y_scores (ndarray): Predicted probabilities for each class.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of evaluation metrics.\n",
    "    \"\"\"\n",
    "    print(\"..............................Report Parameter...............................\")\n",
    "    \n",
    "    # Calculate MCC (Matthews Correlation Coefficient)\n",
    "    mcc = matthews_corrcoef(y_true, y_predict)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_true, y_predict)\n",
    "    \n",
    "    # Calculate PPV (Positive Predictive Value) or Precision\n",
    "    ppv = precision_score(y_true, y_predict)\n",
    "    \n",
    "    # Calculate TPR (True Positive Rate) or Recall/Sensitivity\n",
    "    tpr = recall_score(y_true, y_predict)\n",
    "    \n",
    "    # Calculate Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_predict)\n",
    "    \n",
    "    # Calculate AUC (Area Under ROC Curve)\n",
    "    AUC = roc_auc_score(y_true, y_scores[:, 1])\n",
    "    \n",
    "    # Calculate AUCPR (Area Under Precision-Recall Curve)\n",
    "    AUCPR = average_precision_score(y_true, y_scores[:, 1])\n",
    "    \n",
    "    # Print out the evaluation metrics\n",
    "    print(\"AUCROC:\", AUC * 100)\n",
    "    print(\"AUCPR:\", AUCPR * 100)\n",
    "    print(\"Accuracy:\", accuracy * 100)\n",
    "    print(\"MCC:\", mcc)\n",
    "    print(\"F1 score:\", f1)\n",
    "    print(\"PPV (Precision):\", ppv)\n",
    "    print(\"TPR (Recall):\", tpr)\n",
    "\n",
    "    return [AUC * 100 if AUC else None, AUCPR * 100 if AUCPR else None,\n",
    "            accuracy * 100, mcc, f1, ppv, tpr]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train, X_test, y_test = preprocess_data_OC(df2, df1)\n",
    "\n",
    "# X_total = np.vstack((X_train, X_test))\n",
    "# unique_rows = np.unique(X_total, axis=0)\n",
    "\n",
    "# print(\"Số dòng trong X_total:\", X_total.shape[0])   # Output: 5\n",
    "# print(\"Số dòng khác nhau:\", unique_rows.shape[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files from the GMM-nfst folder\n",
    "\n",
    "# columns = [\"Dataset\",\"Model\",  \"scaled\", \"noise_percentage\", \"AUCROC\", \"AUCPR\", \"Accuracy\", \"MCC\", \"F1 Score\",\n",
    "#            \"Precision\", \"Recall\", \"Time Train\", \"Time Test\"]\n",
    "\n",
    "columns = [\"outlier_mode\", 'scaler', \"nCluster\", \"AUCROC\", \"AUCPR\", \"Accuracy\", \"MCC\", \"F1 Score\",\n",
    "           \"Precision\", \"Recall\", \"Time Train\", \"Time Test\"]\n",
    "# Now you can use df1 and df2 as DataFrames\n",
    "\n",
    "#CIC_IoT2023_1000.csv\n",
    "#N_BaIoT_1000.csv\n",
    "#BoT_IoT_1000.csv\n",
    "\n",
    "import cProfile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# def plot_data_2D(X, labels, title=\"Interactive 3D PCA Plot\"):\n",
    "#     \"\"\" Vẽ dữ liệu với PCA và tạo đồ thị 3D có thể tương tác bằng Plotly \"\"\"\n",
    "\n",
    "#     # Ensure that n_components ≤ min(n_samples, n_features)\n",
    "#     n_components = min(3, X.shape[0], X.shape[1])\n",
    "#     if n_components < 3:\n",
    "#         print(f\"⚠️ Warning: Cannot plot 3D, because n_components={n_components}. Skipping...\")\n",
    "#         return\n",
    "    \n",
    "#     # Perform PCA\n",
    "#     pca = PCA(n_components=3)\n",
    "#     X_3D = pca.fit_transform(X)\n",
    "\n",
    "#     # Convert labels to a NumPy array\n",
    "#     labels = np.array(labels)\n",
    "\n",
    "#     # Create a DataFrame for better visualization\n",
    "#     import pandas as pd\n",
    "#     df = pd.DataFrame(X_3D, columns=['PCA 1', 'PCA 2', 'PCA 3'])\n",
    "#     df['Class'] = labels\n",
    "\n",
    "#     # Create an interactive 3D scatter plot\n",
    "#     fig = px.scatter_3d(df, x='PCA 1', y='PCA 2', z='PCA 3', \n",
    "#                          color=df['Class'].astype(str),  # Color by class\n",
    "#                          title=title, labels={'color': 'Class'},\n",
    "#                          opacity=0.8)\n",
    "\n",
    "#     fig.update_traces(marker=dict(size=5))  # Adjust marker size\n",
    "#     fig.show()\n",
    "\n",
    "def plot_data_2D(X, labels, title=\"Dữ liệu gốc trước khi biến đổi\"):\n",
    "    \"\"\" Vẽ dữ liệu với PCA để giảm xuống 2D \"\"\"\n",
    "    # return 0 \n",
    "    pca = PCA(n_components=3)\n",
    "    X_2D = pca.fit_transform(X)  # Chuyển về dạng (100, 2)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    labels = np.array(labels)  \n",
    "    for i in np.unique(labels):\n",
    "        plt.scatter(X_2D[labels == i, 0], X_2D[labels == i, 1], label=f\"Class {i}\", alpha=0.7)\n",
    "\n",
    "    plt.xlabel(\"PCA 1\")\n",
    "    plt.ylabel(\"PCA 2\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def function(df1,df2, outlier_mode, scaler):\n",
    "\n",
    "    X_train0, y_train0, X_test, y_test = preprocess_data_noise( df1,df2, 0)\n",
    "\n",
    "    for ncluster in range(291,min(400, X_train0.shape[0]), 5000):\n",
    "        t0 = time.time()\n",
    "        X_train , y_train = cluster_kmeans( X_train0 , ncluster )\n",
    "        t5 = time.time() \n",
    "        plot_data_2D(X_train, y_train, \"Du lieu sau clusterr\") \n",
    "        npd = calculate_NPD(X_train, y_train)\n",
    "\n",
    "        t4 = time.time() \n",
    "\n",
    "        print(\" time to calculate NPD: \" , t4 - t5) \n",
    "        \n",
    "        y_proba, y_predict, t1, t2 = learn( npd , X_train, y_train, X_test, y_test)  #,y_predict\n",
    "        \n",
    "        # print(\"...............................Timing Model................................\")\n",
    "        # print(\"Time train:\", t1-t0)\n",
    "        # print(\"Time test:\" , t2-t1)\n",
    "        # print(\"...........................................................................\")\n",
    "        # print(y_predict)\n",
    "        v = Model_evaluating(y_test, y_predict, y_proba)\n",
    "        # best_thresholds = BruteForce_Threshold( y_test, y_proba, 0, 1)  \n",
    "        result = [outlier_mode, scaler] + [ncluster] + v + [t1-t0, t2-t1]\n",
    "        # for metric, (threshold, score) in best_thresholds.items():\n",
    "        #     result = result + [threshold, score ]  \n",
    "\n",
    "        result_df = pd.DataFrame([result], columns=columns)\n",
    "        result_df.to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)\n",
    "        # result_df.to_csv(output_file, mode='a', header=(mark==0), index=False)\n",
    "        \n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "-------- data_N_BaIoT.csv ------------------------------\n",
      "--------------------------------------------------\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_cluster_RobustScaler_data_N_BaIoT.csv or ../../Datascaled/OutlierData/Test_cluster_RobustScaler_data_N_BaIoT.csv: [Errno 2] No such file or directory: '../../Datascaled/OutlierData/Train_cluster_RobustScaler_data_N_BaIoT.csv'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_cluster_QuantileTransformer_data_N_BaIoT.csv or ../../Datascaled/OutlierData/Test_cluster_QuantileTransformer_data_N_BaIoT.csv: [Errno 2] No such file or directory: '../../Datascaled/OutlierData/Train_cluster_QuantileTransformer_data_N_BaIoT.csv'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_cluster_MinMaxScaler_data_N_BaIoT.csv or ../../Datascaled/OutlierData/Test_cluster_MinMaxScaler_data_N_BaIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_N_BaIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_cluster_Normalizer_data_N_BaIoT.csv or ../../Datascaled/OutlierData/Test_cluster_Normalizer_data_N_BaIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_N_BaIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_cluster_StandardScaler_data_N_BaIoT.csv or ../../Datascaled/OutlierData/Test_cluster_StandardScaler_data_N_BaIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_N_BaIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_local_RobustScaler_data_N_BaIoT.csv or ../../Datascaled/OutlierData/Test_local_RobustScaler_data_N_BaIoT.csv: [Errno 2] No such file or directory: '../../Datascaled/OutlierData/Train_local_RobustScaler_data_N_BaIoT.csv'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_local_QuantileTransformer_data_N_BaIoT.csv or ../../Datascaled/OutlierData/Test_local_QuantileTransformer_data_N_BaIoT.csv: [Errno 2] No such file or directory: '../../Datascaled/OutlierData/Train_local_QuantileTransformer_data_N_BaIoT.csv'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_local_MinMaxScaler_data_N_BaIoT.csv or ../../Datascaled/OutlierData/Test_local_MinMaxScaler_data_N_BaIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_N_BaIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_local_Normalizer_data_N_BaIoT.csv or ../../Datascaled/OutlierData/Test_local_Normalizer_data_N_BaIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_N_BaIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_local_StandardScaler_data_N_BaIoT.csv or ../../Datascaled/OutlierData/Test_local_StandardScaler_data_N_BaIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_N_BaIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_global_RobustScaler_data_N_BaIoT.csv or ../../Datascaled/OutlierData/Test_global_RobustScaler_data_N_BaIoT.csv: [Errno 2] No such file or directory: '../../Datascaled/OutlierData/Train_global_RobustScaler_data_N_BaIoT.csv'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_global_QuantileTransformer_data_N_BaIoT.csv or ../../Datascaled/OutlierData/Test_global_QuantileTransformer_data_N_BaIoT.csv: [Errno 2] No such file or directory: '../../Datascaled/OutlierData/Train_global_QuantileTransformer_data_N_BaIoT.csv'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_global_MinMaxScaler_data_N_BaIoT.csv or ../../Datascaled/OutlierData/Test_global_MinMaxScaler_data_N_BaIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_N_BaIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_global_Normalizer_data_N_BaIoT.csv or ../../Datascaled/OutlierData/Test_global_Normalizer_data_N_BaIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_N_BaIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_global_StandardScaler_data_N_BaIoT.csv or ../../Datascaled/OutlierData/Test_global_StandardScaler_data_N_BaIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_N_BaIoT.csv_modellog.txt'\n",
      "--------------------------------------------------\n",
      "-------- data_ToNIoT.csv ------------------------------\n",
      "--------------------------------------------------\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_cluster_RobustScaler_data_ToNIoT.csv or ../../Datascaled/OutlierData/Test_cluster_RobustScaler_data_ToNIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_ToNIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_cluster_QuantileTransformer_data_ToNIoT.csv or ../../Datascaled/OutlierData/Test_cluster_QuantileTransformer_data_ToNIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_ToNIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_cluster_MinMaxScaler_data_ToNIoT.csv or ../../Datascaled/OutlierData/Test_cluster_MinMaxScaler_data_ToNIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_ToNIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_cluster_Normalizer_data_ToNIoT.csv or ../../Datascaled/OutlierData/Test_cluster_Normalizer_data_ToNIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_ToNIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_cluster_StandardScaler_data_ToNIoT.csv or ../../Datascaled/OutlierData/Test_cluster_StandardScaler_data_ToNIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_ToNIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_local_RobustScaler_data_ToNIoT.csv or ../../Datascaled/OutlierData/Test_local_RobustScaler_data_ToNIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_ToNIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_local_QuantileTransformer_data_ToNIoT.csv or ../../Datascaled/OutlierData/Test_local_QuantileTransformer_data_ToNIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_ToNIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_local_MinMaxScaler_data_ToNIoT.csv or ../../Datascaled/OutlierData/Test_local_MinMaxScaler_data_ToNIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_ToNIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_local_Normalizer_data_ToNIoT.csv or ../../Datascaled/OutlierData/Test_local_Normalizer_data_ToNIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_ToNIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_local_StandardScaler_data_ToNIoT.csv or ../../Datascaled/OutlierData/Test_local_StandardScaler_data_ToNIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_ToNIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_global_RobustScaler_data_ToNIoT.csv or ../../Datascaled/OutlierData/Test_global_RobustScaler_data_ToNIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_ToNIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_global_QuantileTransformer_data_ToNIoT.csv or ../../Datascaled/OutlierData/Test_global_QuantileTransformer_data_ToNIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_ToNIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_global_MinMaxScaler_data_ToNIoT.csv or ../../Datascaled/OutlierData/Test_global_MinMaxScaler_data_ToNIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_ToNIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_global_Normalizer_data_ToNIoT.csv or ../../Datascaled/OutlierData/Test_global_Normalizer_data_ToNIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_ToNIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_global_StandardScaler_data_ToNIoT.csv or ../../Datascaled/OutlierData/Test_global_StandardScaler_data_ToNIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_ToNIoT.csv_modellog.txt'\n",
      "--------------------------------------------------\n",
      "-------- data_CICIoT2023.csv ------------------------------\n",
      "--------------------------------------------------\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_cluster_RobustScaler_data_CICIoT2023.csv or ../../Datascaled/OutlierData/Test_cluster_RobustScaler_data_CICIoT2023.csv: [Errno 2] No such file or directory: '../../Datascaled/OutlierData/Train_cluster_RobustScaler_data_CICIoT2023.csv'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_cluster_QuantileTransformer_data_CICIoT2023.csv or ../../Datascaled/OutlierData/Test_cluster_QuantileTransformer_data_CICIoT2023.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_CICIoT2023.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_cluster_MinMaxScaler_data_CICIoT2023.csv or ../../Datascaled/OutlierData/Test_cluster_MinMaxScaler_data_CICIoT2023.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_CICIoT2023.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_cluster_Normalizer_data_CICIoT2023.csv or ../../Datascaled/OutlierData/Test_cluster_Normalizer_data_CICIoT2023.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_CICIoT2023.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_cluster_StandardScaler_data_CICIoT2023.csv or ../../Datascaled/OutlierData/Test_cluster_StandardScaler_data_CICIoT2023.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_CICIoT2023.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_local_RobustScaler_data_CICIoT2023.csv or ../../Datascaled/OutlierData/Test_local_RobustScaler_data_CICIoT2023.csv: [Errno 2] No such file or directory: '../../Datascaled/OutlierData/Train_local_RobustScaler_data_CICIoT2023.csv'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_local_QuantileTransformer_data_CICIoT2023.csv or ../../Datascaled/OutlierData/Test_local_QuantileTransformer_data_CICIoT2023.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_CICIoT2023.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_local_MinMaxScaler_data_CICIoT2023.csv or ../../Datascaled/OutlierData/Test_local_MinMaxScaler_data_CICIoT2023.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_CICIoT2023.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_local_Normalizer_data_CICIoT2023.csv or ../../Datascaled/OutlierData/Test_local_Normalizer_data_CICIoT2023.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_CICIoT2023.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_local_StandardScaler_data_CICIoT2023.csv or ../../Datascaled/OutlierData/Test_local_StandardScaler_data_CICIoT2023.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_CICIoT2023.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_global_RobustScaler_data_CICIoT2023.csv or ../../Datascaled/OutlierData/Test_global_RobustScaler_data_CICIoT2023.csv: [Errno 2] No such file or directory: '../../Datascaled/OutlierData/Train_global_RobustScaler_data_CICIoT2023.csv'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_global_QuantileTransformer_data_CICIoT2023.csv or ../../Datascaled/OutlierData/Test_global_QuantileTransformer_data_CICIoT2023.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_CICIoT2023.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_global_MinMaxScaler_data_CICIoT2023.csv or ../../Datascaled/OutlierData/Test_global_MinMaxScaler_data_CICIoT2023.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_CICIoT2023.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_global_Normalizer_data_CICIoT2023.csv or ../../Datascaled/OutlierData/Test_global_Normalizer_data_CICIoT2023.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_CICIoT2023.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_global_StandardScaler_data_CICIoT2023.csv or ../../Datascaled/OutlierData/Test_global_StandardScaler_data_CICIoT2023.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_CICIoT2023.csv_modellog.txt'\n",
      "--------------------------------------------------\n",
      "-------- data_BoTIoT.csv ------------------------------\n",
      "--------------------------------------------------\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_cluster_RobustScaler_data_BoTIoT.csv or ../../Datascaled/OutlierData/Test_cluster_RobustScaler_data_BoTIoT.csv: [Errno 2] No such file or directory: '../../Datascaled/OutlierData/Train_cluster_RobustScaler_data_BoTIoT.csv'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_cluster_QuantileTransformer_data_BoTIoT.csv or ../../Datascaled/OutlierData/Test_cluster_QuantileTransformer_data_BoTIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_BoTIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_cluster_MinMaxScaler_data_BoTIoT.csv or ../../Datascaled/OutlierData/Test_cluster_MinMaxScaler_data_BoTIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_BoTIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_cluster_Normalizer_data_BoTIoT.csv or ../../Datascaled/OutlierData/Test_cluster_Normalizer_data_BoTIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_BoTIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_cluster_StandardScaler_data_BoTIoT.csv or ../../Datascaled/OutlierData/Test_cluster_StandardScaler_data_BoTIoT.csv: [Errno 2] No such file or directory: '../../Datascaled/OutlierData/Test_cluster_StandardScaler_data_BoTIoT.csv'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_local_RobustScaler_data_BoTIoT.csv or ../../Datascaled/OutlierData/Test_local_RobustScaler_data_BoTIoT.csv: [Errno 2] No such file or directory: '../../Datascaled/OutlierData/Train_local_RobustScaler_data_BoTIoT.csv'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_local_QuantileTransformer_data_BoTIoT.csv or ../../Datascaled/OutlierData/Test_local_QuantileTransformer_data_BoTIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_BoTIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_local_MinMaxScaler_data_BoTIoT.csv or ../../Datascaled/OutlierData/Test_local_MinMaxScaler_data_BoTIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_BoTIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_local_Normalizer_data_BoTIoT.csv or ../../Datascaled/OutlierData/Test_local_Normalizer_data_BoTIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_BoTIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_local_StandardScaler_data_BoTIoT.csv or ../../Datascaled/OutlierData/Test_local_StandardScaler_data_BoTIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_BoTIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_global_RobustScaler_data_BoTIoT.csv or ../../Datascaled/OutlierData/Test_global_RobustScaler_data_BoTIoT.csv: [Errno 2] No such file or directory: '../../Datascaled/OutlierData/Train_global_RobustScaler_data_BoTIoT.csv'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_global_QuantileTransformer_data_BoTIoT.csv or ../../Datascaled/OutlierData/Test_global_QuantileTransformer_data_BoTIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_BoTIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_global_MinMaxScaler_data_BoTIoT.csv or ../../Datascaled/OutlierData/Test_global_MinMaxScaler_data_BoTIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_BoTIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_global_Normalizer_data_BoTIoT.csv or ../../Datascaled/OutlierData/Test_global_Normalizer_data_BoTIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_BoTIoT.csv_modellog.txt'\n",
      "⚠️ Error processing ../../Datascaled/OutlierData/Train_global_StandardScaler_data_BoTIoT.csv or ../../Datascaled/OutlierData/Test_global_StandardScaler_data_BoTIoT.csv: [Errno 2] No such file or directory: '../../Results/OURMODEL/SCALERS/data_BoTIoT.csv_modellog.txt'\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats    \n",
    "\n",
    "# dataset_prefixes =  ['N_BaIoT_dataloader.csv']\n",
    "# dataset_prefixes =  ['data_CICIoT2023.csv', 'data_N_BaIoT.csv', 'data_BoTIoT.csv']\n",
    "dataset_prefixes =  ['data_N_BaIoT.csv', 'data_ToNIoT.csv', 'data_CICIoT2023.csv', 'data_BoTIoT.csv']\n",
    "# dataset_prefixes =  ['data_CICIoT2023.csv']\n",
    "\n",
    "\n",
    "#  'data_N_BaIoT.csv',  \n",
    "# scaler_names = ['MinMaxScaler']\n",
    "scaler_names = ['RobustScaler',  'QuantileTransformer','MinMaxScaler','Normalizer','StandardScaler' ]\n",
    "# scaler_names = ['MinMaxScaler','StandardScaler','QuantileTransformer','Normalizer']\n",
    "outlier_modes = ['cluster', 'local', 'global']\n",
    "\n",
    "for prefix in dataset_prefixes:\n",
    "    \n",
    "    print(\"-\"*50)\n",
    "    print(\"--------\", prefix , \"-\"*30)\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    #  Structure_Result/NOISEGAU \n",
    "    base_output_file = f\"outlier_{prefix}\" \n",
    "    output_file = base_output_file + \"0.csv\"\n",
    "\n",
    "    # Increment the filename if it already exists\n",
    "    counter = 0\n",
    "    while os.path.exists(output_file):\n",
    "        counter += 1\n",
    "        output_file = f\"{base_output_file}{counter}.csv\"\n",
    "\n",
    "        \n",
    "    for outlier_mode in outlier_modes: \n",
    "        # print(f\"Processing dataset {prefix} with {scaler} scaler ...\")\n",
    "        \n",
    "        # Construct file paths for train and test datasets with 'Train_' and 'Test_' prefixes\n",
    "        for scaler in scaler_names: \n",
    "            train_file = f'../../Datascaled/OutlierData/Train_{outlier_mode}_{scaler}_{prefix}'\n",
    "            test_file = f'../../Datascaled/OutlierData/Test_{outlier_mode}_{scaler}_{prefix}'\n",
    "\n",
    "            # train_file = f'Datascaled/Train_{scaler}_{prefix}'\n",
    "            # test_file = f'Datascaled/Test_{scaler}_{prefix}'\n",
    "\n",
    "            try:\n",
    "                # Load the CSV files\n",
    "                df_train = pd.read_csv(train_file)\n",
    "                df_test = pd.read_csv(test_file)\n",
    "\n",
    "                log_file = f'../../Results/OURMODEL/SCALERS/{prefix}_modellog.txt'\n",
    "                with open(log_file, \"w\") as f:\n",
    "                    profiler = cProfile.Profile()\n",
    "                    profiler.enable()\n",
    "                    \n",
    "                    function(df_train, df_test, outlier_mode, scaler)  # Chạy hàm cần profile\n",
    "                \n",
    "                    profiler.disable()\n",
    "                    stats = pstats.Stats(profiler, stream=f)\n",
    "                    stats.sort_stats(\"cumulative\").print_stats()\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error processing {train_file} or {test_file}: {e}\")\n",
    "                continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
