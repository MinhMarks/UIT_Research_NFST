{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (roc_auc_score, precision_score, average_precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error, roc_curve, auc, classification_report,auc,confusion_matrix,matthews_corrcoef)\n",
    "from sklearn import logger\n",
    "from sklearn.datasets import make_blobs,make_multilabel_classification\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import KernelCenterer,LabelEncoder, MinMaxScaler, Normalizer, QuantileTransformer, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import scipy as sp\n",
    "from scipy.linalg import svd,null_space\n",
    "import os\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.cluster import KMeans,AgglomerativeClustering,SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.sparse import csr_matrix as sp\n",
    "import math\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.spatial.distance import cdist\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Import 3D plotting tools\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hàm\tĐộ phức tạp tính toán\tBộ nhớ cần\n",
    "# preprocess_data_OC\tO(n⋅d²)\tO(n⋅d²)\n",
    "# clusterr\tO(n⋅d⋅k⋅iters)\tO(n⋅d)\n",
    "# nullspace\tO(d³)\tO(d²)\n",
    "# gram_schmidt\tO(d³)\tO(d²)\n",
    "# minimum_distance\tO(n⋅m⋅d)\tO(n⋅m)\n",
    "# distance_vector\tO(n⋅m⋅d)\tO(n⋅m)\n",
    "# calculate_NPD\tO(d³)\tO(d²)\n",
    "# learn\tO(n²⋅d)\tO(n²)\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "alpha = 0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data_noise(train_data, test_data, noise_percentage=10):\n",
    "  \n",
    "    print(\"..............................Data Overview................................\")\n",
    "    print(\"Train Data Shape:\", train_data.shape)\n",
    "    print(\"Test Data Shape:\", test_data.shape)\n",
    "    \n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    X_train_total = train_data.iloc[:, :-1].to_numpy()\n",
    "    y_train_total = train_data.iloc[:, -1].to_numpy()\n",
    "\n",
    "    # Separate the samples with label 0\n",
    "    X_train = X_train_total[y_train_total == 0]\n",
    "    y_train = y_train_total[y_train_total == 0]\n",
    "\n",
    "    print(\"Train Data Labels [0]:\", np.unique(y_train))\n",
    "\n",
    "    # Calculate how many samples to add noise to based on the provided percentage\n",
    "    n_samples = X_train.shape[0]\n",
    "    noise_samples_count = int(n_samples * (noise_percentage / 100))\n",
    "\n",
    "    # Get the samples with label 1 (for generating noise)\n",
    "    X_train_noise = X_train_total[y_train_total == 1]\n",
    "    \n",
    "    # Randomly select noise_samples_count from X_train_noise\n",
    "    noisy_indices = np.random.choice(X_train_noise.shape[0], size=noise_samples_count, replace=False)\n",
    "    X_train_noise = X_train_noise[noisy_indices]\n",
    "    \n",
    "    # Add the noisy samples to the training set\n",
    "    X_train = np.vstack((X_train, X_train_noise))\n",
    "    y_train = np.concatenate((y_train, np.ones(X_train_noise.shape[0])))\n",
    "    print(y_train) \n",
    "    \n",
    "    # Prepare test data\n",
    "    X_test = test_data.iloc[:, :-1].to_numpy()\n",
    "    y_test = test_data.iloc[:, -1].to_numpy()\n",
    "\n",
    "    # Print the new size of training data\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_features = X_train.shape[1]\n",
    "    print(\"Number of samples after adding noise:\", n_samples)\n",
    "    print(\"Number of features:\", n_features)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "def cluster_kmeans(data, initial_k):\n",
    "    print(\"Starting GMM clustering...\")\n",
    "\n",
    "    # Thực hiện GMM clustering với số cụm initial_k\n",
    "    gmm = GaussianMixture(n_components=initial_k, random_state=42)\n",
    "    cluster_labels = gmm.fit_predict(data)\n",
    "\n",
    "    # Gán nhãn cụm vào biến labels\n",
    "    labels = cluster_labels\n",
    "\n",
    "    # Sắp xếp dữ liệu theo nhãn cụm\n",
    "    sorted_indices = np.argsort(labels)\n",
    "    sorted_data = data[sorted_indices]\n",
    "    sorted_labels = labels[sorted_indices]\n",
    "\n",
    "    print(\"Final number of clusters:\", len(np.unique(sorted_labels)))\n",
    "    return sorted_data, sorted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nullspace(A):\n",
    "    _, s, vh = np.linalg.svd(A)\n",
    "    null_mask = np.isclose(s, 0)\n",
    "    null_space = vh[null_mask].T\n",
    "    return null_space\n",
    "\n",
    "\n",
    "def minimum_distance( A, B):\n",
    "        \"\"\"\n",
    "        Compute the minimum Euclidean distance from each point in A to all points in B.\n",
    "        \n",
    "        Parameters:\n",
    "        - A (ndarray): Set of points (N_A, d).\n",
    "        - B (ndarray): Set of points (N_B, d).\n",
    "        \n",
    "        Returns:\n",
    "        - min_distances (ndarray): Minimum distances from each point in A to the nearest point in B.\n",
    "        \"\"\"\n",
    "        A = np.asarray(A)\n",
    "        B = np.asarray(B)\n",
    "        \n",
    "        # Initialize an array for storing minimum distances\n",
    "        min_distances = np.empty(A.shape[0], dtype=np.float64)\n",
    "        \n",
    "        # Iterate over each point in A and calculate minimum distance to points in B\n",
    "        for i, a in enumerate(A):\n",
    "            distances = cdist([a], B, metric='euclidean')  # Compute all pairwise distances\n",
    "            min_distances[i] = np.min(distances)  # Store minimum distance\n",
    "        \n",
    "        return min_distances   \n",
    "\n",
    "def distance_vector(point_X, point_Y):\n",
    "    \"\"\"\n",
    "    Calculate pairwise Euclidean distance between two sets of points.\n",
    "    \n",
    "    Args:\n",
    "        point_X (ndarray): Array of shape (N_train, d) where N_train is the number of training samples and d is the number of features.\n",
    "        point_Y (ndarray): Array of shape (N_test, d) where N_test is the number of test samples and d is the number of features.\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: Distance matrix of shape (N_test, N_train) containing Euclidean distances between each pair of points.\n",
    "    \"\"\"\n",
    "    print( 'Complexity of calculate: ', point_X )\n",
    "    # Compute squared norms for each point\n",
    "    norm_X = np.sum(point_X**2, axis=1)  # (N_train,)\n",
    "    norm_Y = np.sum(point_Y**2, axis=1)  # (N_test,)\n",
    "    \n",
    "    # Compute the dot product between the two sets of points\n",
    "    dot_product = np.dot(point_Y, point_X.T)  # (N_test, N_train)\n",
    "    \n",
    "    # Apply Euclidean distance formula\n",
    "    distance = np.sqrt(abs(norm_Y[:, np.newaxis] + norm_X[np.newaxis, :] - 2 * dot_product))\n",
    "    return distance\n",
    "\n",
    "\n",
    "\n",
    "def calculate_NPD(X,y):\n",
    "    print(\"Begin calculating NPD --------------\")\n",
    "    X = X.T\n",
    "    print('shape of X', X.shape)\n",
    "    c = len(np.unique(y))  # Số lớp\n",
    "    d, N = X.shape  # Số đặc trưng và số mẫu\n",
    "    # Tính trung bình toàn cục và tạo ma trận P_t với zero-mean\n",
    "    mean_total = np.mean(X, axis=1, keepdims=True)\n",
    "    P_t = X - mean_total  # P_t là ma trận zero-mean có kích thước d x N\n",
    "    \n",
    "    # Tính P_w cho từng lớp\n",
    "    P_w = np.zeros_like(X)\n",
    "    for i in range(c):\n",
    "        class_mean = np.mean(X[:, y == i], axis=1, keepdims=True)\n",
    "        P_w[:, y == i] = X[:, y == i] - class_mean  # Tạo ma trận P_w có kích thước d x N\n",
    "        \n",
    "    # Tính ma trận phương sai S_w và S_t\n",
    "    S_w = np.dot(P_w, P_w.T) / N  # S_w là d x d\n",
    "    S_t = np.dot(P_t, P_t.T) / N  # S_t là d x d\n",
    "\n",
    "    print( \"rank sw st :\", np.linalg.matrix_rank(S_t) , np.linalg.matrix_rank(S_w)) \n",
    "    print( \"rank sw st :\", np.linalg.matrix_rank(P_t) , np.linalg.matrix_rank(P_w)) \n",
    "\n",
    "    \n",
    "    U, S, Vt = np.linalg.svd(P_t, full_matrices=False)\n",
    "    Q = U\n",
    "    B = nullspace(Q.T @ S_w @ Q)\n",
    "    W = Q @ B  # W có kích thước d x (c - 1)\n",
    "\n",
    "    # In kết quả\n",
    "    print(\"N =\", N,  \"  d =\", d, \"   c =\", c )\n",
    "    print(\"P_w : d x N =\", P_w.shape)  # d x N\n",
    "    print(\"P_t : d x N =\", P_t.shape)  # d x N\n",
    "    print(\"S_w : d x d =\", S_w.shape)  # d x d\n",
    "    print(\"S_t : d x d =\", S_t.shape)  # d x d\n",
    "    print(\"Q   : d x N =\", Q.shape)     # d x r\n",
    "    print(\"B   : N x L =\", B.shape)     # r x L\n",
    "    print(\"W   : d x L =\", W.shape)     # d x L\n",
    "    return W    \n",
    "\n",
    "    \n",
    "\n",
    "def BruteForce_Threshold(y_true, y_prob, minth=0.0, maxth=1.0, num_thresholds=1000):\n",
    "    \"\"\"\n",
    "    Finds the best classification threshold for a binary model using brute force search.\n",
    "\n",
    "    Args:\n",
    "        y_true (ndarray): True labels (0 or 1), shape (n_samples,).\n",
    "        y_prob (ndarray): Predicted probabilities for class 1, shape (n_samples,).\n",
    "        minth (float, optional): Minimum threshold value. Default is 0.0.\n",
    "        maxth (float, optional): Maximum threshold value. Default is 1.0.\n",
    "        num_thresholds (int, optional): Number of threshold values to search. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the best threshold for each evaluation metric.\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(minth, maxth, num_thresholds)  # Generate candidate thresholds\n",
    "\n",
    "    # Initialize best metrics\n",
    "    best_results = {\n",
    "        \"accuracy\": (0, 0),  # (best_threshold, best_score)\n",
    "        \"f1\": (0, 0),\n",
    "        \"mcc\": (0, 0),\n",
    "        \"auc_roc\": (0, 0),\n",
    "        \"auc_pr\": (0, 0)\n",
    "    }\n",
    "\n",
    "    return best_results \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob[:,1] >= threshold).astype(int)\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "        auc_roc = roc_auc_score(y_true, y_prob[:, 1])\n",
    "        auc_pr = average_precision_score(y_true, y_prob[:, 1])\n",
    "\n",
    "        # Update best threshold for each metric\n",
    "        if acc > best_results[\"accuracy\"][1]:\n",
    "            best_results[\"accuracy\"] = (threshold, acc)\n",
    "        if f1 > best_results[\"f1\"][1]:\n",
    "            best_results[\"f1\"] = (threshold, f1)\n",
    "        if mcc > best_results[\"mcc\"][1]:\n",
    "            best_results[\"mcc\"] = (threshold, mcc)\n",
    "        if auc_roc > best_results[\"auc_roc\"][1]:\n",
    "            best_results[\"auc_roc\"] = (threshold, auc_roc)\n",
    "        if auc_pr > best_results[\"auc_pr\"][1]:\n",
    "            best_results[\"auc_pr\"] = (threshold, auc_pr)\n",
    "\n",
    "    return best_results\n",
    "\n",
    "def learn( npd, X_train, y_train , X_test, t0):\n",
    "    '''\n",
    "    X_train n1, d \n",
    "\n",
    "    C: n * n * d \n",
    "    '''\n",
    "    t1 = time.time()\n",
    "    null_point_X = (sp(X_train).dot(sp(npd))).toarray()\n",
    "    null_point_X_test = (sp(X_test).dot(sp(npd))).toarray()  \n",
    "\n",
    "    train_score_tmp = distance_vector(null_point_X, null_point_X)\n",
    "    for i in range(len(train_score_tmp)):\n",
    "        train_score_tmp[i , i] = 1e9                                                      \n",
    "    train_score = np.amin(train_score_tmp, axis=1)\n",
    "    \n",
    "    y_score = minimum_distance(null_point_X_test, null_point_X)\n",
    "    y_proba = np.zeros((len(y_score), 2))\n",
    "    y_proba[:, 1] = np.minimum(y_score / np.max(train_score), 1)                         \n",
    "    y_proba[:, 0] = 1 - y_proba[:, 1]                                                     # Probability for class 0\n",
    "    \n",
    "    y_proba = np.nan_to_num(y_proba, nan=1.0)\n",
    "\n",
    "    y_predict = (y_proba[:, 1] > 0.2).astype(int) \n",
    "    \n",
    "    \n",
    "    \n",
    "    t2 = time.time()\n",
    "    print(\"...............................Timing Model................................\")\n",
    "    print(\"Time train:\", t1-t0)\n",
    "    print(\"Time test:\" , t2-t1)\n",
    "    return y_proba, y_predict, t1, t2  \n",
    "\n",
    "def Model_evaluating(y_true, y_predict, y_scores):\n",
    "    \"\"\"\n",
    "    Evaluate the model using various metrics such as AUC, AUCPR, Accuracy, MCC, F1-score, Precision, and Recall.\n",
    "    \n",
    "    Args:\n",
    "        y_true (ndarray): True labels.\n",
    "        y_predict (ndarray): Predicted labels.\n",
    "        y_scores (ndarray): Predicted probabilities for each class.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of evaluation metrics.\n",
    "    \"\"\"\n",
    "    print(\"..............................Report Parameter...............................\")\n",
    "    \n",
    "    # Calculate MCC (Matthews Correlation Coefficient)\n",
    "    mcc = matthews_corrcoef(y_true, y_predict)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_true, y_predict)\n",
    "    \n",
    "    # Calculate PPV (Positive Predictive Value) or Precision\n",
    "    ppv = precision_score(y_true, y_predict)\n",
    "    \n",
    "    # Calculate TPR (True Positive Rate) or Recall/Sensitivity\n",
    "    tpr = recall_score(y_true, y_predict)\n",
    "    \n",
    "    # Calculate Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_predict)\n",
    "    \n",
    "    # Calculate AUC (Area Under ROC Curve)\n",
    "    AUC = roc_auc_score(y_true, y_scores[:, 1])\n",
    "    \n",
    "    # Calculate AUCPR (Area Under Precision-Recall Curve)\n",
    "    AUCPR = average_precision_score(y_true, y_scores[:, 1])\n",
    "    \n",
    "    # Print out the evaluation metrics\n",
    "    print(\"AUCROC:\", AUC * 100)\n",
    "    print(\"AUCPR:\", AUCPR * 100)\n",
    "    print(\"Accuracy:\", accuracy * 100)\n",
    "    print(\"MCC:\", mcc)\n",
    "    print(\"F1 score:\", f1)\n",
    "    print(\"PPV (Precision):\", ppv)\n",
    "    print(\"TPR (Recall):\", tpr)\n",
    "\n",
    "    return [AUC * 100 if AUC else None, AUCPR * 100 if AUCPR else None,\n",
    "            accuracy * 100, mcc, f1, ppv, tpr]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train, X_test, y_test = preprocess_data_OC(df2, df1)\n",
    "\n",
    "# X_total = np.vstack((X_train, X_test))\n",
    "# unique_rows = np.unique(X_total, axis=0)\n",
    "\n",
    "# print(\"Số dòng trong X_total:\", X_total.shape[0])   # Output: 5\n",
    "# print(\"Số dòng khác nhau:\", unique_rows.shape[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files from the GMM-nfst folder\n",
    "\n",
    "columns = [\"scaler\",\"nCluster\", 'noise_percentage', \"AUCROC\", \"AUCPR\", \"Accuracy\", \"MCC\", \"F1 Score\",\n",
    "           \"Precision\", \"Recall\", \"Time Train\", \"Time Test\"]\n",
    "# Now you can use df1 and df2 as DataFrames\n",
    "\n",
    "#CIC_IoT2023_1000.csv\n",
    "#N_BaIoT_1000.csv\n",
    "#BoT_IoT_1000.csv\n",
    "\n",
    "import cProfile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# def plot_data_2D(X, labels, title=\"Interactive 3D PCA Plot\"):\n",
    "#     \"\"\" Vẽ dữ liệu với PCA và tạo đồ thị 3D có thể tương tác bằng Plotly \"\"\"\n",
    "\n",
    "#     # Ensure that n_components ≤ min(n_samples, n_features)\n",
    "#     n_components = min(3, X.shape[0], X.shape[1])\n",
    "#     if n_components < 3:\n",
    "#         print(f\"⚠️ Warning: Cannot plot 3D, because n_components={n_components}. Skipping...\")\n",
    "#         return\n",
    "    \n",
    "#     # Perform PCA\n",
    "#     pca = PCA(n_components=3)\n",
    "#     X_3D = pca.fit_transform(X)\n",
    "\n",
    "#     # Convert labels to a NumPy array\n",
    "#     labels = np.array(labels)\n",
    "\n",
    "#     # Create a DataFrame for better visualization\n",
    "#     import pandas as pd\n",
    "#     df = pd.DataFrame(X_3D, columns=['PCA 1', 'PCA 2', 'PCA 3'])\n",
    "#     df['Class'] = labels\n",
    "\n",
    "#     # Create an interactive 3D scatter plot\n",
    "#     fig = px.scatter_3d(df, x='PCA 1', y='PCA 2', z='PCA 3', \n",
    "#                          color=df['Class'].astype(str),  # Color by class\n",
    "#                          title=title, labels={'color': 'Class'},\n",
    "#                          opacity=0.8)\n",
    "\n",
    "#     fig.update_traces(marker=dict(size=5))  # Adjust marker size\n",
    "#     fig.show()\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def plot_data_2D(X, labels, title=\"Dữ liệu gốc trước khi biến đổi\"):\n",
    "    \"\"\" Vẽ dữ liệu với PCA để giảm xuống 2D \"\"\"\n",
    "    return 0 \n",
    "    pca = PCA(n_components=3)\n",
    "    X_2D = pca.fit_transform(X)  # Chuyển về dạng (100, 2)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    labels = np.array(labels)  \n",
    "    for i in np.unique(labels):\n",
    "        plt.scatter(X_2D[labels == i, 0], X_2D[labels == i, 1], label=f\"Class {i}\", alpha=0.7)\n",
    "\n",
    "    plt.xlabel(\"PCA 1\")\n",
    "    plt.ylabel(\"PCA 2\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def function(df1,df2, scaler, noise):\n",
    "\n",
    "    X_train0, y_train0, X_test, y_test = preprocess_data_noise( df1,df2, noise)\n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"mean\") \n",
    "    X_train0[np.isinf(X_train0)] = np.nan  # Đổi vô hạn thành NaN\n",
    "    X_train0 = imputer.fit_transform(X_train0)\n",
    "    for ncluster in range(1, 310, 3):\n",
    "        t0 = time.time()\n",
    "        X_train , y_train = cluster_kmeans( X_train0 , ncluster )\n",
    "        plot_data_2D(X_train, y_train, \"Du lieu sau clusterr\") \n",
    "        npd = calculate_NPD(X_train, y_train)\n",
    "        \n",
    "        y_proba, y_predict, t1, t2 = learn( npd , X_train, y_train, X_test, y_test)  #,y_predict\n",
    "        \n",
    "        # print(\"...............................Timing Model................................\")\n",
    "        # print(\"Time train:\", t1-t0)\n",
    "        # print(\"Time test:\" , t2-t1)\n",
    "        # print(\"...........................................................................\")\n",
    "        # print(y_predict)\n",
    "        v = Model_evaluating(y_test, y_predict, y_proba)\n",
    "        # best_thresholds = BruteForce_Threshold( y_test, y_proba, 0, 1)  \n",
    "        result = [scaler] + [ncluster] + [noise] + v + [t1-t0, t2-t1]\n",
    "        # for metric, (threshold, score) in best_thresholds.items():\n",
    "        #     result = result + [threshold, score ]  \n",
    "\n",
    "        result_df = pd.DataFrame([result], columns=columns)\n",
    "        result_df.to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)\n",
    "        # result_df.to_csv(output_file, mode='a', header=(mark==0), index=False)\n",
    "        \n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "-------- data_ToNIoT.csv ------------------------------\n",
      "--------------------------------------------------\n",
      "Processing dataset data_ToNIoT.csv with QuantileTransformer scaler ...\n",
      "..............................Data Overview................................\n",
      "Train Data Shape: (39028, 29)\n",
      "Test Data Shape: (9757, 29)\n",
      "Train Data Labels [0]: [0]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Number of samples after adding noise: 4491\n",
      "Number of features: 28\n",
      "Starting GMM clustering...\n",
      "Final number of clusters: 1\n",
      "Begin calculating NPD --------------\n",
      "shape of X (28, 4491)\n",
      "rank sw st : 24 24\n",
      "rank sw st : 24 24\n",
      "N = 4491   d = 28    c = 1\n",
      "P_w : d x N = (28, 4491)\n",
      "P_t : d x N = (28, 4491)\n",
      "S_w : d x d = (28, 28)\n",
      "S_t : d x d = (28, 28)\n",
      "Q   : d x N = (28, 28)\n",
      "B   : N x L = (28, 7)\n",
      "W   : d x L = (28, 7)\n",
      "Complexity of calculate:  [[-4.22919417e-07 -4.09126492e-06  8.90825423e-08 ...  1.80190588e-16\n",
      "  -1.70435998e-35 -1.46505858e-14]\n",
      " [-2.15200838e-06 -1.29924507e-06 -9.60999314e-09 ... -1.78537245e-17\n",
      "   9.17426400e-34  1.25166730e-15]\n",
      " [-1.80347826e-06 -1.09566562e-06 -5.11800728e-08 ... -9.82441193e-17\n",
      "   2.64133696e-34  7.51862329e-15]\n",
      " ...\n",
      " [-2.66573581e-06 -1.50933838e-06  8.43012716e-08 ...  1.68718232e-16\n",
      "  -9.49795507e-34 -1.34739135e-14]\n",
      " [-9.84168134e-05 -9.54634361e-05  6.57741976e-07 ...  1.40938747e-15\n",
      "  -4.80963308e-32 -1.18473913e-13]\n",
      " [-2.02709345e-06  9.12597233e-07  1.40919583e-07 ...  2.77675751e-16\n",
      "  -1.03172116e-33 -2.18095989e-14]]\n",
      "...............................Timing Model................................\n",
      "Time train: [1.75755506e+09 1.75755506e+09 1.75755506e+09 ... 1.75755506e+09\n",
      " 1.75755506e+09 1.75755506e+09]\n",
      "Time test: 0.6229097843170166\n",
      "..............................Report Parameter...............................\n",
      "AUCROC: 77.25142514967574\n",
      "AUCPR: 95.25752326377734\n",
      "Accuracy: 11.632673977657067\n",
      "MCC: 0.016279187133789542\n",
      "F1 score: 0.004617871161394597\n",
      "PPV (Precision): 1.0\n",
      "TPR (Recall): 0.0023142791020597086\n",
      "Starting GMM clustering...\n",
      "Final number of clusters: 4\n",
      "Begin calculating NPD --------------\n",
      "shape of X (28, 4491)\n",
      "rank sw st : 24 24\n",
      "rank sw st : 24 24\n",
      "N = 4491   d = 28    c = 4\n",
      "P_w : d x N = (28, 4491)\n",
      "P_t : d x N = (28, 4491)\n",
      "S_w : d x d = (28, 28)\n",
      "S_t : d x d = (28, 28)\n",
      "Q   : d x N = (28, 28)\n",
      "B   : N x L = (28, 7)\n",
      "W   : d x L = (28, 7)\n",
      "Complexity of calculate:  [[ 8.46991997e-06  2.09911526e-07 -1.24316953e-06 ...  3.16082477e-15\n",
      "   6.95917573e-18  6.96358459e-35]\n",
      " [ 8.43902332e-06  1.54568857e-07 -1.25012235e-06 ...  3.26202812e-15\n",
      "   9.00654507e-18  8.27613045e-35]\n",
      " [ 8.27109313e-06  3.91845057e-07 -1.24087408e-06 ...  2.92239620e-15\n",
      "   7.05920682e-18  6.02782897e-35]\n",
      " ...\n",
      " [-9.45958062e-05 -9.84390114e-05  5.52872270e-07 ...  1.28563724e-13\n",
      "  -7.62064038e-18 -7.58162813e-34]\n",
      " [ 5.17056104e-06 -8.55057463e-06 -5.04313565e-08 ...  1.10703200e-14\n",
      "   8.14719418e-18  5.05062624e-35]\n",
      " [ 2.56947366e-06 -5.30362669e-06 -9.52501749e-08 ...  7.10145293e-15\n",
      "   1.13059726e-17  4.41044245e-35]]\n",
      "...............................Timing Model................................\n",
      "Time train: [1.75755506e+09 1.75755506e+09 1.75755506e+09 ... 1.75755506e+09\n",
      " 1.75755506e+09 1.75755506e+09]\n",
      "Time test: 1.1478271484375\n",
      "..............................Report Parameter...............................\n",
      "AUCROC: 73.62228266791755\n",
      "AUCPR: 94.61392844076273\n",
      "Accuracy: 11.632673977657067\n",
      "MCC: 0.016279187133789542\n",
      "F1 score: 0.004617871161394597\n",
      "PPV (Precision): 1.0\n",
      "TPR (Recall): 0.0023142791020597086\n",
      "Starting GMM clustering...\n",
      "Final number of clusters: 7\n",
      "Begin calculating NPD --------------\n",
      "shape of X (28, 4491)\n",
      "rank sw st : 24 24\n",
      "rank sw st : 24 24\n",
      "N = 4491   d = 28    c = 7\n",
      "P_w : d x N = (28, 4491)\n",
      "P_t : d x N = (28, 4491)\n",
      "S_w : d x d = (28, 28)\n",
      "S_t : d x d = (28, 28)\n",
      "Q   : d x N = (28, 28)\n",
      "B   : N x L = (28, 7)\n",
      "W   : d x L = (28, 7)\n",
      "Complexity of calculate:  [[ 4.67789447e-06  3.19158831e-05 -1.03789111e-06 ...  2.10879103e-15\n",
      "   2.05356090e-17 -6.64651119e-32]\n",
      " [ 4.66815976e-06  3.15284035e-05 -1.03960171e-06 ...  2.36188601e-15\n",
      "   1.74595508e-17 -6.55577201e-32]\n",
      " [ 3.71106059e-06  3.33414131e-05 -1.06721174e-06 ...  2.02797754e-15\n",
      "   5.95004477e-18 -6.58732143e-32]\n",
      " ...\n",
      " [-3.40081970e-06  7.77126474e-07 -8.79899418e-09 ... -1.58041252e-16\n",
      "  -5.38711521e-18  2.57084129e-34]\n",
      " [-6.44126012e-06  2.84189525e-06 -1.07315147e-08 ... -1.14277208e-15\n",
      "  -8.12715585e-18  6.44067630e-34]\n",
      " [-5.65031125e-06 -1.46124596e-06 -3.67836759e-10 ...  8.69083778e-16\n",
      "  -7.77170980e-19  1.40167405e-33]]\n",
      "...............................Timing Model................................\n",
      "Time train: [1.75755507e+09 1.75755507e+09 1.75755507e+09 ... 1.75755507e+09\n",
      " 1.75755507e+09 1.75755507e+09]\n",
      "Time test: 0.8371098041534424\n",
      "..............................Report Parameter...............................\n",
      "AUCROC: 57.041292758382\n",
      "AUCPR: 89.52008397277238\n",
      "Accuracy: 11.632673977657067\n",
      "MCC: 0.016279187133789542\n",
      "F1 score: 0.004617871161394597\n",
      "PPV (Precision): 1.0\n",
      "TPR (Recall): 0.0023142791020597086\n",
      "Starting GMM clustering...\n",
      "Final number of clusters: 10\n",
      "Begin calculating NPD --------------\n",
      "shape of X (28, 4491)\n",
      "rank sw st : 24 24\n",
      "rank sw st : 24 24\n",
      "N = 4491   d = 28    c = 10\n",
      "P_w : d x N = (28, 4491)\n",
      "P_t : d x N = (28, 4491)\n",
      "S_w : d x d = (28, 28)\n",
      "S_t : d x d = (28, 28)\n",
      "Q   : d x N = (28, 28)\n",
      "B   : N x L = (28, 7)\n",
      "W   : d x L = (28, 7)\n",
      "Complexity of calculate:  [[ 3.77799269e-06  9.91559884e-06 -9.74097134e-07 ...  4.94490504e-15\n",
      "   1.18758437e-16 -8.31022038e-33]\n",
      " [ 3.65474241e-06  1.02787911e-05 -9.76834571e-07 ...  5.19657182e-15\n",
      "   1.19666504e-16 -8.48957086e-33]\n",
      " [ 3.76835751e-06  9.88239697e-06 -1.02293452e-06 ...  4.57327822e-15\n",
      "   1.10537637e-16 -8.54047149e-33]\n",
      " ...\n",
      " [-3.41511794e-06 -1.47668494e-06  1.57296092e-07 ... -1.44260708e-16\n",
      "   2.89679228e-17  2.78555522e-34]\n",
      " [ 1.39520399e-06 -5.14890571e-06  1.68272789e-07 ... -2.85768662e-15\n",
      "   3.25820208e-17 -2.12631862e-35]\n",
      " [ 4.59600390e-07 -5.14703761e-06  1.51025946e-08 ... -3.95868642e-15\n",
      "   3.19399560e-18 -1.49888692e-35]]\n",
      "...............................Timing Model................................\n",
      "Time train: [1.75755507e+09 1.75755507e+09 1.75755507e+09 ... 1.75755507e+09\n",
      " 1.75755507e+09 1.75755507e+09]\n",
      "Time test: 0.6258032321929932\n",
      "..............................Report Parameter...............................\n",
      "AUCROC: 63.64627644945998\n",
      "AUCPR: 92.7554416412875\n",
      "Accuracy: 11.632673977657067\n",
      "MCC: 0.016279187133789542\n",
      "F1 score: 0.004617871161394597\n",
      "PPV (Precision): 1.0\n",
      "TPR (Recall): 0.0023142791020597086\n",
      "Starting GMM clustering...\n",
      "Final number of clusters: 13\n",
      "Begin calculating NPD --------------\n",
      "shape of X (28, 4491)\n",
      "rank sw st : 24 24\n",
      "rank sw st : 24 24\n",
      "N = 4491   d = 28    c = 13\n",
      "P_w : d x N = (28, 4491)\n",
      "P_t : d x N = (28, 4491)\n",
      "S_w : d x d = (28, 28)\n",
      "S_t : d x d = (28, 28)\n",
      "Q   : d x N = (28, 28)\n",
      "B   : N x L = (28, 7)\n",
      "W   : d x L = (28, 7)\n",
      "Complexity of calculate:  [[-3.80766265e-06  4.67828239e-05 -1.69064435e-06 ...  1.98013319e-16\n",
      "  -4.80378346e-32 -2.44092842e-12]\n",
      " [-3.69350902e-06  4.59827366e-05 -1.72440680e-06 ...  1.89525759e-16\n",
      "  -4.72263053e-32 -2.48856598e-12]\n",
      " [-3.72533725e-06  4.63178086e-05 -1.70290940e-06 ...  1.95866517e-16\n",
      "  -4.79089609e-32 -2.45812107e-12]\n",
      " ...\n",
      " [ 4.70365999e-07  5.54408373e-06  6.45587306e-07 ...  9.23402119e-17\n",
      "  -5.78734799e-34  9.17214724e-13]\n",
      " [ 4.70443638e-07  5.54353305e-06  6.45792459e-07 ...  9.23700905e-17\n",
      "  -5.78492151e-34  9.17507712e-13]\n",
      " [ 4.67017023e-07  5.56783766e-06  6.36737951e-07 ...  9.10513886e-17\n",
      "  -5.89201488e-34  9.04576594e-13]]\n",
      "...............................Timing Model................................\n",
      "Time train: [1.75755509e+09 1.75755509e+09 1.75755509e+09 ... 1.75755509e+09\n",
      " 1.75755509e+09 1.75755509e+09]\n",
      "Time test: 0.7684834003448486\n",
      "..............................Report Parameter...............................\n",
      "AUCROC: 61.742444605187096\n",
      "AUCPR: 91.15978023391617\n",
      "Accuracy: 11.632673977657067\n",
      "MCC: 0.016279187133789542\n",
      "F1 score: 0.004617871161394597\n",
      "PPV (Precision): 1.0\n",
      "TPR (Recall): 0.0023142791020597086\n",
      "Starting GMM clustering...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cProfile\n",
    "import pstats    \n",
    "\n",
    "# dataset_prefixes =  ['N_BaIoT_dataloader.csv']\n",
    "dataset_prefixes =  ['data_ToNIoT.csv', 'data_CICIoT2023.csv','data_N_BaIoT.csv', 'data_BoTIoT.csv']\n",
    "# dataset_prefixes =  ['data_CICIoT2023.csv']\n",
    "\n",
    "\n",
    "#  'data_N_BaIoT.csv',  \n",
    "# scaler_names = ['MinMaxScaler']\n",
    "scaler_names = ['QuantileTransformer','MinMaxScaler','Normalizer','RobustScaler', 'StandardScaler' ]\n",
    "  \n",
    "# scaler_names = ['QuantileTransformer', 'StandardScaler','QuantileTransformer'] 'StandardScaler', \n",
    "\n",
    "    # Iterate through dataset prefixes and process each dataset pair\n",
    "\n",
    "\n",
    "\n",
    "for prefix in dataset_prefixes:\n",
    "    \n",
    "    print(\"-\"*50)\n",
    "    print(\"--------\", prefix , \"-\"*30)\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    #  NFST/GMM-nfst/Structure_Result/NOISEGAU \n",
    "    base_output_file = f\"Structure_Result/NOISEGAU/{prefix}\" \n",
    "    output_file = base_output_file + \"0.csv\"\n",
    "\n",
    "    # Increment the filename if it already exists\n",
    "    counter = 0\n",
    "    while os.path.exists(output_file):\n",
    "        counter += 1\n",
    "        output_file = f\"{base_output_file}{counter}.csv\"\n",
    "\n",
    "        \n",
    "    for scaler in scaler_names: \n",
    "        print(f\"Processing dataset {prefix} with {scaler} scaler ...\")\n",
    "        \n",
    "        # Construct file paths for train and test datasets with 'Train_' and 'Test_' prefixes\n",
    "        train_file = f'Datascaled/NoiseOCData/Train_{scaler}_{prefix}'\n",
    "        test_file = f'Datascaled/NoiseOCData/Test_{scaler}_{prefix}'\n",
    "        \n",
    "        # Load the CSV files\n",
    "        df_train = pd.read_csv(train_file)\n",
    "        df_test = pd.read_csv(test_file)\n",
    "\n",
    "        for noise in [0,1,3,5] : \n",
    "            \n",
    "            # log_file = f'Results/OURMODEL/SCALERS/{prefix}_modellog.txt'\n",
    "            # with open(log_file, \"w\") as f:\n",
    "            #     profiler = cProfile.Profile()\n",
    "            #     profiler.enable()\n",
    "                \n",
    "            function(df_train, df_test, scaler, noise)  # Chạy hàm cần profile\n",
    "            \n",
    "                # profiler.disable()\n",
    "                # stats = pstats.Stats(profiler, stream=f)\n",
    "                # stats.sort_stats(\"cumulative\").print_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
