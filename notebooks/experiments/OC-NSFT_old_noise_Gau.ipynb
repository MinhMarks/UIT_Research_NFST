{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (roc_auc_score, precision_score, average_precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error, roc_curve, auc, classification_report,auc,confusion_matrix,matthews_corrcoef)\n",
    "from sklearn import logger\n",
    "from sklearn.datasets import make_blobs,make_multilabel_classification\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import KernelCenterer,LabelEncoder, MinMaxScaler, Normalizer, QuantileTransformer, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import scipy as sp\n",
    "from scipy.linalg import svd,null_space\n",
    "import os\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.cluster import KMeans,AgglomerativeClustering,SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.sparse import csr_matrix as sp\n",
    "import math\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.spatial.distance import cdist\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Import 3D plotting tools\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hàm\tĐộ phức tạp tính toán\tBộ nhớ cần\n",
    "# preprocess_data_OC\tO(n⋅d²)\tO(n⋅d²)\n",
    "# clusterr\tO(n⋅d⋅k⋅iters)\tO(n⋅d)\n",
    "# nullspace\tO(d³)\tO(d²)\n",
    "# gram_schmidt\tO(d³)\tO(d²)\n",
    "# minimum_distance\tO(n⋅m⋅d)\tO(n⋅m)\n",
    "# distance_vector\tO(n⋅m⋅d)\tO(n⋅m)\n",
    "# calculate_NPD\tO(d³)\tO(d²)\n",
    "# learn\tO(n²⋅d)\tO(n²)\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "alpha = 0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data_noise(train_data, test_data, noise_percentage=10):\n",
    "  \n",
    "    print(\"..............................Data Overview................................\")\n",
    "    print(\"Train Data Shape:\", train_data.shape)\n",
    "    print(\"Test Data Shape:\", test_data.shape)\n",
    "    \n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    X_train_total = train_data.iloc[:, :-1].to_numpy()\n",
    "    y_train_total = train_data.iloc[:, -1].to_numpy()\n",
    "\n",
    "    # Separate the samples with label 0\n",
    "    X_train = X_train_total[y_train_total == 0]\n",
    "    y_train = y_train_total[y_train_total == 0]\n",
    "\n",
    "    print(\"Train Data Labels [0]:\", np.unique(y_train))\n",
    "\n",
    "    # Calculate how many samples to add noise to based on the provided percentage\n",
    "    n_samples = X_train.shape[0]\n",
    "    noise_samples_count = int(n_samples * (noise_percentage / 100))\n",
    "\n",
    "    # Get the samples with label 1 (for generating noise)\n",
    "    X_train_noise = X_train_total[y_train_total == 1]\n",
    "    \n",
    "    # Randomly select noise_samples_count from X_train_noise\n",
    "    noisy_indices = np.random.choice(X_train_noise.shape[0], size=noise_samples_count, replace=False)\n",
    "    X_train_noise = X_train_noise[noisy_indices]\n",
    "    \n",
    "    # Add the noisy samples to the training set\n",
    "    X_train = np.vstack((X_train, X_train_noise))\n",
    "    y_train = np.concatenate((y_train, np.ones(X_train_noise.shape[0])))\n",
    "    print(y_train) \n",
    "    \n",
    "    # Prepare test data\n",
    "    X_test = test_data.iloc[:, :-1].to_numpy()\n",
    "    y_test = test_data.iloc[:, -1].to_numpy()\n",
    "\n",
    "    # Print the new size of training data\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_features = X_train.shape[1]\n",
    "    print(\"Number of samples after adding noise:\", n_samples)\n",
    "    print(\"Number of features:\", n_features)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "def cluster_kmeans(data, initial_k):\n",
    "    print(\"Starting GMM clustering...\")\n",
    "\n",
    "    # Thực hiện GMM clustering với số cụm initial_k\n",
    "    gmm = GaussianMixture(n_components=initial_k, random_state=42)\n",
    "    cluster_labels = gmm.fit_predict(data)\n",
    "\n",
    "    # Gán nhãn cụm vào biến labels\n",
    "    labels = cluster_labels\n",
    "\n",
    "    # Sắp xếp dữ liệu theo nhãn cụm\n",
    "    sorted_indices = np.argsort(labels)\n",
    "    sorted_data = data[sorted_indices]\n",
    "    sorted_labels = labels[sorted_indices]\n",
    "\n",
    "    print(\"Final number of clusters:\", len(np.unique(sorted_labels)))\n",
    "    return sorted_data, sorted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nullspace(A):\n",
    "    _, s, vh = np.linalg.svd(A)\n",
    "    null_mask = np.isclose(s, 0)\n",
    "    null_space = vh[null_mask].T\n",
    "    return null_space\n",
    "\n",
    "\n",
    "def minimum_distance( A, B):\n",
    "        \"\"\"\n",
    "        Compute the minimum Euclidean distance from each point in A to all points in B.\n",
    "        \n",
    "        Parameters:\n",
    "        - A (ndarray): Set of points (N_A, d).\n",
    "        - B (ndarray): Set of points (N_B, d).\n",
    "        \n",
    "        Returns:\n",
    "        - min_distances (ndarray): Minimum distances from each point in A to the nearest point in B.\n",
    "        \"\"\"\n",
    "        A = np.asarray(A)\n",
    "        B = np.asarray(B)\n",
    "        \n",
    "        # Initialize an array for storing minimum distances\n",
    "        min_distances = np.empty(A.shape[0], dtype=np.float64)\n",
    "        \n",
    "        # Iterate over each point in A and calculate minimum distance to points in B\n",
    "        for i, a in enumerate(A):\n",
    "            distances = cdist([a], B, metric='euclidean')  # Compute all pairwise distances\n",
    "            min_distances[i] = np.min(distances)  # Store minimum distance\n",
    "        \n",
    "        return min_distances   \n",
    "\n",
    "def distance_vector(point_X, point_Y):\n",
    "    \"\"\"\n",
    "    Calculate pairwise Euclidean distance between two sets of points.\n",
    "    \n",
    "    Args:\n",
    "        point_X (ndarray): Array of shape (N_train, d) where N_train is the number of training samples and d is the number of features.\n",
    "        point_Y (ndarray): Array of shape (N_test, d) where N_test is the number of test samples and d is the number of features.\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: Distance matrix of shape (N_test, N_train) containing Euclidean distances between each pair of points.\n",
    "    \"\"\"\n",
    "    print( 'Complexity of calculate: ', point_X )\n",
    "    # Compute squared norms for each point\n",
    "    norm_X = np.sum(point_X**2, axis=1)  # (N_train,)\n",
    "    norm_Y = np.sum(point_Y**2, axis=1)  # (N_test,)\n",
    "    \n",
    "    # Compute the dot product between the two sets of points\n",
    "    dot_product = np.dot(point_Y, point_X.T)  # (N_test, N_train)\n",
    "    \n",
    "    # Apply Euclidean distance formula\n",
    "    distance = np.sqrt(abs(norm_Y[:, np.newaxis] + norm_X[np.newaxis, :] - 2 * dot_product))\n",
    "    return distance\n",
    "\n",
    "\n",
    "\n",
    "def calculate_NPD(X,y):\n",
    "    print(\"Begin calculating NPD --------------\")\n",
    "    X = X.T\n",
    "    print('shape of X', X.shape)\n",
    "    c = len(np.unique(y))  # Số lớp\n",
    "    d, N = X.shape  # Số đặc trưng và số mẫu\n",
    "    # Tính trung bình toàn cục và tạo ma trận P_t với zero-mean\n",
    "    mean_total = np.mean(X, axis=1, keepdims=True)\n",
    "    P_t = X - mean_total  # P_t là ma trận zero-mean có kích thước d x N\n",
    "    \n",
    "    # Tính P_w cho từng lớp\n",
    "    P_w = np.zeros_like(X)\n",
    "    for i in range(c):\n",
    "        class_mean = np.mean(X[:, y == i], axis=1, keepdims=True)\n",
    "        P_w[:, y == i] = X[:, y == i] - class_mean  # Tạo ma trận P_w có kích thước d x N\n",
    "        \n",
    "    # Tính ma trận phương sai S_w và S_t\n",
    "    S_w = np.dot(P_w, P_w.T) / N  # S_w là d x d\n",
    "    S_t = np.dot(P_t, P_t.T) / N  # S_t là d x d\n",
    "\n",
    "    print( \"rank sw st :\", np.linalg.matrix_rank(S_t) , np.linalg.matrix_rank(S_w)) \n",
    "    print( \"rank sw st :\", np.linalg.matrix_rank(P_t) , np.linalg.matrix_rank(P_w)) \n",
    "\n",
    "    \n",
    "    U, S, Vt = np.linalg.svd(P_t, full_matrices=False)\n",
    "    Q = U\n",
    "    B = nullspace(Q.T @ S_w @ Q)\n",
    "    W = Q @ B  # W có kích thước d x (c - 1)\n",
    "\n",
    "    # In kết quả\n",
    "    print(\"N =\", N,  \"  d =\", d, \"   c =\", c )\n",
    "    print(\"P_w : d x N =\", P_w.shape)  # d x N\n",
    "    print(\"P_t : d x N =\", P_t.shape)  # d x N\n",
    "    print(\"S_w : d x d =\", S_w.shape)  # d x d\n",
    "    print(\"S_t : d x d =\", S_t.shape)  # d x d\n",
    "    print(\"Q   : d x N =\", Q.shape)     # d x r\n",
    "    print(\"B   : N x L =\", B.shape)     # r x L\n",
    "    print(\"W   : d x L =\", W.shape)     # d x L\n",
    "    return W    \n",
    "\n",
    "    \n",
    "\n",
    "def BruteForce_Threshold(y_true, y_prob, minth=0.0, maxth=1.0, num_thresholds=1000):\n",
    "    \"\"\"\n",
    "    Finds the best classification threshold for a binary model using brute force search.\n",
    "\n",
    "    Args:\n",
    "        y_true (ndarray): True labels (0 or 1), shape (n_samples,).\n",
    "        y_prob (ndarray): Predicted probabilities for class 1, shape (n_samples,).\n",
    "        minth (float, optional): Minimum threshold value. Default is 0.0.\n",
    "        maxth (float, optional): Maximum threshold value. Default is 1.0.\n",
    "        num_thresholds (int, optional): Number of threshold values to search. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the best threshold for each evaluation metric.\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(minth, maxth, num_thresholds)  # Generate candidate thresholds\n",
    "\n",
    "    # Initialize best metrics\n",
    "    best_results = {\n",
    "        \"accuracy\": (0, 0),  # (best_threshold, best_score)\n",
    "        \"f1\": (0, 0),\n",
    "        \"mcc\": (0, 0),\n",
    "        \"auc_roc\": (0, 0),\n",
    "        \"auc_pr\": (0, 0)\n",
    "    }\n",
    "\n",
    "    return best_results \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob[:,1] >= threshold).astype(int)\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "        auc_roc = roc_auc_score(y_true, y_prob[:, 1])\n",
    "        auc_pr = average_precision_score(y_true, y_prob[:, 1])\n",
    "\n",
    "        # Update best threshold for each metric\n",
    "        if acc > best_results[\"accuracy\"][1]:\n",
    "            best_results[\"accuracy\"] = (threshold, acc)\n",
    "        if f1 > best_results[\"f1\"][1]:\n",
    "            best_results[\"f1\"] = (threshold, f1)\n",
    "        if mcc > best_results[\"mcc\"][1]:\n",
    "            best_results[\"mcc\"] = (threshold, mcc)\n",
    "        if auc_roc > best_results[\"auc_roc\"][1]:\n",
    "            best_results[\"auc_roc\"] = (threshold, auc_roc)\n",
    "        if auc_pr > best_results[\"auc_pr\"][1]:\n",
    "            best_results[\"auc_pr\"] = (threshold, auc_pr)\n",
    "\n",
    "    return best_results\n",
    "\n",
    "def learn( npd, X_train, y_train , X_test, t0):\n",
    "    '''\n",
    "    X_train n1, d \n",
    "\n",
    "    C: n * n * d \n",
    "    '''\n",
    "    t1 = time.time()\n",
    "    null_point_X = (sp(X_train).dot(sp(npd))).toarray()\n",
    "    null_point_X_test = (sp(X_test).dot(sp(npd))).toarray()  \n",
    "\n",
    "    train_score_tmp = distance_vector(null_point_X, null_point_X)\n",
    "    for i in range(len(train_score_tmp)):\n",
    "        train_score_tmp[i , i] = 1e9                                                      \n",
    "    train_score = np.amin(train_score_tmp, axis=1)\n",
    "    \n",
    "    y_score = minimum_distance(null_point_X_test, null_point_X)\n",
    "    y_proba = np.zeros((len(y_score), 2))\n",
    "    y_proba[:, 1] = np.minimum(y_score / np.max(train_score), 1)                         \n",
    "    y_proba[:, 0] = 1 - y_proba[:, 1]                                                     # Probability for class 0\n",
    "    \n",
    "    y_proba = np.nan_to_num(y_proba, nan=1.0)\n",
    "\n",
    "    y_predict = (y_proba[:, 1] > 0.2).astype(int) \n",
    "    \n",
    "    \n",
    "    \n",
    "    t2 = time.time()\n",
    "    print(\"...............................Timing Model................................\")\n",
    "    print(\"Time train:\", t1-t0)\n",
    "    print(\"Time test:\" , t2-t1)\n",
    "    return y_proba, y_predict, t1, t2  \n",
    "\n",
    "def Model_evaluating(y_true, y_predict, y_scores):\n",
    "    \"\"\"\n",
    "    Evaluate the model using various metrics such as AUC, AUCPR, Accuracy, MCC, F1-score, Precision, and Recall.\n",
    "    \n",
    "    Args:\n",
    "        y_true (ndarray): True labels.\n",
    "        y_predict (ndarray): Predicted labels.\n",
    "        y_scores (ndarray): Predicted probabilities for each class.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of evaluation metrics.\n",
    "    \"\"\"\n",
    "    print(\"..............................Report Parameter...............................\")\n",
    "    \n",
    "    # Calculate MCC (Matthews Correlation Coefficient)\n",
    "    mcc = matthews_corrcoef(y_true, y_predict)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_true, y_predict)\n",
    "    \n",
    "    # Calculate PPV (Positive Predictive Value) or Precision\n",
    "    ppv = precision_score(y_true, y_predict)\n",
    "    \n",
    "    # Calculate TPR (True Positive Rate) or Recall/Sensitivity\n",
    "    tpr = recall_score(y_true, y_predict)\n",
    "    \n",
    "    # Calculate Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_predict)\n",
    "    \n",
    "    # Calculate AUC (Area Under ROC Curve)\n",
    "    AUC = roc_auc_score(y_true, y_scores[:, 1])\n",
    "    \n",
    "    # Calculate AUCPR (Area Under Precision-Recall Curve)\n",
    "    AUCPR = average_precision_score(y_true, y_scores[:, 1])\n",
    "    \n",
    "    # Print out the evaluation metrics\n",
    "    print(\"AUCROC:\", AUC * 100)\n",
    "    print(\"AUCPR:\", AUCPR * 100)\n",
    "    print(\"Accuracy:\", accuracy * 100)\n",
    "    print(\"MCC:\", mcc)\n",
    "    print(\"F1 score:\", f1)\n",
    "    print(\"PPV (Precision):\", ppv)\n",
    "    print(\"TPR (Recall):\", tpr)\n",
    "\n",
    "    return [AUC * 100 if AUC else None, AUCPR * 100 if AUCPR else None,\n",
    "            accuracy * 100, mcc, f1, ppv, tpr]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train, X_test, y_test = preprocess_data_OC(df2, df1)\n",
    "\n",
    "# X_total = np.vstack((X_train, X_test))\n",
    "# unique_rows = np.unique(X_total, axis=0)\n",
    "\n",
    "# print(\"Số dòng trong X_total:\", X_total.shape[0])   # Output: 5\n",
    "# print(\"Số dòng khác nhau:\", unique_rows.shape[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files from the GMM-nfst folder\n",
    "\n",
    "columns = [\"scaler\",\"nCluster\", 'noise_percentage', \"AUCROC\", \"AUCPR\", \"Accuracy\", \"MCC\", \"F1 Score\",\n",
    "           \"Precision\", \"Recall\", \"Time Train\", \"Time Test\"]\n",
    "# Now you can use df1 and df2 as DataFrames\n",
    "\n",
    "#CIC_IoT2023_1000.csv\n",
    "#N_BaIoT_1000.csv\n",
    "#BoT_IoT_1000.csv\n",
    "\n",
    "import cProfile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# def plot_data_2D(X, labels, title=\"Interactive 3D PCA Plot\"):\n",
    "#     \"\"\" Vẽ dữ liệu với PCA và tạo đồ thị 3D có thể tương tác bằng Plotly \"\"\"\n",
    "\n",
    "#     # Ensure that n_components ≤ min(n_samples, n_features)\n",
    "#     n_components = min(3, X.shape[0], X.shape[1])\n",
    "#     if n_components < 3:\n",
    "#         print(f\"⚠️ Warning: Cannot plot 3D, because n_components={n_components}. Skipping...\")\n",
    "#         return\n",
    "    \n",
    "#     # Perform PCA\n",
    "#     pca = PCA(n_components=3)\n",
    "#     X_3D = pca.fit_transform(X)\n",
    "\n",
    "#     # Convert labels to a NumPy array\n",
    "#     labels = np.array(labels)\n",
    "\n",
    "#     # Create a DataFrame for better visualization\n",
    "#     import pandas as pd\n",
    "#     df = pd.DataFrame(X_3D, columns=['PCA 1', 'PCA 2', 'PCA 3'])\n",
    "#     df['Class'] = labels\n",
    "\n",
    "#     # Create an interactive 3D scatter plot\n",
    "#     fig = px.scatter_3d(df, x='PCA 1', y='PCA 2', z='PCA 3', \n",
    "#                          color=df['Class'].astype(str),  # Color by class\n",
    "#                          title=title, labels={'color': 'Class'},\n",
    "#                          opacity=0.8)\n",
    "\n",
    "#     fig.update_traces(marker=dict(size=5))  # Adjust marker size\n",
    "#     fig.show()\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def plot_data_2D(X, labels, title=\"Dữ liệu gốc trước khi biến đổi\"):\n",
    "    \"\"\" Vẽ dữ liệu với PCA để giảm xuống 2D \"\"\"\n",
    "    return 0 \n",
    "    pca = PCA(n_components=3)\n",
    "    X_2D = pca.fit_transform(X)  # Chuyển về dạng (100, 2)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    labels = np.array(labels)  \n",
    "    for i in np.unique(labels):\n",
    "        plt.scatter(X_2D[labels == i, 0], X_2D[labels == i, 1], label=f\"Class {i}\", alpha=0.7)\n",
    "\n",
    "    plt.xlabel(\"PCA 1\")\n",
    "    plt.ylabel(\"PCA 2\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def function(df1,df2, scaler, noise):\n",
    "\n",
    "    X_train0, y_train0, X_test, y_test = preprocess_data_noise( df1,df2, noise)\n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"mean\") \n",
    "    X_train0[np.isinf(X_train0)] = np.nan  # Đổi vô hạn thành NaN\n",
    "    X_train0 = imputer.fit_transform(X_train0)\n",
    "    for ncluster in [20] : # , 50, 100, 90, 150, 151, 152, 160, 170] : # range(1, 310, 3):\n",
    "        t0 = time.time()\n",
    "        X_train , y_train = cluster_kmeans( X_train0 , ncluster )\n",
    "        plot_data_2D(X_train, y_train, \"Du lieu sau clusterr\") \n",
    "        npd = calculate_NPD(X_train, y_train)\n",
    "        \n",
    "        y_proba, y_predict, t1, t2 = learn( npd , X_train, y_train, X_test, t0)  #,y_predict\n",
    "        \n",
    "        # print(\"...............................Timing Model................................\")\n",
    "        # print(\"Time train:\", t1-t0)\n",
    "        # print(\"Time test:\" , t2-t1)\n",
    "        # print(\"...........................................................................\")\n",
    "        # print(y_predict)\n",
    "        v = Model_evaluating(y_test, y_predict, y_proba)\n",
    "        # best_thresholds = BruteForce_Threshold( y_test, y_proba, 0, 1)  \n",
    "        result = [scaler] + [ncluster] + [noise] + v + [t1-t0, t2-t1]\n",
    "        # for metric, (threshold, score) in best_thresholds.items():\n",
    "        #     result = result + [threshold, score ]  \n",
    "\n",
    "        result_df = pd.DataFrame([result], columns=columns)\n",
    "        result_df.to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)\n",
    "        # result_df.to_csv(output_file, mode='a', header=(mark==0), index=False)\n",
    "        \n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "-------- data_ToNIoT.csv ------------------------------\n",
      "--------------------------------------------------\n",
      "Processing dataset data_ToNIoT.csv with QuantileTransformer scaler ...\n",
      "..............................Data Overview................................\n",
      "Train Data Shape: (39028, 29)\n",
      "Test Data Shape: (9757, 29)\n",
      "Train Data Labels [0]: [0]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Number of samples after adding noise: 4491\n",
      "Number of features: 28\n",
      "Starting GMM clustering...\n",
      "Final number of clusters: 20\n",
      "Begin calculating NPD --------------\n",
      "shape of X (28, 4491)\n",
      "rank sw st : 24 24\n",
      "rank sw st : 24 24\n",
      "N = 4491   d = 28    c = 20\n",
      "P_w : d x N = (28, 4491)\n",
      "P_t : d x N = (28, 4491)\n",
      "S_w : d x d = (28, 28)\n",
      "S_t : d x d = (28, 28)\n",
      "Q   : d x N = (28, 28)\n",
      "B   : N x L = (28, 7)\n",
      "W   : d x L = (28, 7)\n",
      "Complexity of calculate:  [[-8.84600977e-05  8.06637448e-05  9.30883802e-06 ... -1.41668065e-15\n",
      "  -1.93627796e-18 -2.75470384e-33]\n",
      " [-8.89090849e-05  8.15465158e-05  9.28839773e-06 ... -9.34938430e-16\n",
      "  -2.53048838e-18 -5.58158172e-33]\n",
      " [-8.82393413e-05  8.07872815e-05  9.22745129e-06 ... -1.02979100e-15\n",
      "  -1.62867642e-18 -2.80905641e-33]\n",
      " ...\n",
      " [-9.25815559e-05  8.37825208e-05  9.36713918e-06 ... -2.10949143e-16\n",
      "  -1.26832797e-18 -3.03989853e-33]\n",
      " [-8.86813359e-05  8.11319564e-05  9.32730453e-06 ... -1.28102954e-15\n",
      "  -2.97265282e-18 -6.21225863e-33]\n",
      " [-8.96884608e-05  8.15279935e-05  9.37980251e-06 ... -1.30966781e-15\n",
      "  -1.70005477e-18 -2.21764401e-33]]\n",
      "...............................Timing Model................................\n",
      "Time train: 11.4676833152771\n",
      "Time test: 0.486236572265625\n",
      "..............................Report Parameter...............................\n",
      "AUCROC: 64.21078412549826\n",
      "AUCPR: 91.67158333100456\n",
      "Accuracy: 11.632673977657067\n",
      "MCC: 0.016279187133789542\n",
      "F1 score: 0.004617871161394597\n",
      "PPV (Precision): 1.0\n",
      "TPR (Recall): 0.0023142791020597086\n",
      "--------------------------------------------------\n",
      "-------- data_CICIoT2023.csv ------------------------------\n",
      "--------------------------------------------------\n",
      "Processing dataset data_CICIoT2023.csv with QuantileTransformer scaler ...\n",
      "..............................Data Overview................................\n",
      "Train Data Shape: (79611, 41)\n",
      "Test Data Shape: (19903, 41)\n",
      "Train Data Labels [0]: [0]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Number of samples after adding noise: 2396\n",
      "Number of features: 40\n",
      "Starting GMM clustering...\n",
      "Final number of clusters: 20\n",
      "Begin calculating NPD --------------\n",
      "shape of X (40, 2396)\n",
      "rank sw st : 32 31\n",
      "rank sw st : 32 31\n",
      "N = 2396   d = 40    c = 20\n",
      "P_w : d x N = (40, 2396)\n",
      "P_t : d x N = (40, 2396)\n",
      "S_w : d x d = (40, 40)\n",
      "S_t : d x d = (40, 40)\n",
      "Q   : d x N = (40, 40)\n",
      "B   : N x L = (40, 9)\n",
      "W   : d x L = (40, 9)\n",
      "Complexity of calculate:  [[ 2.29417713e-02  9.78461001e-17 -7.59896890e-17 ...  1.82770895e-16\n",
      "   1.78947669e-16 -3.85117143e-16]\n",
      " [ 2.29417713e-02  9.79362240e-17 -7.59743186e-17 ...  1.82745480e-16\n",
      "   1.78941915e-16 -3.85115695e-16]\n",
      " [ 2.29417713e-02  9.79199292e-17 -7.59850001e-17 ...  1.82758809e-16\n",
      "   1.78945661e-16 -3.85115562e-16]\n",
      " ...\n",
      " [ 2.29417713e-02  9.79431870e-17 -7.59961727e-17 ...  1.82765452e-16\n",
      "   1.78946564e-16 -3.85116520e-16]\n",
      " [ 2.29417713e-02  9.78818373e-17 -7.59863589e-17 ...  1.82765731e-16\n",
      "   1.78945923e-16 -3.85116875e-16]\n",
      " [ 2.29417713e-02  9.78669029e-17 -7.59945844e-17 ...  1.82772548e-16\n",
      "   1.78947018e-16 -3.85117927e-16]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3076204/4111812562.py:166: RuntimeWarning: divide by zero encountered in divide\n",
      "  y_proba[:, 1] = np.minimum(y_score / np.max(train_score), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............................Timing Model................................\n",
      "Time train: 5.3965654373168945\n",
      "Time test: 0.4547591209411621\n",
      "..............................Report Parameter...............................\n",
      "AUCROC: 50.0\n",
      "AUCPR: 96.9652816158368\n",
      "Accuracy: 96.9652816158368\n",
      "MCC: 0.0\n",
      "F1 score: 0.9845926228253661\n",
      "PPV (Precision): 0.9696528161583681\n",
      "TPR (Recall): 1.0\n",
      "--------------------------------------------------\n",
      "-------- data_N_BaIoT.csv ------------------------------\n",
      "--------------------------------------------------\n",
      "Processing dataset data_N_BaIoT.csv with QuantileTransformer scaler ...\n",
      "..............................Data Overview................................\n",
      "Train Data Shape: (75976, 116)\n",
      "Test Data Shape: (18994, 116)\n",
      "Train Data Labels [0]: [0]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Number of samples after adding noise: 7141\n",
      "Number of features: 115\n",
      "Starting GMM clustering...\n",
      "Final number of clusters: 20\n",
      "Begin calculating NPD --------------\n",
      "shape of X (115, 7141)\n",
      "rank sw st : 115 115\n",
      "rank sw st : 115 115\n",
      "N = 7141   d = 115    c = 20\n",
      "P_w : d x N = (115, 7141)\n",
      "P_t : d x N = (115, 7141)\n",
      "S_w : d x d = (115, 115)\n",
      "S_t : d x d = (115, 115)\n",
      "Q   : d x N = (115, 115)\n",
      "B   : N x L = (115, 0)\n",
      "W   : d x L = (115, 0)\n",
      "Complexity of calculate:  []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3076204/4111812562.py:166: RuntimeWarning: invalid value encountered in divide\n",
      "  y_proba[:, 1] = np.minimum(y_score / np.max(train_score), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............................Timing Model................................\n",
      "Time train: 15.930247783660889\n",
      "Time test: 0.953881025314331\n",
      "..............................Report Parameter...............................\n",
      "AUCROC: 50.0\n",
      "AUCPR: 90.33378961777403\n",
      "Accuracy: 90.33378961777403\n",
      "MCC: 0.0\n",
      "F1 score: 0.94921442797079\n",
      "PPV (Precision): 0.9033378961777403\n",
      "TPR (Recall): 1.0\n",
      "--------------------------------------------------\n",
      "-------- data_BoTIoT.csv ------------------------------\n",
      "--------------------------------------------------\n",
      "Processing dataset data_BoTIoT.csv with QuantileTransformer scaler ...\n",
      "..............................Data Overview................................\n",
      "Train Data Shape: (76491, 27)\n",
      "Test Data Shape: (19123, 27)\n",
      "Train Data Labels [0]: [0]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Number of samples after adding noise: 7275\n",
      "Number of features: 26\n",
      "Starting GMM clustering...\n",
      "Final number of clusters: 20\n",
      "Begin calculating NPD --------------\n",
      "shape of X (26, 7275)\n",
      "rank sw st : 26 26\n",
      "rank sw st : 26 26\n",
      "N = 7275   d = 26    c = 20\n",
      "P_w : d x N = (26, 7275)\n",
      "P_t : d x N = (26, 7275)\n",
      "S_w : d x d = (26, 26)\n",
      "S_t : d x d = (26, 26)\n",
      "Q   : d x N = (26, 26)\n",
      "B   : N x L = (26, 0)\n",
      "W   : d x L = (26, 0)\n",
      "Complexity of calculate:  []\n",
      "...............................Timing Model................................\n",
      "Time train: 3.153191089630127\n",
      "Time test: 0.8938705921173096\n",
      "..............................Report Parameter...............................\n",
      "AUCROC: 50.0\n",
      "AUCPR: 90.60293886942425\n",
      "Accuracy: 90.60293886942425\n",
      "MCC: 0.0\n",
      "F1 score: 0.9506982358912452\n",
      "PPV (Precision): 0.9060293886942425\n",
      "TPR (Recall): 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3076204/4111812562.py:166: RuntimeWarning: invalid value encountered in divide\n",
      "  y_proba[:, 1] = np.minimum(y_score / np.max(train_score), 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cProfile\n",
    "import pstats    \n",
    "\n",
    "# dataset_prefixes =  ['N_BaIoT_dataloader.csv']\n",
    "dataset_prefixes =  ['data_ToNIoT.csv' , 'data_CICIoT2023.csv','data_N_BaIoT.csv', 'data_BoTIoT.csv']\n",
    "# dataset_prefixes =  ['data_CICIoT2023.csv']\n",
    "\n",
    "\n",
    "#  'data_N_BaIoT.csv',  \n",
    "# scaler_names = ['MinMaxScaler']\n",
    "scaler_names = ['QuantileTransformer'] # ,'MinMaxScaler','Normalizer','RobustScaler', 'StandardScaler' ]\n",
    "  \n",
    "# scaler_names = ['QuantileTransformer', 'StandardScaler','QuantileTransformer'] 'StandardScaler', \n",
    "\n",
    "    # Iterate through dataset prefixes and process each dataset pair\n",
    "\n",
    "\n",
    "\n",
    "for prefix in dataset_prefixes:\n",
    "    \n",
    "    print(\"-\"*50)\n",
    "    print(\"--------\", prefix , \"-\"*30)\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    #  NFST/GMM-nfst/Structure_Result/NOISEGAU \n",
    "    base_output_file = f\"{prefix}\" \n",
    "    output_file = base_output_file + \"0.csv\"\n",
    "\n",
    "    # Increment the filename if it already exists\n",
    "    counter = 0\n",
    "    while os.path.exists(output_file):\n",
    "        counter += 1\n",
    "        output_file = f\"{base_output_file}{counter}.csv\"\n",
    "\n",
    "        \n",
    "    for scaler in scaler_names: \n",
    "        print(f\"Processing dataset {prefix} with {scaler} scaler ...\")\n",
    "        \n",
    "        # Construct file paths for train and test datasets with 'Train_' and 'Test_' prefixes\n",
    "        train_file = f'../../Datascaled/NoiseOCData/Train_{scaler}_{prefix}'\n",
    "        test_file = f'../../Datascaled/NoiseOCData/Test_{scaler}_{prefix}'\n",
    "        \n",
    "        # Load the CSV files\n",
    "        df_train = pd.read_csv(train_file)\n",
    "        df_test = pd.read_csv(test_file)\n",
    "\n",
    "        for noise in [0]: # ,1,3,5] : \n",
    "            \n",
    "            # log_file = f'Results/OURMODEL/SCALERS/{prefix}_modellog.txt'\n",
    "            # with open(log_file, \"w\") as f:\n",
    "            #     profiler = cProfile.Profile()\n",
    "            #     profiler.enable()\n",
    "                \n",
    "            function(df_train, df_test, scaler, noise)  # Chạy hàm cần profile\n",
    "            \n",
    "                # profiler.disable()\n",
    "                # stats = pstats.Stats(profiler, stream=f)\n",
    "                # stats.sort_stats(\"cumulative\").print_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
