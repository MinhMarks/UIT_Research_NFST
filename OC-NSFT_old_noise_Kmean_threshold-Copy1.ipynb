{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (roc_auc_score, precision_score, average_precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error, roc_curve, auc, classification_report,auc,confusion_matrix,matthews_corrcoef)\n",
    "from sklearn import logger\n",
    "from sklearn.datasets import make_blobs,make_multilabel_classification\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import KernelCenterer,LabelEncoder, MinMaxScaler, Normalizer, QuantileTransformer, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import scipy as sp\n",
    "from scipy.linalg import svd,null_space\n",
    "import os\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.cluster import KMeans,AgglomerativeClustering,SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.sparse import csr_matrix as sp\n",
    "import math\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.spatial.distance import cdist\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Import 3D plotting tools\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hàm\tĐộ phức tạp tính toán\tBộ nhớ cần\n",
    "# preprocess_data_OC\tO(n⋅d²)\tO(n⋅d²)\n",
    "# clusterr\tO(n⋅d⋅k⋅iters)\tO(n⋅d)\n",
    "# nullspace\tO(d³)\tO(d²)\n",
    "# gram_schmidt\tO(d³)\tO(d²)\n",
    "# minimum_distance\tO(n⋅m⋅d)\tO(n⋅m)\n",
    "# distance_vector\tO(n⋅m⋅d)\tO(n⋅m)\n",
    "# calculate_NPD\tO(d³)\tO(d²)\n",
    "# learn\tO(n²⋅d)\tO(n²)\n",
    "\n",
    "\n",
    "\n",
    "alpha = 0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data_noise(train_data, test_data, noise_percentage=10):\n",
    "  \n",
    "    print(\"..............................Data Overview................................\")\n",
    "    print(\"Train Data Shape:\", train_data.shape)\n",
    "    print(\"Test Data Shape:\", test_data.shape)\n",
    "    \n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    X_train_total = train_data.iloc[:, :-1].to_numpy()\n",
    "    y_train_total = train_data.iloc[:, -1].to_numpy()\n",
    "\n",
    "    # Separate the samples with label 0\n",
    "    X_train = X_train_total[y_train_total == 0]\n",
    "    y_train = y_train_total[y_train_total == 0]\n",
    "\n",
    "    print(\"Train Data Labels [0]:\", np.unique(y_train))\n",
    "\n",
    "    # Calculate how many samples to add noise to based on the provided percentage\n",
    "    n_samples = X_train.shape[0]\n",
    "    noise_samples_count = int(n_samples * (noise_percentage / 100))\n",
    "\n",
    "    # Get the samples with label 1 (for generating noise)\n",
    "    X_train_noise = X_train_total[y_train_total == 1]\n",
    "    \n",
    "    # Randomly select noise_samples_count from X_train_noise\n",
    "    noisy_indices = np.random.choice(X_train_noise.shape[0], size=noise_samples_count, replace=False)\n",
    "    X_train_noise = X_train_noise[noisy_indices]\n",
    "    \n",
    "    # Add the noisy samples to the training set\n",
    "    X_train = np.vstack((X_train, X_train_noise))\n",
    "    y_train = np.concatenate((y_train, np.ones(X_train_noise.shape[0])))\n",
    "    print(y_train) \n",
    "    \n",
    "    # Prepare test data\n",
    "    X_test = test_data.iloc[:, :-1].to_numpy()\n",
    "    y_test = test_data.iloc[:, -1].to_numpy()\n",
    "\n",
    "    # Print the new size of training data\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_features = X_train.shape[1]\n",
    "    print(\"Number of samples after adding noise:\", n_samples)\n",
    "    print(\"Number of features:\", n_features)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def cluster_kmeans(data, initial_k):\n",
    "    print(\"Starting K-Means clustering...\")\n",
    "\n",
    "    # Thực hiện K-Means clustering với số cụm initial_k\n",
    "    kmeans = KMeans(n_clusters=initial_k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(data)\n",
    "\n",
    "    # Gán nhãn cụm vào biến labels\n",
    "    labels = cluster_labels\n",
    "\n",
    "    # Sắp xếp dữ liệu theo nhãn cụm\n",
    "    sorted_indices = np.argsort(labels)\n",
    "    sorted_data = data[sorted_indices]\n",
    "    sorted_labels = labels[sorted_indices]\n",
    "\n",
    "    print(\"Final number of clusters:\", len(np.unique(sorted_labels)))\n",
    "    return sorted_data, sorted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nullspace(A):\n",
    "    _, s, vh = np.linalg.svd(A)\n",
    "    null_mask = np.isclose(s, 0)\n",
    "    null_space = vh[null_mask].T\n",
    "    return null_space\n",
    "\n",
    "\n",
    "def minimum_distance( A, B):\n",
    "        \"\"\"\n",
    "        Compute the minimum Euclidean distance from each point in A to all points in B.\n",
    "        \n",
    "        Parameters:\n",
    "        - A (ndarray): Set of points (N_A, d).\n",
    "        - B (ndarray): Set of points (N_B, d).\n",
    "        \n",
    "        Returns:\n",
    "        - min_distances (ndarray): Minimum distances from each point in A to the nearest point in B.\n",
    "        \"\"\"\n",
    "        A = np.asarray(A)\n",
    "        B = np.asarray(B)\n",
    "        \n",
    "        # Initialize an array for storing minimum distances\n",
    "        min_distances = np.empty(A.shape[0], dtype=np.float64)\n",
    "        \n",
    "        # Iterate over each point in A and calculate minimum distance to points in B\n",
    "        for i, a in enumerate(A):\n",
    "            distances = cdist([a], B, metric='euclidean')  # Compute all pairwise distances\n",
    "            min_distances[i] = np.min(distances)  # Store minimum distance\n",
    "        \n",
    "        return min_distances   \n",
    "\n",
    "def distance_vector(point_X, point_Y):\n",
    "    \"\"\"\n",
    "    Calculate pairwise Euclidean distance between two sets of points.\n",
    "    \n",
    "    Args:\n",
    "        point_X (ndarray): Array of shape (N_train, d) where N_train is the number of training samples and d is the number of features.\n",
    "        point_Y (ndarray): Array of shape (N_test, d) where N_test is the number of test samples and d is the number of features.\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: Distance matrix of shape (N_test, N_train) containing Euclidean distances between each pair of points.\n",
    "    \"\"\"\n",
    "    print( 'Complexity of calculate: ', point_X )\n",
    "    # Compute squared norms for each point\n",
    "    norm_X = np.sum(point_X**2, axis=1)  # (N_train,)\n",
    "    norm_Y = np.sum(point_Y**2, axis=1)  # (N_test,)\n",
    "    \n",
    "    # Compute the dot product between the two sets of points\n",
    "    dot_product = np.dot(point_Y, point_X.T)  # (N_test, N_train)\n",
    "    \n",
    "    # Apply Euclidean distance formula\n",
    "    distance = np.sqrt(abs(norm_Y[:, np.newaxis] + norm_X[np.newaxis, :] - 2 * dot_product))\n",
    "    return distance\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import null_space  # Sử dụng null_space thay vì nullspace tự định nghĩa\n",
    "\n",
    "def calculate_NPD(X, y, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Tính Null Projecting Directions (NPDs) và ước lượng hằng số k.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Ma trận dữ liệu (d x N), với d là số đặc trưng, N là số mẫu.\n",
    "    - y: Nhãn cluster/lớp (N,).\n",
    "    - epsilon: Ngưỡng để xác định giá trị kỳ dị khác không.\n",
    "    \n",
    "    Returns:\n",
    "    - W: Ma trận NPDs (d x L).\n",
    "    - k: Hằng số k, ước lượng mức độ giảm của rank(P_w).\n",
    "    \"\"\"\n",
    "    print(\"Begin calculating NPD and k --------------\")\n",
    "    X = X.T  # Chuyển thành d x N\n",
    "    print('Shape of X:', X.shape)\n",
    "    \n",
    "    c = len(np.unique(y))  # Số lớp hoặc cluster\n",
    "    d, N = X.shape  # Số đặc trưng và số mẫu\n",
    "    \n",
    "    # Tính trung bình toàn cục và tạo ma trận P_t với zero-mean\n",
    "    mean_total = np.mean(X, axis=1, keepdims=True)\n",
    "    P_t = X - mean_total  # P_t: d x N\n",
    "    \n",
    "    # Tính P_w cho từng lớp\n",
    "    P_w = np.zeros_like(X)\n",
    "    for i in np.unique(y):\n",
    "        class_mean = np.mean(X[:, y == i], axis=1, keepdims=True)\n",
    "        P_w[:, y == i] = X[:, y == i] - class_mean  # P_w: d x N\n",
    "    \n",
    "    # Tính ma trận phương sai S_w và S_t\n",
    "    S_w = np.dot(P_w, P_w.T) / N  # S_w: d x d\n",
    "    S_t = np.dot(P_t, P_t.T) / N  # S_t: d x d\n",
    "    \n",
    "    # Tính rank của P_w và P_t bằng SVD\n",
    "    _, singular_values_Pw, _ = np.linalg.svd(P_w, full_matrices=False)\n",
    "    rank_Pw = np.sum(singular_values_Pw > epsilon)  # Rank của P_w\n",
    "    _, singular_values_Pt, _ = np.linalg.svd(P_t, full_matrices=False)\n",
    "    rank_Pt = np.sum(singular_values_Pt > epsilon)  # Rank của P_t\n",
    "    \n",
    "    print(\"Rank S_w:\", np.linalg.matrix_rank(S_w, tol=epsilon))\n",
    "    print(\"Rank S_t:\", np.linalg.matrix_rank(S_t, tol=epsilon))\n",
    "    print(\"Rank P_w:\", rank_Pw)\n",
    "    print(\"Rank P_t:\", rank_Pt)\n",
    "    \n",
    "    # Ước lượng k\n",
    "    theoretical_rank_Pw = N - c  # Giới hạn trên của rank(P_w)\n",
    "    k = theoretical_rank_Pw - rank_Pw\n",
    "    print(\"Theoretical rank(P_w) = N - c =\", theoretical_rank_Pw)\n",
    "    print(\"Estimated k =\", k)\n",
    "    \n",
    "    # Tính ma trận Q từ SVD của P_t\n",
    "    U, _, _ = np.linalg.svd(P_t, full_matrices=False)\n",
    "    Q = U  # Q: d x rank(P_t)\n",
    "    \n",
    "    # Tính nullspace của Q.T @ S_w @ Q\n",
    "    B = null_space(Q.T @ S_w @ Q)  # B: rank(P_t) x L\n",
    "    \n",
    "    # Tính ma trận NPDs\n",
    "    W = Q @ B  # W: d x L\n",
    "    \n",
    "    # In thông tin\n",
    "    print(\"N =\", N, \"d =\", d, \"c =\", c)\n",
    "    print(\"P_w : d x N =\", P_w.shape)\n",
    "    print(\"P_t : d x N =\", P_t.shape)\n",
    "    print(\"S_w : d x d =\", S_w.shape)\n",
    "    print(\"S_t : d x d =\", S_t.shape)\n",
    "    print(\"Q   : d x rank(P_t) =\", Q.shape)\n",
    "    print(\"B   : rank(P_t) x L =\", B.shape)\n",
    "    print(\"W   : d x L =\", W.shape)\n",
    "    print(\"Threshold c_th =\", N - d - k + 1)\n",
    "    \n",
    "    return W, k\n",
    "\n",
    "    \n",
    "\n",
    "def BruteForce_Threshold(y_true, y_prob, minth=0.0, maxth=1.0, num_thresholds=1000):\n",
    "    \"\"\"\n",
    "    Finds the best classification threshold for a binary model using brute force search.\n",
    "\n",
    "    Args:\n",
    "        y_true (ndarray): True labels (0 or 1), shape (n_samples,).\n",
    "        y_prob (ndarray): Predicted probabilities for class 1, shape (n_samples,).\n",
    "        minth (float, optional): Minimum threshold value. Default is 0.0.\n",
    "        maxth (float, optional): Maximum threshold value. Default is 1.0.\n",
    "        num_thresholds (int, optional): Number of threshold values to search. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the best threshold for each evaluation metric.\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(minth, maxth, num_thresholds)  # Generate candidate thresholds\n",
    "\n",
    "    # Initialize best metrics\n",
    "    best_results = {\n",
    "        \"accuracy\": (0, 0),  # (best_threshold, best_score)\n",
    "        \"f1\": (0, 0),\n",
    "        \"mcc\": (0, 0),\n",
    "        \"auc_roc\": (0, 0),\n",
    "        \"auc_pr\": (0, 0)\n",
    "    }\n",
    "\n",
    "    return best_results \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob[:,1] >= threshold).astype(int)\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "        auc_roc = roc_auc_score(y_true, y_prob[:, 1])\n",
    "        auc_pr = average_precision_score(y_true, y_prob[:, 1])\n",
    "\n",
    "        # Update best threshold for each metric\n",
    "        if acc > best_results[\"accuracy\"][1]:\n",
    "            best_results[\"accuracy\"] = (threshold, acc)\n",
    "        if f1 > best_results[\"f1\"][1]:\n",
    "            best_results[\"f1\"] = (threshold, f1)\n",
    "        if mcc > best_results[\"mcc\"][1]:\n",
    "            best_results[\"mcc\"] = (threshold, mcc)\n",
    "        if auc_roc > best_results[\"auc_roc\"][1]:\n",
    "            best_results[\"auc_roc\"] = (threshold, auc_roc)\n",
    "        if auc_pr > best_results[\"auc_pr\"][1]:\n",
    "            best_results[\"auc_pr\"] = (threshold, auc_pr)\n",
    "    return best_results\n",
    "\n",
    "\n",
    "def learn( npd, X_train, y_train , X_test, t0):\n",
    "    '''\n",
    "    X_train n1, d \n",
    "\n",
    "    C: n * n * d \n",
    "    '''\n",
    "    t1 = time.time()\n",
    "    null_point_X = (sp(X_train).dot(sp(npd))).toarray()\n",
    "    null_point_X_test = (sp(X_test).dot(sp(npd))).toarray()  \n",
    "\n",
    "    train_score_tmp = distance_vector(null_point_X, null_point_X)\n",
    "    for i in range(len(train_score_tmp)):\n",
    "        train_score_tmp[i , i] = 1e9                                                      \n",
    "    train_score = np.amin(train_score_tmp, axis=1)\n",
    "    t2 = time.time()\n",
    "    \n",
    "    y_score = minimum_distance(null_point_X_test, null_point_X)\n",
    "    y_proba = np.zeros((len(y_score), 2))\n",
    "    # percentile_threshold = np.percentile(train_score, 95)\n",
    "    # y_proba[:, 1] = np.minimum(y_score / percentile_threshold, 1)\n",
    "    y_proba[:, 1] = np.minimum(y_score / np.max(train_score), 1)                         \n",
    "    y_proba[:, 0] = 1 - y_proba[:, 1]                                                     # Probability for class 0\n",
    "    y_proba = np.nan_to_num(y_proba, nan=1.0)\n",
    "    y_predict = (y_proba[:, 1] > 0.001).astype(int) \n",
    "    \n",
    "    print(\"...............................Timing Model................................\")\n",
    "    print(\"Time train:\", t1-t0)\n",
    "    print(\"Time test:\" , t2-t1)\n",
    "    return y_proba, y_predict, t1, t2  \n",
    "    \n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "def Model_evaluating(y_true, y_predict, y_scores):\n",
    "    \"\"\"\n",
    "    Evaluate the model using threshold derived from ROC curve (Youden’s J statistic).\n",
    "    \n",
    "    Args:\n",
    "        y_true (ndarray): True labels (binary: 0 or 1).\n",
    "        y_scores (ndarray): Predicted probabilities for each class (2D array).\n",
    "        \n",
    "    Returns:\n",
    "        list: List of evaluation metrics [AUC, AUCPR, Accuracy, MCC, F1, Precision, Recall].\n",
    "    \"\"\"\n",
    "    print(\"..............................Report Parameter...............................\")\n",
    "    \n",
    "    # Lấy xác suất cho lớp dương (lớp 1)\n",
    "    y_prob = y_scores[:, 1]\n",
    "    \n",
    "    # Tính ROC và threshold tối ưu theo Youden’s J statistic\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    j_scores = tpr - fpr\n",
    "    optimal_idx = np.argmax(j_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    print(\"Optimal threshold (Youden's J):\", optimal_threshold)\n",
    "\n",
    "    # Dự đoán nhãn với threshold tối ưu\n",
    "    y_predict_optimal = (y_prob >= optimal_threshold).astype(int)\n",
    "\n",
    "    # Tính các chỉ số đánh giá\n",
    "    mcc = matthews_corrcoef(y_true, y_predict_optimal)\n",
    "    f1 = f1_score(y_true, y_predict_optimal)\n",
    "    ppv = precision_score(y_true, y_predict_optimal)\n",
    "    recall = recall_score(y_true, y_predict_optimal)\n",
    "    accuracy = accuracy_score(y_true, y_predict_optimal)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    aucpr = average_precision_score(y_true, y_prob)\n",
    "    \n",
    "    # In ra các kết quả\n",
    "    print(\"AUCROC:\", auc * 100)\n",
    "    print(\"AUCPR:\", aucpr * 100)\n",
    "    print(\"Accuracy:\", accuracy * 100)\n",
    "    print(\"MCC:\", mcc)\n",
    "    print(\"F1 score:\", f1)\n",
    "    print(\"PPV (Precision):\", ppv)\n",
    "    print(\"TPR (Recall):\", recall)\n",
    "\n",
    "    return [auc * 100, aucpr * 100, accuracy * 100, mcc, f1, ppv, recall]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train, X_test, y_test = preprocess_data_OC(df2, df1)\n",
    "\n",
    "# X_total = np.vstack((X_train, X_test))\n",
    "# unique_rows = np.unique(X_total, axis=0)\n",
    "\n",
    "# print(\"Số dòng trong X_total:\", X_total.shape[0])   # Output: 5\n",
    "# print(\"Số dòng khác nhau:\", unique_rows.shape[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files from the GMM-nfst folder\n",
    "\n",
    "columns = [\"scaler\",\"nCluster\", 'noise_percentage', \"AUCROC\", \"AUCPR\", \"Accuracy\", \"MCC\", \"F1 Score\",\n",
    "           \"Precision\", \"Recall\", \"Time Train\", \"Time Test\"]\n",
    "# Now you can use df1 and df2 as DataFrames\n",
    "\n",
    "#CIC_IoT2023_1000.csv\n",
    "#N_BaIoT_1000.csv\n",
    "#BoT_IoT_1000.csv\n",
    "\n",
    "import cProfile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# def plot_data_2D(X, labels, title=\"Interactive 3D PCA Plot\"):\n",
    "#     \"\"\" Vẽ dữ liệu với PCA và tạo đồ thị 3D có thể tương tác bằng Plotly \"\"\"\n",
    "\n",
    "#     # Ensure that n_components ≤ min(n_samples, n_features)\n",
    "#     n_components = min(3, X.shape[0], X.shape[1])\n",
    "#     if n_components < 3:\n",
    "#         print(f\"⚠️ Warning: Cannot plot 3D, because n_components={n_components}. Skipping...\")\n",
    "#         return\n",
    "    \n",
    "#     # Perform PCA\n",
    "#     pca = PCA(n_components=3)\n",
    "#     X_3D = pca.fit_transform(X)\n",
    "\n",
    "#     # Convert labels to a NumPy array\n",
    "#     labels = np.array(labels)\n",
    "\n",
    "#     # Create a DataFrame for better visualization\n",
    "#     import pandas as pd\n",
    "#     df = pd.DataFrame(X_3D, columns=['PCA 1', 'PCA 2', 'PCA 3'])\n",
    "#     df['Class'] = labels\n",
    "\n",
    "#     # Create an interactive 3D scatter plot\n",
    "#     fig = px.scatter_3d(df, x='PCA 1', y='PCA 2', z='PCA 3', \n",
    "#                          color=df['Class'].astype(str),  # Color by class\n",
    "#                          title=title, labels={'color': 'Class'},\n",
    "#                          opacity=0.8)\n",
    "\n",
    "#     fig.update_traces(marker=dict(size=5))  # Adjust marker size\n",
    "#     fig.show()\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def plot_data_2D(X, labels, title=\"Dữ liệu gốc trước khi biến đổi\"):\n",
    "    \"\"\" Vẽ dữ liệu với PCA để giảm xuống 2D \"\"\"\n",
    "    return 0 \n",
    "    pca = PCA(n_components=3)\n",
    "    X_2D = pca.fit_transform(X)  # Chuyển về dạng (100, 2)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    labels = np.array(labels)  \n",
    "    for i in np.unique(labels):\n",
    "        plt.scatter(X_2D[labels == i, 0], X_2D[labels == i, 1], label=f\"Class {i}\", alpha=0.7)\n",
    "\n",
    "    plt.xlabel(\"PCA 1\")\n",
    "    plt.ylabel(\"PCA 2\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def function(df1,df2, scaler, noise):\n",
    "\n",
    "    X_train0, y_train0, X_test, y_test = preprocess_data_noise( df1,df2, noise)\n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"mean\") \n",
    "    X_train0[np.isinf(X_train0)] = np.nan  # Đổi vô hạn thành NaN\n",
    "    X_train0 = imputer.fit_transform(X_train0)\n",
    "    \n",
    "    for ncluster in (1,139,184,301):\n",
    "        t0 = time.time()\n",
    "        X_train , y_train = cluster_kmeans( X_train0 , ncluster )\n",
    "        plot_data_2D(X_train, y_train, \"Du lieu sau clusterr\") \n",
    "        npd = calculate_NPD(X_train, y_train)\n",
    "        \n",
    "        y_proba, y_predict, t1, t2 = learn( npd , X_train, y_train, X_test, y_test)  #,y_predict\n",
    "        \n",
    "        # print(\"...............................Timing Model................................\")\n",
    "        # print(\"Time train:\", t1-t0)\n",
    "        # print(\"Time test:\" , t2-t1)\n",
    "        # print(\"...........................................................................\")\n",
    "        # print(y_predict)\n",
    "        v = Model_evaluating(y_test, y_predict, y_proba)\n",
    "        # best_thresholds = BruteForce_Threshold( y_test, y_proba, 0, 1)  \n",
    "        result = [scaler] + [ncluster] + [noise] + v + [t1-t0, t2-t1]\n",
    "        # for metric, (threshold, score) in best_thresholds.items():\n",
    "        #     result = result + [threshold, score ]  \n",
    "\n",
    "        result_df = pd.DataFrame([result], columns=columns)\n",
    "        result_df.to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)\n",
    "        # result_df.to_csv(output_file, mode='a', header=(mark==0), index=False)\n",
    "        \n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "-------- data_ToNIoT ------------------------------\n",
      "--------------------------------------------------\n",
      "Processing dataset data_ToNIoT with StandardScaler scaler ...\n",
      "..............................Data Overview................................\n",
      "Train Data Shape: (34149, 29)\n",
      "Test Data Shape: (14636, 29)\n",
      "Train Data Labels [0]: [0]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Number of samples after adding noise: 3888\n",
      "Number of features: 28\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "Begin calculating NPD and k --------------\n",
      "Shape of X: (28, 3888)\n",
      "Rank S_w: 23\n",
      "Rank S_t: 23\n",
      "Rank P_w: 24\n",
      "Rank P_t: 24\n",
      "Theoretical rank(P_w) = N - c = 3887\n",
      "Estimated k = 3863\n",
      "N = 3888 d = 28 c = 1\n",
      "P_w : d x N = (28, 3888)\n",
      "P_t : d x N = (28, 3888)\n",
      "S_w : d x d = (28, 28)\n",
      "S_t : d x d = (28, 28)\n",
      "Q   : d x rank(P_t) = (28, 28)\n",
      "B   : rank(P_t) x L = (28, 4)\n",
      "W   : d x L = (28, 4)\n",
      "Threshold c_th = -2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.int64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 61\u001b[0m\n\u001b[1;32m     52\u001b[0m df_train_new, df_test_new \u001b[38;5;241m=\u001b[39m train_test_split(df_full, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m noise \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# for noise in range (0, 6): \u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m#     profiler = cProfile.Profile()\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m#     profiler.enable()\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_test_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Chạy hàm cần profile\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;66;03m# profiler.disable()\u001b[39;00m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;66;03m# stats = pstats.Stats(profiler, stream=f)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;66;03m# stats.sort_stats(\"cumulative\").print_stats()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 87\u001b[0m, in \u001b[0;36mfunction\u001b[0;34m(df1, df2, scaler, noise)\u001b[0m\n\u001b[1;32m     84\u001b[0m plot_data_2D(X_train, y_train, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDu lieu sau clusterr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m     85\u001b[0m npd \u001b[38;5;241m=\u001b[39m calculate_NPD(X_train, y_train)\n\u001b[0;32m---> 87\u001b[0m y_proba, y_predict, t1, t2 \u001b[38;5;241m=\u001b[39m \u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpd\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#,y_predict\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# print(\"...............................Timing Model................................\")\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# print(\"Time train:\", t1-t0)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# print(\"Time test:\" , t2-t1)\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# print(\"...........................................................................\")\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# print(y_predict)\u001b[39;00m\n\u001b[1;32m     94\u001b[0m v \u001b[38;5;241m=\u001b[39m Model_evaluating(y_test, y_predict, y_proba)\n",
      "Cell \u001b[0;32mIn[4], line 193\u001b[0m, in \u001b[0;36mlearn\u001b[0;34m(npd, X_train, y_train, X_test, t0)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03mX_train n1, d \u001b[39;00m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03mC: n * n * d \u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    192\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 193\u001b[0m null_point_X \u001b[38;5;241m=\u001b[39m (sp(X_train)\u001b[38;5;241m.\u001b[39mdot(\u001b[43msp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpd\u001b[49m\u001b[43m)\u001b[49m))\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m    194\u001b[0m null_point_X_test \u001b[38;5;241m=\u001b[39m (sp(X_test)\u001b[38;5;241m.\u001b[39mdot(sp(npd)))\u001b[38;5;241m.\u001b[39mtoarray()  \n\u001b[1;32m    196\u001b[0m train_score_tmp \u001b[38;5;241m=\u001b[39m distance_vector(null_point_X, null_point_X)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/sparse/_compressed.py:56\u001b[0m, in \u001b[0;36m_cs_matrix.__init__\u001b[0;34m(self, arg1, shape, dtype, copy, maxprint)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arg1) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;66;03m# (data, ij) format\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m         coo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coo_container\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m coo\u001b[38;5;241m.\u001b[39m_coo_to_compressed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap)\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindptr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape \u001b[38;5;241m=\u001b[39m arrays\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/sparse/_coo.py:51\u001b[0m, in \u001b[0;36m_coo_base.__init__\u001b[0;34m(self, arg1, shape, dtype, copy, maxprint)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid input format\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mlen\u001b[39m(idx) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m coords):\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcannot infer dimensions from zero \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     53\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msized index arrays\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     54\u001b[0m     shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(operator\u001b[38;5;241m.\u001b[39mindex(np\u001b[38;5;241m.\u001b[39mmax(idx)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     55\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m coords)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.int64' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "import cProfile\n",
    "import pstats    \n",
    "\n",
    "# dataset_prefixes =  ['N_BaIoT_dataloader.csv']\n",
    "dataset_prefixes =  ['data_ToNIoT', 'data_CICIoT2023','data_N_BaIoT','data_BoTIoT' ] # ,'data_N_BaIoT', 'data_BoTIoT']\n",
    "# dataset_prefixes =  ['data_CICIoT2023.csv']\n",
    "\n",
    "#  'data_N_BaIoT.csv',  \n",
    "# scaler_names = ['MinMaxScaler']\n",
    "scaler_names = ['StandardScaler', 'MinMaxScaler','Normalizer',  'QuantileTransformer','RobustScaler','Normalizer']\n",
    "#  \n",
    "# scaler_names = ['QuantileTransformer', 'StandardScaler','QuantileTransformer']\n",
    "\n",
    "    # Iterate through dataset prefixes and process each dataset pair\n",
    "\n",
    "\n",
    "\n",
    "for prefix in dataset_prefixes:\n",
    "    \n",
    "    print(\"-\"*50)\n",
    "    print(\"--------\", prefix , \"-\"*30)\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    base_output_file = f\"Structure_Result/{prefix}_noise0t5ttestk.csv\" \n",
    "    output_file = base_output_file + \"0.csv\"\n",
    "\n",
    "    # Increment the filename if it already exists\n",
    "    counter = 0\n",
    "    while os.path.exists(output_file):\n",
    "        counter += 1\n",
    "        output_file = f\"{base_output_file}{counter}ddd.csv\"\n",
    "\n",
    "        \n",
    "    for scaler in scaler_names: \n",
    "        print(f\"Processing dataset {prefix} with {scaler} scaler ...\")\n",
    "        \n",
    "        # Construct file paths for train and test datasets with 'Train_' and 'Test_' prefixes\n",
    "        train_file = f'Datascaled/NoiseOCData/Train_{scaler}_{prefix}.csv'\n",
    "        test_file = f'Datascaled/NoiseOCData/Test_{scaler}_{prefix}.csv'\n",
    "        \n",
    "        # Load the CSV files\n",
    "        df_train = pd.read_csv(train_file)\n",
    "        df_test = pd.read_csv(test_file)\n",
    "\n",
    "        df_train = df_train.dropna() \n",
    "        df_test = df_test.dropna() \n",
    "            \n",
    "            # Nối lại thành 1 DataFrame\n",
    "        df_full = pd.concat([df_train, df_test], ignore_index=True)\n",
    "            \n",
    "            # Chia theo tỉ lệ 70% train, 30% test\n",
    "        df_train_new, df_test_new = train_test_split(df_full, test_size=0.3, random_state=42)\n",
    "        for noise in [0]:\n",
    "        # for noise in range (0, 6): \n",
    "            \n",
    "            # log_file = f'Results/OURMODEL/SCALERS/{prefix}_modellog.txt'\n",
    "            # with open(log_file, \"w\") as f:\n",
    "            #     profiler = cProfile.Profile()\n",
    "            #     profiler.enable()\n",
    "                \n",
    "            function(df_train_new, df_test_new, scaler, noise)  # Chạy hàm cần profile\n",
    "            \n",
    "                # profiler.disable()\n",
    "                # stats = pstats.Stats(profiler, stream=f)\n",
    "                # stats.sort_stats(\"cumulative\").print_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
