{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f10ad9-0167-4d96-970d-cb415f9c4d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (roc_auc_score, precision_score, average_precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error, roc_curve, auc, classification_report,auc,confusion_matrix,matthews_corrcoef)\n",
    "from sklearn import logger\n",
    "from sklearn.datasets import make_blobs,make_multilabel_classification\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import KernelCenterer,LabelEncoder, MinMaxScaler, Normalizer, QuantileTransformer, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import scipy as sp\n",
    "from scipy.linalg import svd,null_space\n",
    "import os    \n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.cluster import KMeans,AgglomerativeClustering,SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.sparse import csr_matrix as sp\n",
    "import math\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.spatial.distance import cdist\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Import 3D plotting tools\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f2204b-5ec6-49c2-8a5c-dfeb9172856b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "-------- data_CICIoT2023.csv ------------------------------\n",
      "--------------------------------------------------\n",
      "Processing dataset data_CICIoT2023.csv with MinMaxScaler scaler ...\n",
      "..............................Data Overview................................\n",
      "Train Data Shape: (69659, 41)\n",
      "Test Data Shape: (29855, 41)\n",
      "Train Data Labels [0]: [0]\n",
      "Number of samples after adding noise: 2119\n",
      "Number of features: 40\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "N = 2119, d = 40, c = 1\n",
      "P_w: d x N = (40, 2119)\n",
      "P_t: d x N = (40, 2119)\n",
      "S_w: d x d = (40, 40)\n",
      "S_t: d x d = (40, 40)\n",
      "Q: d x r = (40, 40)\n",
      "B: r x L = (40, 11)\n",
      "W: d x L = (40, 11)\n",
      "Results for data_CICIoT2023.csv with MinMaxScaler:\n",
      "Accuracy: 0.9705, Precision: 0.9705, Recall: 1.0000, F1-Score: 0.9850, AUC-ROC: 0.5000\n",
      "Processing dataset data_CICIoT2023.csv with Normalizer scaler ...\n",
      "..............................Data Overview................................\n",
      "Train Data Shape: (69659, 41)\n",
      "Test Data Shape: (29855, 41)\n",
      "Train Data Labels [0]: [0]\n",
      "Number of samples after adding noise: 2119\n",
      "Number of features: 40\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "N = 2119, d = 40, c = 1\n",
      "P_w: d x N = (40, 2119)\n",
      "P_t: d x N = (40, 2119)\n",
      "S_w: d x d = (40, 40)\n",
      "S_t: d x d = (40, 40)\n",
      "Q: d x r = (40, 40)\n",
      "B: r x L = (40, 12)\n",
      "W: d x L = (40, 12)\n",
      "Results for data_CICIoT2023.csv with Normalizer:\n",
      "Accuracy: 0.9705, Precision: 0.9705, Recall: 1.0000, F1-Score: 0.9850, AUC-ROC: 0.5000\n",
      "Processing dataset data_CICIoT2023.csv with StandardScaler scaler ...\n",
      "..............................Data Overview................................\n",
      "Train Data Shape: (69659, 41)\n",
      "Test Data Shape: (29855, 41)\n",
      "Train Data Labels [0]: [0]\n",
      "Number of samples after adding noise: 2119\n",
      "Number of features: 40\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "N = 2119, d = 40, c = 1\n",
      "P_w: d x N = (40, 2119)\n",
      "P_t: d x N = (40, 2119)\n",
      "S_w: d x d = (40, 40)\n",
      "S_t: d x d = (40, 40)\n",
      "Q: d x r = (40, 40)\n",
      "B: r x L = (40, 11)\n",
      "W: d x L = (40, 11)\n",
      "Results for data_CICIoT2023.csv with StandardScaler:\n",
      "Accuracy: 0.9705, Precision: 0.9705, Recall: 1.0000, F1-Score: 0.9850, AUC-ROC: 0.5000\n",
      "Processing dataset data_CICIoT2023.csv with QuantileTransformer scaler ...\n",
      "..............................Data Overview................................\n",
      "Train Data Shape: (69659, 41)\n",
      "Test Data Shape: (29855, 41)\n",
      "Train Data Labels [0]: [0]\n",
      "Number of samples after adding noise: 2119\n",
      "Number of features: 40\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "N = 2119, d = 40, c = 1\n",
      "P_w: d x N = (40, 2119)\n",
      "P_t: d x N = (40, 2119)\n",
      "S_w: d x d = (40, 40)\n",
      "S_t: d x d = (40, 40)\n",
      "Q: d x r = (40, 40)\n",
      "B: r x L = (40, 11)\n",
      "W: d x L = (40, 11)\n",
      "Results for data_CICIoT2023.csv with QuantileTransformer:\n",
      "Accuracy: 0.9705, Precision: 0.9705, Recall: 1.0000, F1-Score: 0.9850, AUC-ROC: 0.5000\n",
      "Results saved to Results/OURMODEL/SCALERS/data_CICIoT2023.csv_100k_oldlear555n1.csv\n",
      "--------------------------------------------------\n",
      "-------- data_N_BaIoT.csv ------------------------------\n",
      "--------------------------------------------------\n",
      "Processing dataset data_N_BaIoT.csv with MinMaxScaler scaler ...\n",
      "..............................Data Overview................................\n",
      "Train Data Shape: (66479, 116)\n",
      "Test Data Shape: (28491, 116)\n",
      "Train Data Labels [0]: [0]\n",
      "Number of samples after adding noise: 6285\n",
      "Number of features: 115\n",
      "Starting K-Means clustering...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/multiarray.py:345\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m    inner(a, b, /)\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    340\u001b[0m \n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, b)\n\u001b[0;32m--> 345\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mwhere)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwhere\u001b[39m(condition, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    where(condition, [x, y], /)\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03m           [ 0,  3, -1]])\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (condition, x, y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'sklearn.cluster._k_means_common._relocate_empty_clusters_dense'\n",
      "Traceback (most recent call last):\n",
      "  File \"<__array_function__ internals>\", line 180, in where\n",
      "  File \"/home/jupyter-iec_23se07/.local/lib/python3.10/site-packages/numpy/core/multiarray.py\", line 345, in where\n",
      "    @array_function_from_c_func_and_dispatcher(_multiarray_umath.where)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of clusters: 1\n",
      "N = 6285, d = 115, c = 1\n",
      "P_w: d x N = (115, 6285)\n",
      "P_t: d x N = (115, 6285)\n",
      "S_w: d x d = (115, 115)\n",
      "S_t: d x d = (115, 115)\n",
      "Q: d x r = (115, 115)\n",
      "B: r x L = (115, 16)\n",
      "W: d x L = (115, 16)\n",
      "Results for data_N_BaIoT.csv with MinMaxScaler:\n",
      "Accuracy: 0.9055, Precision: 0.9055, Recall: 1.0000, F1-Score: 0.9504, AUC-ROC: 0.5000\n",
      "Processing dataset data_N_BaIoT.csv with Normalizer scaler ...\n",
      "..............................Data Overview................................\n",
      "Train Data Shape: (66479, 116)\n",
      "Test Data Shape: (28491, 116)\n",
      "Train Data Labels [0]: [0]\n",
      "Number of samples after adding noise: 6285\n",
      "Number of features: 115\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "N = 6285, d = 115, c = 1\n",
      "P_w: d x N = (115, 6285)\n",
      "P_t: d x N = (115, 6285)\n",
      "S_w: d x d = (115, 115)\n",
      "S_t: d x d = (115, 115)\n",
      "Q: d x r = (115, 115)\n",
      "B: r x L = (115, 16)\n",
      "W: d x L = (115, 16)\n",
      "Results for data_N_BaIoT.csv with Normalizer:\n",
      "Accuracy: 0.9055, Precision: 0.9055, Recall: 1.0000, F1-Score: 0.9504, AUC-ROC: 0.5000\n",
      "Processing dataset data_N_BaIoT.csv with StandardScaler scaler ...\n",
      "..............................Data Overview................................\n",
      "Train Data Shape: (66479, 116)\n",
      "Test Data Shape: (28491, 116)\n",
      "Train Data Labels [0]: [0]\n",
      "Number of samples after adding noise: 6285\n",
      "Number of features: 115\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "N = 6285, d = 115, c = 1\n",
      "P_w: d x N = (115, 6285)\n",
      "P_t: d x N = (115, 6285)\n",
      "S_w: d x d = (115, 115)\n",
      "S_t: d x d = (115, 115)\n",
      "Q: d x r = (115, 115)\n",
      "B: r x L = (115, 16)\n",
      "W: d x L = (115, 16)\n",
      "Results for data_N_BaIoT.csv with StandardScaler:\n",
      "Accuracy: 0.9055, Precision: 0.9055, Recall: 1.0000, F1-Score: 0.9504, AUC-ROC: 0.5000\n",
      "Processing dataset data_N_BaIoT.csv with QuantileTransformer scaler ...\n",
      "..............................Data Overview................................\n",
      "Train Data Shape: (66479, 116)\n",
      "Test Data Shape: (28491, 116)\n",
      "Train Data Labels [0]: [0]\n",
      "Number of samples after adding noise: 6285\n",
      "Number of features: 115\n",
      "Starting K-Means clustering...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 196\u001b[0m\n\u001b[1;32m    193\u001b[0m profiler\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Predict labels and probabilities\u001b[39;00m\n\u001b[1;32m    199\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_label(X_test)\n",
      "Cell \u001b[0;32mIn[2], line 68\u001b[0m, in \u001b[0;36mGMM_NFST.fit\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_train, y_train):\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnpd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_NPD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnullpoint_X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnpd)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Project centroids to null space\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 26\u001b[0m, in \u001b[0;36mGMM_NFST.calculate_NPD\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     24\u001b[0m X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X_train)\n\u001b[1;32m     25\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_train)\n\u001b[0;32m---> 26\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster_kmeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m X \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     28\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_train))\n",
      "Cell \u001b[0;32mIn[2], line 84\u001b[0m, in \u001b[0;36mGMM_NFST.cluster_kmeans\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     82\u001b[0m initial_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_train))\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39minitial_k, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m cluster_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcentroids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkmeans\u001b[38;5;241m.\u001b[39mcluster_centers_\n\u001b[1;32m     86\u001b[0m sorted_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(cluster_labels)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1033\u001b[0m, in \u001b[0;36m_BaseKMeans.fit_predict\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute cluster centers and predict cluster index for each sample.\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \n\u001b[1;32m   1013\u001b[0m \u001b[38;5;124;03m    Convenience method; equivalent to calling fit(X) followed by\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;124;03m        Index of the cluster each sample belongs to.\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1417\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m \n\u001b[1;32m   1392\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m-> 1417\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1419\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1423\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[1;32m   1428\u001b[0m random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.linalg import null_space\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "class GMM_NFST:\n",
    "    def __init__(self, type=\"SemiSup\", npd=None, null_point_X=None, alpha=None):\n",
    "        if type != \"OC\" and alpha is not None:\n",
    "            if not isinstance(alpha, (int, float)) or alpha <= 0:\n",
    "                raise ValueError(\"Alpha must be a positive number when type is not 'OC'\")\n",
    "        \n",
    "        self.type = type\n",
    "        self.npd = npd if npd is not None else np.array([])\n",
    "        self.nullpoint_X = null_point_X if null_point_X is not None else np.array([])\n",
    "        self.centroids = None\n",
    "        self.kmeans = None\n",
    "\n",
    "    def calculate_NPD(self, X_train, y_train):\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "        X_train, y_train = self.cluster_kmeans(X_train, y_train)\n",
    "        X = X_train.T\n",
    "        c = len(np.unique(y_train))\n",
    "        d, N = X.shape\n",
    "\n",
    "        print(f\"N = {N}, d = {d}, c = {c}\")\n",
    "\n",
    "        mean_total = np.mean(X, axis=1, keepdims=True)\n",
    "        P_t = X - mean_total\n",
    "        P_w = np.zeros_like(X)\n",
    "        for i in range(c):\n",
    "            class_indices = y_train == i\n",
    "            class_mean = np.mean(X[:, class_indices], axis=1, keepdims=True)\n",
    "            P_w[:, class_indices] = X[:, class_indices] - class_mean\n",
    "\n",
    "        S_w = np.dot(P_w, P_w.T) / N\n",
    "        S_t = np.dot(P_t, P_t.T) / N\n",
    "\n",
    "        print(f\"P_w: d x N = {P_w.shape}\")\n",
    "        print(f\"P_t: d x N = {P_t.shape}\")\n",
    "        print(f\"S_w: d x d = {S_w.shape}\")\n",
    "        print(f\"S_t: d x d = {S_t.shape}\")\n",
    "\n",
    "        U, S, Vt = np.linalg.svd(P_t, full_matrices=False)\n",
    "        Q = U\n",
    "        B = null_space(Q.T @ S_w @ Q)\n",
    "        W = Q @ B\n",
    "\n",
    "        print(f\"Q: d x r = {Q.shape}\")\n",
    "        print(f\"B: r x L = {B.shape}\")\n",
    "        print(f\"W: d x L = {W.shape}\")\n",
    "\n",
    "        return W\n",
    "\n",
    "    def distance_vector(self, null_point_X, null_point_Y):\n",
    "        norm_X = np.sum(null_point_X**2, axis=1)\n",
    "        norm_Y = np.sum(null_point_Y**2, axis=1)\n",
    "        dot_product = np.dot(null_point_Y, null_point_X.T)\n",
    "        distance = np.sqrt(np.maximum(norm_Y[:, np.newaxis] + norm_X[np.newaxis, :] - 2 * dot_product, 0))\n",
    "        return distance\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.npd = self.calculate_NPD(X_train, y_train)\n",
    "        self.nullpoint_X = np.dot(X_train, self.npd)\n",
    "        # Project centroids to null space\n",
    "        if self.centroids is not None:\n",
    "            self.centroids = np.dot(self.centroids, self.npd)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        null_point_X_t = np.dot(X_test, self.npd)\n",
    "        distest = self.distance_vector(self.nullpoint_X, null_point_X_t)\n",
    "        scores = np.amin(distest, axis=1)\n",
    "        return scores\n",
    "\n",
    "    def cluster_kmeans(self, X_train, y_train):\n",
    "        print(\"Starting K-Means clustering...\")\n",
    "        initial_k = len(np.unique(y_train))\n",
    "        self.kmeans = KMeans(n_clusters=initial_k, random_state=42, n_init=10)\n",
    "        cluster_labels = self.kmeans.fit_predict(X_train)\n",
    "        self.centroids = self.kmeans.cluster_centers_\n",
    "        sorted_indices = np.argsort(cluster_labels)\n",
    "        sorted_data = X_train[sorted_indices]\n",
    "        sorted_labels = cluster_labels[sorted_indices]\n",
    "        print(f\"Final number of clusters: {len(np.unique(sorted_labels))}\")\n",
    "        return sorted_data, sorted_labels\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        if self.centroids is None or self.npd is None:\n",
    "            raise ValueError(\"Model must be fitted before calling predict_proba.\")\n",
    "\n",
    "        # Project test points to null space\n",
    "        X_test_null = np.dot(X_test, self.npd)\n",
    "        # Compute distances from test points to projected centroids\n",
    "        distances = cdist(X_test_null, self.centroids, metric='euclidean')\n",
    "        min_distances = np.min(distances, axis=1)\n",
    "        closest_centroid_idx = np.argmin(distances, axis=1)\n",
    "\n",
    "        # Compute distances between all pairs of centroids in null space\n",
    "        centroid_distances = cdist(self.centroids, self.centroids, metric='euclidean')\n",
    "        np.fill_diagonal(centroid_distances, np.inf)\n",
    "        min_centroid_distances = np.min(centroid_distances, axis=1)\n",
    "\n",
    "        proba = np.zeros(X_test.shape[0])\n",
    "        for i in range(X_test.shape[0]):\n",
    "            d_test = min_distances[i]\n",
    "            closest_centroid = closest_centroid_idx[i]\n",
    "            d_centroids = min_centroid_distances[closest_centroid]\n",
    "            threshold = d_centroids / 2\n",
    "            ratio = d_test / threshold\n",
    "            proba[i] = 1 / (1 + np.exp(ratio - 1))\n",
    "\n",
    "        return proba\n",
    "\n",
    "    def predict_label(self, X_test, threshold=0.5):\n",
    "        proba = self.predict_proba(X_test)\n",
    "        return (proba >= threshold).astype(int)\n",
    "\n",
    "def preprocess_data_noise(train_data, test_data, noise_percentage=0):\n",
    "    print(\"..............................Data Overview................................\")\n",
    "    print(\"Train Data Shape:\", train_data.shape)\n",
    "    print(\"Test Data Shape:\", test_data.shape)\n",
    "    \n",
    "    X_train_total = train_data.iloc[:, :-1].to_numpy()\n",
    "    y_train_total = train_data.iloc[:, -1].to_numpy()\n",
    "\n",
    "    X_train = X_train_total[y_train_total == 0]\n",
    "    y_train = y_train_total[y_train_total == 0]\n",
    "\n",
    "    print(\"Train Data Labels [0]:\", np.unique(y_train))\n",
    "\n",
    "    n_samples = X_train.shape[0]\n",
    "    noise_samples_count = int(n_samples * (noise_percentage / 100))\n",
    "\n",
    "    X_train_noise = X_train_total[y_train_total == 1]\n",
    "    noisy_indices = np.random.choice(X_train_noise.shape[0], size=noise_samples_count, replace=False)\n",
    "    X_train_noise = X_train_noise[noisy_indices]\n",
    "    \n",
    "    X_train = np.vstack((X_train, X_train_noise))\n",
    "    y_train = np.concatenate((y_train, np.ones(X_train_noise.shape[0])))\n",
    "    \n",
    "    X_test = test_data.iloc[:, :-1].to_numpy()\n",
    "    y_test = test_data.iloc[:, -1].to_numpy()\n",
    "\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_features = X_train.shape[1]\n",
    "    print(\"Number of samples after adding noise:\", n_samples)\n",
    "    print(\"Number of features:\", n_features)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Main processing loop\n",
    "dataset_prefixes = ['data_CICIoT2023.csv', 'data_N_BaIoT.csv', 'data_BoTIoT.csv']\n",
    "scaler_names = ['MinMaxScaler', 'Normalizer', 'StandardScaler', 'QuantileTransformer']\n",
    "noise_percentage = 0  # As per the 5% in the filename\n",
    "results = []\n",
    "\n",
    "for prefix in dataset_prefixes:\n",
    "    print(\"-\"*50)\n",
    "    print(\"--------\", prefix, \"-\"*30)\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    base_output_file = f\"Results/OURMODEL/SCALERS/{prefix}_100k_oldlear555n\"\n",
    "    output_file = base_output_file + \"0.csv\"\n",
    "    counter = 0\n",
    "    while os.path.exists(output_file):\n",
    "        counter += 1\n",
    "        output_file = f\"{base_output_file}{counter}.csv\"\n",
    "\n",
    "    for scaler in scaler_names:\n",
    "        print(f\"Processing dataset {prefix} with {scaler} scaler ...\")\n",
    "        \n",
    "        train_file = f'Datascaled/Train_{scaler}_{prefix}'\n",
    "        test_file = f'Datascaled/Test_{scaler}_{prefix}'\n",
    "        \n",
    "        try:\n",
    "            df_train = pd.read_csv(train_file)\n",
    "            df_test = pd.read_csv(test_file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Files {train_file} or {test_file} not found. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Preprocess data with noise\n",
    "        X_train, y_train, X_test, y_test = preprocess_data_noise(df_train, df_test, noise_percentage)\n",
    "\n",
    "        # Initialize and profile the model\n",
    "        model = GMM_NFST(type=\"SemiSup\")\n",
    "        profiler = cProfile.Profile()\n",
    "        profiler.enable()\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict labels and probabilities\n",
    "        y_pred = model.predict_label(X_test)\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "\n",
    "        profiler.disable()\n",
    "        stats = pstats.Stats(profiler).sort_stats('cumulative')\n",
    "        stats.dump_stats(f\"Results/OURMODEL/SCALERS/56profile_{prefix}_{scaler}.prof\")\n",
    "\n",
    "        # Evaluate performance\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "        auc = roc_auc_score(y_test, y_proba) if len(np.unique(y_test)) > 1 else np.nan\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Dataset': prefix,\n",
    "            'Scaler': scaler,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1,\n",
    "            'AUC-ROC': auc\n",
    "        })\n",
    "\n",
    "        print(f\"Results for {prefix} with {scaler}:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}, AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
