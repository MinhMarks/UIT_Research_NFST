{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f35404f-67eb-4c03-9494-3a60b0452c09",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'algorithms'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyod\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msuod\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SUOD\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbaseline_model\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mneutralad_wrapper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NeuTraLAD\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbaseline_model\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdif_wrapper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DIF\n\u001b[32m     37\u001b[39m models_list = [\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# \"CBLOF\",\u001b[39;00m\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# \"KNN\",\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDIF\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     60\u001b[39m ]\n\u001b[32m     62\u001b[39m output_file = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResults/DIF_noise.csv\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NFST/GMM-nfst/baseline_model/dif_wrapper.py:13\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mDeep iForest (DIF) Wrapper - Tích hợp DIF vào baseline với interface giống PyOD\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[33;03m    IEEE Transactions on Knowledge and Data Engineering.\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01malgorithms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DIF \u001b[38;5;28;01mas\u001b[39;00m DIFBase\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mDIF\u001b[39;00m:\n\u001b[32m     17\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03m    Deep Isolation Forest (DIF) wrapper với interface tương thích PyOD\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m \u001b[33;03m            Verbosity level: 0=silent, 1=progress, 2=detailed\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'algorithms'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    matthews_corrcoef, f1_score, precision_score, recall_score,\n",
    "    accuracy_score, roc_auc_score, average_precision_score\n",
    ")\n",
    "\n",
    "from pyod.models.ae1svm import AE1SVM\n",
    "from pyod.models.devnet import DevNet\n",
    "from pyod.models.lunar import LUNAR\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.deep_svdd import DeepSVDD\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.pca import PCA\n",
    "from pyod.models.sod import SOD\n",
    "from pyod.models.cof import COF\n",
    "from pyod.models.loda import LODA\n",
    "from pyod.models.alad import ALAD\n",
    "from pyod.models.ecod import ECOD\n",
    "from pyod.models.copod import COPOD\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.models.vae import VAE\n",
    "from pyod.models.so_gaal import SO_GAAL  # đúng: \"sogaal\" (không phải \"so_gaal\")\n",
    "from pyod.models.mo_gaal import MO_GAAL  # đúng: \"mogaal\" (không phải \"mo_gaal\")\n",
    "from baseline_model.dasvdd_wrapper import DASVDD\n",
    "from pyod.models.suod import SUOD\n",
    "from baseline_model.neutralad_wrapper import NeuTraLAD\n",
    "from baseline_model.dif_wrapper import DIF\n",
    "\n",
    "models_list = [\n",
    "    # \"CBLOF\",\n",
    "    # \"KNN\",\n",
    "    # \"IForest\",\n",
    "    # \"OCSVM\",\n",
    "    # \"LOF\",\n",
    "    # \"DeepSVDD\",\n",
    "    # \"HBOS\",\n",
    "    # \"PCA\",\n",
    "    # \"LODA\",\n",
    "    # \"ECOD\",\n",
    "    # \"COPOD\",\n",
    "    # \"AutoEncoder\",\n",
    "    # \"DevNet\",\n",
    "    # \"LUNAR\",\n",
    "    # \"AE1SVM\",\n",
    "    # \"ALAD\",\n",
    "    # \"VAE\"\n",
    "    # \"SO_GAAL\",\n",
    "    # \"MO_GAAL\"\n",
    "    # \"DASVDD\", \n",
    "    # \"SUOD\", \n",
    "    \"DIF\", \n",
    "]\n",
    "\n",
    "output_file = f\"Results/DIF_noise.csv\"\n",
    "columns = [\"Dataset\",\"Model\",  \"scaled\", \"noise_percentage\", \"AUCROC\", \"AUCPR\", \"Accuracy\", \"MCC\", \"F1 Score\",\n",
    "           \"Precision\", \"Recall\", \"Time Train\", \"Time Test\"]\n",
    "\n",
    "def preprocess_data_noise(train_data, test_data, noise_percentage=10):\n",
    "    \"\"\"\n",
    "    Preprocess the training and testing data by separating features and labels,\n",
    "    and return the preprocessed feature sets and labels with added noise.\n",
    "    \n",
    "    Args:\n",
    "        train_data (DataFrame): Training data containing features and labels.\n",
    "        test_data (DataFrame): Testing data containing features and labels.\n",
    "        noise_percentage (float): The percentage of training samples to add noise to.\n",
    "        \n",
    "    Returns:\n",
    "        X_train (ndarray): Preprocessed training features with added noise.\n",
    "        y_train (ndarray): Preprocessed training labels with added noise.\n",
    "        X_test (ndarray): Preprocessed testing features.\n",
    "        y_test (ndarray): Preprocessed testing labels.\n",
    "    \"\"\"\n",
    "    print(\"..............................Data Overview................................\")\n",
    "    print(\"Train Data Shape:\", train_data.shape)\n",
    "    print(\"Test Data Shape:\", test_data.shape)\n",
    "    \n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    X_train_total = train_data.iloc[:, :-1].to_numpy()\n",
    "    y_train_total = train_data.iloc[:, -1].to_numpy()\n",
    "\n",
    "    # Separate the samples with label 0\n",
    "    X_train = X_train_total[y_train_total == 0]\n",
    "    y_train = y_train_total[y_train_total == 0]\n",
    "\n",
    "    print(\"Train Data Labels [0]:\", np.unique(y_train))\n",
    "\n",
    "    # Calculate how many samples to add noise to based on the provided percentage\n",
    "    n_samples = X_train.shape[0]\n",
    "    noise_samples_count = int(n_samples * (noise_percentage / 100))\n",
    "\n",
    "    # Get the samples with label 1 (for generating noise)\n",
    "    X_train_noise = X_train_total[y_train_total == 1]\n",
    "    \n",
    "    # Randomly select noise_samples_count from X_train_noise\n",
    "    noisy_indices = np.random.choice(X_train_noise.shape[0], size=noise_samples_count, replace=False)\n",
    "    X_train_noise = X_train_noise[noisy_indices]\n",
    "    \n",
    "    # Add the noisy samples to the training set\n",
    "    X_train = np.vstack((X_train, X_train_noise))\n",
    "    # y_train = np.concatenate((y_train, np.ones(X_train_noise.shape[0])))\n",
    "    y_train = np.concatenate((y_train, np.zeros(X_train_noise.shape[0])))\n",
    "\n",
    "    print(y_train) \n",
    "    # Prepare test data\n",
    "    X_test = test_data.iloc[:, :-1].to_numpy()\n",
    "    y_test = test_data.iloc[:, -1].to_numpy()\n",
    "\n",
    "    # Print the new size of training data\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_features = X_train.shape[1]\n",
    "    print(\"Number of samples after adding noise:\", n_samples)\n",
    "    print(\"Number of features:\", n_features)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred, y_scores=None, y_probabilities=None):\n",
    "    \"\"\"\n",
    "    Evaluates the model using multiple metrics and prints the results.\n",
    "    This version includes AUCROC and AUCPR using predicted probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true (np.ndarray): True labels of the test data.\n",
    "    - y_pred (np.ndarray): Predicted labels of the test data.\n",
    "    - y_scores (np.ndarray, optional): Scores used to compute AUCROC and AUCPR.\n",
    "    - y_probabilities (np.ndarray, optional): Predicted probabilities for each class.\n",
    "\n",
    "    Returns:\n",
    "    - metrics (list): A list containing AUCROC, AUCPR, accuracy, MCC, F1 score, precision, and recall.\n",
    "    \"\"\"\n",
    "    print(\"..............................Evaluation Metrics...............................\")\n",
    "\n",
    "    # Calculate standard metrics\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)  # Matthew's correlation coefficient\n",
    "    f1 = f1_score(y_true, y_pred)  # F1 score\n",
    "    precision = precision_score(y_true, y_pred)  # Precision\n",
    "    recall = recall_score(y_true, y_pred)  # Recall\n",
    "    accuracy = accuracy_score(y_true, y_pred)  # Accuracy\n",
    "\n",
    "    # ROC and PR curve scores using predicted probabilities\n",
    "    auc_roc, auc_pr = None, None\n",
    "    if y_probabilities is not None:\n",
    "        auc_roc = roc_auc_score(y_true, y_probabilities[:, 1])  # Probabilities for class 1 (outliers)\n",
    "        auc_pr = average_precision_score(y_true, y_probabilities[:, 1])  # AUCPR for class 1\n",
    "\n",
    "    # Display metrics\n",
    "    print(f\"AUCROC: {auc_roc * 100 if auc_roc else 'N/A'}\")\n",
    "    print(f\"AUCPR: {auc_pr * 100 if auc_pr else 'N/A'}\")\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}\")\n",
    "    print(f\"MCC: {mcc:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "    # Return the evaluation metrics as a list\n",
    "    return [auc_roc * 100 if auc_roc else None, auc_pr * 100 if auc_pr else None,\n",
    "            accuracy * 100, mcc, f1, precision, recall]\n",
    "\n",
    "\n",
    "def get_model(name, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns the appropriate PyOD model based on the provided name.\n",
    "    \n",
    "    Parameters:\n",
    "    - name (str): The name of the anomaly detection model (e.g., 'CBLOF', 'KNN').\n",
    "    - kwargs (dict): Additional parameters to initialize the model.\n",
    "\n",
    "    Returns:\n",
    "    - model (object): An instance of the specified anomaly detection model.\n",
    "    \"\"\"\n",
    "    model_dict = {\n",
    "        \"CBLOF\": CBLOF,\n",
    "        \"KNN\": KNN,\n",
    "        \"IForest\": IForest,\n",
    "        \"OCSVM\": OCSVM,\n",
    "        \"LOF\": LOF,\n",
    "        \"DeepSVDD\": DeepSVDD,\n",
    "        \"HBOS\": HBOS,\n",
    "        \"SOD\": SOD,\n",
    "        \"COF\": COF,\n",
    "        \"LODA\": LODA,\n",
    "        \"PCA\": PCA,\n",
    "        \"ECOD\": ECOD,\n",
    "        \"COPOD\": COPOD,\n",
    "        \"AutoEncoder\": AutoEncoder,\n",
    "        \"DevNet\": DevNet,\n",
    "        \"LUNAR\": LUNAR,\n",
    "        \"AE1SVM\": AE1SVM,\n",
    "        \"ALAD\": ALAD,\n",
    "        \"VAE\": VAE,\n",
    "        \"SO_GAAL\": SO_GAAL,\n",
    "        \"MO_GAAL\": MO_GAAL, \n",
    "        \"DASVDD\": DASVDD, \n",
    "        \"SUOD\": SUOD, \n",
    "        \"NeuTraLAD\": NeuTraLAD, \n",
    "        \"DIF\": DIF, \n",
    "    }\n",
    "    \n",
    "    # Fetch the model class based on the provided model name\n",
    "    model_class = model_dict.get(name)\n",
    "    \n",
    "    # Raise an error if the model name is not found\n",
    "    if model_class is None:\n",
    "        raise ValueError(f\"Model {name} not found.\")\n",
    "    \n",
    "    # Return the model initialized with the provided parameters\n",
    "    return model_class(**kwargs)\n",
    "\n",
    "\n",
    "# Initialize output CSV file\n",
    "if not os.path.exists(output_file):\n",
    "    pd.DataFrame(columns=columns).to_csv(output_file, index=False)\n",
    "\n",
    "# Define a function to run the model training and evaluation\n",
    "def run_experiment(X_train, y_train, X_test, y_test, name, noise_percentage, scaler ):\n",
    "    for model_name in models_list:\n",
    "        try:\n",
    "            print(f\"\\nRunning dataset {name} with model {model_name}\")\n",
    "            \n",
    "            # train_data = pd.concat([X_train, y_train], axis=1, ignore_index=True)\n",
    "            # test_data = pd.concat([X_test, y_test], axis=1, ignore_index=True)\n",
    "\n",
    "            # # Preprocess data (assuming preprocess_data is defined elsewhere)\n",
    "            # X_train, y_train, X_test, y_test = preprocess_data(train_data, test_data)\n",
    "            \n",
    "            if model_name == 'DeepSVDD':\n",
    "                start_time = time.time()\n",
    "                n_features = X_train.shape[1]\n",
    "                model = DeepSVDD(n_features=n_features)\n",
    "                model.fit(X_train)\n",
    "            elif model_name == 'DASVDD':\n",
    "                start_time = time.time()\n",
    "                model = DASVDD(\n",
    "                    code_size=32,\n",
    "                    num_epochs=100,\n",
    "                    batch_size=128,\n",
    "                    lr=1e-3,\n",
    "                    K=0.9,\n",
    "                    T=10,\n",
    "                    verbose=1\n",
    "                )\n",
    "                model.fit(X_train)\n",
    "                train_time = time.time() - start_time\n",
    "            elif model_name == \"NeuTraLAD\": \n",
    "                start_time = time.time()\n",
    "                model = NeuTraLAD(\n",
    "                    latent_dim=32,      # Kích thước latent space\n",
    "                    enc_hdim=32,        # Hidden dim encoder\n",
    "                    num_trans=11,       # Số transformations\n",
    "                    num_epochs=200,     # Số epochs\n",
    "                    batch_size=128,\n",
    "                    lr=0.001,\n",
    "                    device='cuda',      # hoặc 'cpu'\n",
    "                    verbose=1           # 1 để hiển thị progress\n",
    "                )\n",
    "                # Fit model (chỉ dùng normal data)\n",
    "                model.fit(X_train)\n",
    "            elif model_name == \"DIF\": \n",
    "                start_time = time.time()\n",
    "                model = DIF(n_ensemble=50, n_estimators=6)\n",
    "\n",
    "                # Hoặc với custom config\n",
    "                model = DIF(\n",
    "                    n_ensemble=50,\n",
    "                    n_estimators=6,\n",
    "                    max_samples=256,\n",
    "                    hidden_dim=[500, 100],\n",
    "                    rep_dim=20,\n",
    "                    activation='tanh',\n",
    "                    batch_size=64,\n",
    "                    device='cuda',\n",
    "                    verbose=1\n",
    "                )\n",
    "                model.fit(X_train)\n",
    "                \n",
    "            elif model_name == \"SUOD\": \n",
    "                \n",
    "                \n",
    "                base_estimators = [\n",
    "                    HBOS(n_bins=10),\n",
    "                    COPOD(),\n",
    "                    ECOD(),\n",
    "                    IForest(n_estimators=50, max_samples=128, random_state=42),\n",
    "                    PCA(n_components=10, random_state=42),\n",
    "                ]\n",
    "                \n",
    "                model = SUOD(\n",
    "                    base_estimators=base_estimators,\n",
    "                    n_jobs=1,                    # BẮT BUỘC = 1 để tránh hoàn toàn bug joblib\n",
    "                    rp_flag_global=True,         # vẫn tăng tốc\n",
    "                    bps_flag=False,              # tắt BPS → nguyên nhân chính của lỗi\n",
    "                    contamination=0.1,\n",
    "                    verbose=True                 # in chi tiết để bạn thấy nó chạy\n",
    "                )\n",
    "                \n",
    "                model.fit(X_train)  # fit all models with X\n",
    "            else:\n",
    "                model = get_model(model_name)\n",
    "                start_time = time.time()\n",
    "                if model_name == 'DevNet':\n",
    "                    model.fit(X_train, y_train)\n",
    "                else:\n",
    "                    model.fit(X_train)\n",
    "            train_time = time.time() - start_time\n",
    "\n",
    "            # Test the model\n",
    "            start_time = time.time()\n",
    "            y_pred = model.predict(X_test)\n",
    "            # Check if the model supports predict_proba and calculate probabilities\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                y_probabilities = model.predict_proba(X_test)  # Class probabilities for each instance\n",
    "            else:\n",
    "                y_probabilities = None  # Some models do not have predict_proba method\n",
    "\n",
    "            test_time = time.time() - start_time\n",
    "\n",
    "            # Evaluate the model using the appropriate probabilities\n",
    "            metrics = evaluate_model(y_test, y_pred, y_scores=None, y_probabilities=y_probabilities)\n",
    "            result = [name, model_name] + [scaler, noise] + metrics + [train_time, test_time]\n",
    "            result_df = pd.DataFrame([result], columns=columns)\n",
    "            result_df.to_csv(output_file, mode='a', header=False, index=False)\n",
    "\n",
    "            print(f\"Results saved for {name} with model {model_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with dataset {name}, model {model_name}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    \n",
    "    # dataset_prefixes =  ['N_BaIoT_dataloader.csv']\n",
    "    dataset_prefixes =  ['data_N_BaIoT.csv' , 'data_CICIoT2023.csv', 'data_BoTIoT.csv', 'data_ToNIoT.csv']\n",
    "    \n",
    "    # scaler_names = ['MinMaxScaler']\n",
    "    scaler_names = ['StandardScaler','QuantileTransformer','MinMaxScaler','Normalizer','RobustScaler']\n",
    "    \n",
    "    # scaler_names = ['QuantileTransformer']\n",
    "    \n",
    "    for prefix in dataset_prefixes:\n",
    "        \n",
    "        for scaler in scaler_names: \n",
    "         \n",
    "            # Construct file paths for train and test datasets with 'Train_' and 'Test_' prefixes\n",
    "            train_file = f'Datascaled/NoiseOCData/Train_{scaler}_{prefix}'\n",
    "            test_file = f'Datascaled/NoiseOCData/Test_{scaler}_{prefix}'\n",
    "            \n",
    "            # Load the CSV files\n",
    "            df_train = pd.read_csv(train_file)\n",
    "            df_test = pd.read_csv(test_file)\n",
    "\n",
    "            \n",
    "            df_train = df_train.dropna() \n",
    "            df_test = df_test.dropna() \n",
    "            \n",
    "            # Nối lại thành 1 DataFrame\n",
    "            df_full = pd.concat([df_train, df_test], ignore_index=True)\n",
    "            \n",
    "            # Chia theo tỉ lệ 70% train, 30% test\n",
    "            df_train_new, df_test_new = train_test_split(df_full, test_size=0.3, random_state=42)\n",
    "\n",
    "            # print(f\"Số lượng hàng có NaN: {num_rows_with_nan}\")\n",
    "\n",
    "            for noise in [ 0, 1, 3 ,5 ]: \n",
    "            # for noise in range (0 , 6 ): \n",
    "                # Step 1: Preprocess data\n",
    "                X_train, y_train, X_test, y_test = preprocess_data_noise(df_train_new, df_test_new, noise)\n",
    "                # Run the experiments for each dataset type\n",
    "                run_experiment(X_train, y_train, X_test, y_test, prefix, noise, scaler ) \n",
    "               \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ecbfab-a3e0-4874-be05-57b625b0462a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
