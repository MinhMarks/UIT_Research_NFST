{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (roc_auc_score, precision_score, average_precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error, roc_curve, auc, classification_report,auc,confusion_matrix,matthews_corrcoef)\n",
    "from sklearn import logger\n",
    "from sklearn.datasets import make_blobs,make_multilabel_classification\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import KernelCenterer,LabelEncoder, MinMaxScaler, Normalizer, QuantileTransformer, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import scipy as sp\n",
    "from scipy.linalg import svd,null_space\n",
    "import os\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.cluster import KMeans,AgglomerativeClustering,SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.sparse import csr_matrix as sp\n",
    "import math\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.spatial.distance import cdist\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Import 3D plotting tools\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.datasets import make_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hàm\tĐộ phức tạp tính toán\tBộ nhớ cần\n",
    "# preprocess_data_OC\tO(n⋅d²)\tO(n⋅d²)\n",
    "# clusterr\tO(n⋅d⋅k⋅iters)\tO(n⋅d)\n",
    "# nullspace\tO(d³)\tO(d²)\n",
    "# gram_schmidt\tO(d³)\tO(d²)\n",
    "# minimum_distance\tO(n⋅m⋅d)\tO(n⋅m)\n",
    "# distance_vector\tO(n⋅m⋅d)\tO(n⋅m)\n",
    "# calculate_NPD\tO(d³)\tO(d²)\n",
    "# learn\tO(n²⋅d)\tO(n²)\n",
    "\n",
    "\n",
    "\n",
    "alpha = 0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data_noise(train_data, noise_percentage=10):\n",
    "  \n",
    "    print(\"..............................Data Overview................................\")\n",
    "    # print(\"Train Data Shape:\", train_data.shape) \n",
    "    \n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    X_train_total = train_data[:, :-1]\n",
    "    y_train_total = train_data[:, -1]\n",
    "\n",
    "    # Separate the samples with label 0\n",
    "    X_train = X_train_total[y_train_total == 0]\n",
    "    y_train = y_train_total[y_train_total == 0]\n",
    "\n",
    "    # print(\"Train Data Labels [0]:\", np.unique(y_train))\n",
    "\n",
    "    # Calculate how many samples to add noise to based on the provided percentage\n",
    "    n_samples = X_train.shape[0]\n",
    "    noise_samples_count = int(n_samples * (noise_percentage / 100))\n",
    "\n",
    "    # Get the samples with label 1 (for generating noise)\n",
    "    X_train_noise = X_train_total[y_train_total == 1]\n",
    "    \n",
    "    # Randomly select noise_samples_count from X_train_noise\n",
    "    noisy_indices = np.random.choice(X_train_noise.shape[0], size=noise_samples_count, replace=False)\n",
    "    X_train_noise = X_train_noise[noisy_indices]\n",
    "    \n",
    "    # Add the noisy samples to the training set\n",
    "    X_train = np.vstack((X_train, X_train_noise))\n",
    "    y_train = np.concatenate((y_train, np.ones(X_train_noise.shape[0])))\n",
    "    # print(y_train) \n",
    "    \n",
    "    # Print the new size of training data\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_features = X_train.shape[1]\n",
    "    # print(\"Number of samples after adding noise:\", n_samples)\n",
    "    # print(\"Number of features:\", n_features)\n",
    "\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def cluster_kmeans(data, initial_k):\n",
    "    print(\"Starting K-Means clustering...\")\n",
    "\n",
    "    # Thực hiện K-Means clustering với số cụm initial_k\n",
    "    kmeans = KMeans(n_clusters=initial_k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(data)\n",
    "\n",
    "    # Gán nhãn cụm vào biến labels\n",
    "    labels = cluster_labels\n",
    "\n",
    "    # Sắp xếp dữ liệu theo nhãn cụm\n",
    "    sorted_indices = np.argsort(labels)\n",
    "    sorted_data = data[sorted_indices]\n",
    "    sorted_labels = labels[sorted_indices]\n",
    "\n",
    "    print(\"Final number of clusters:\", len(np.unique(sorted_labels)))\n",
    "    return sorted_data, sorted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nullspace(A):\n",
    "    _, s, vh = np.linalg.svd(A)\n",
    "    null_mask = np.isclose(s, 0)\n",
    "    null_space = vh[null_mask].T\n",
    "    return null_space\n",
    "\n",
    "\n",
    "def minimum_distance( A, B):\n",
    "        \"\"\"\n",
    "        Compute the minimum Euclidean distance from each point in A to all points in B.\n",
    "        \n",
    "        Parameters:\n",
    "        - A (ndarray): Set of points (N_A, d).\n",
    "        - B (ndarray): Set of points (N_B, d).\n",
    "        \n",
    "        Returns:\n",
    "        - min_distances (ndarray): Minimum distances from each point in A to the nearest point in B.\n",
    "        \"\"\"\n",
    "        A = np.asarray(A)\n",
    "        B = np.asarray(B)\n",
    "        \n",
    "        # Initialize an array for storing minimum distances\n",
    "        min_distances = np.empty(A.shape[0], dtype=np.float64)\n",
    "        \n",
    "        # Iterate over each point in A and calculate minimum distance to points in B\n",
    "        for i, a in enumerate(A):\n",
    "            distances = cdist([a], B, metric='euclidean')  # Compute all pairwise distances\n",
    "            min_distances[i] = np.min(distances)  # Store minimum distance\n",
    "        \n",
    "        return min_distances   \n",
    "\n",
    "def distance_vector(point_X, point_Y):\n",
    "    \"\"\"\n",
    "    Calculate pairwise Euclidean distance between two sets of points.\n",
    "    \n",
    "    Args:\n",
    "        point_X (ndarray): Array of shape (N_train, d) where N_train is the number of training samples and d is the number of features.\n",
    "        point_Y (ndarray): Array of shape (N_test, d) where N_test is the number of test samples and d is the number of features.\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: Distance matrix of shape (N_test, N_train) containing Euclidean distances between each pair of points.\n",
    "    \"\"\"\n",
    "    print( 'Complexity of calculate: ', point_X )\n",
    "    # Compute squared norms for each point\n",
    "    norm_X = np.sum(point_X**2, axis=1)  # (N_train,)\n",
    "    norm_Y = np.sum(point_Y**2, axis=1)  # (N_test,)\n",
    "    \n",
    "    # Compute the dot product between the two sets of points\n",
    "    dot_product = np.dot(point_Y, point_X.T)  # (N_test, N_train)\n",
    "    \n",
    "    # Apply Euclidean distance formula\n",
    "    distance = np.sqrt(abs(norm_Y[:, np.newaxis] + norm_X[np.newaxis, :] - 2 * dot_product))\n",
    "    return distance\n",
    "\n",
    "\n",
    "\n",
    "def calculate_NPD(X,y):\n",
    "    print(\"Begin calculating NPD --------------\")\n",
    "    X = X.T\n",
    "    print('shape of X', X.shape)\n",
    "    c = len(np.unique(y))  # Số lớp\n",
    "    d, N = X.shape  # Số đặc trưng và số mẫu\n",
    "    # Tính trung bình toàn cục và tạo ma trận P_t với zero-mean\n",
    "    mean_total = np.mean(X, axis=1, keepdims=True)\n",
    "    P_t = X - mean_total  # P_t là ma trận zero-mean có kích thước d x N\n",
    "    \n",
    "    # Tính P_w cho từng lớp\n",
    "    P_w = np.zeros_like(X)\n",
    "    for i in range(c):\n",
    "        class_mean = np.mean(X[:, y == i], axis=1, keepdims=True)\n",
    "        P_w[:, y == i] = X[:, y == i] - class_mean  # Tạo ma trận P_w có kích thước d x N\n",
    "        \n",
    "    # Tính ma trận phương sai S_w và S_t\n",
    "    S_w = np.dot(P_w, P_w.T) / N  # S_w là d x d\n",
    "    S_t = np.dot(P_t, P_t.T) / N  # S_t là d x d\n",
    "\n",
    "    print( \"rank sw st :\", np.linalg.matrix_rank(S_t) , np.linalg.matrix_rank(S_w)) \n",
    "    print( \"rank sw st :\", np.linalg.matrix_rank(P_t) , np.linalg.matrix_rank(P_w)) \n",
    "\n",
    "    \n",
    "    U, S, Vt = np.linalg.svd(P_t, full_matrices=False)\n",
    "    Q = U\n",
    "    B = nullspace(Q.T @ S_w @ Q)\n",
    "    W = Q @ B  # W có kích thước d x (c - 1)\n",
    "\n",
    "    # In kết quả\n",
    "    print(\"N =\", N,  \"  d =\", d, \"   c =\", c )\n",
    "    print(\"P_w : d x N =\", P_w.shape)  # d x N\n",
    "    print(\"P_t : d x N =\", P_t.shape)  # d x N\n",
    "    print(\"S_w : d x d =\", S_w.shape)  # d x d\n",
    "    print(\"S_t : d x d =\", S_t.shape)  # d x d\n",
    "    print(\"Q   : d x N =\", Q.shape)     # d x r\n",
    "    print(\"B   : N x L =\", B.shape)     # r x L\n",
    "    print(\"W   : d x L =\", W.shape)     # d x L\n",
    "    return W    \n",
    "\n",
    "    \n",
    "\n",
    "def BruteForce_Threshold(y_true, y_prob, minth=0.0, maxth=1.0, num_thresholds=1000):\n",
    "    \"\"\"\n",
    "    Finds the best classification threshold for a binary model using brute force search.\n",
    "\n",
    "    Args:\n",
    "        y_true (ndarray): True labels (0 or 1), shape (n_samples,).\n",
    "        y_prob (ndarray): Predicted probabilities for class 1, shape (n_samples,).\n",
    "        minth (float, optional): Minimum threshold value. Default is 0.0.\n",
    "        maxth (float, optional): Maximum threshold value. Default is 1.0.\n",
    "        num_thresholds (int, optional): Number of threshold values to search. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the best threshold for each evaluation metric.\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(minth, maxth, num_thresholds)  # Generate candidate thresholds\n",
    "\n",
    "    # Initialize best metrics\n",
    "    best_results = {\n",
    "        \"accuracy\": (0, 0),  # (best_threshold, best_score)\n",
    "        \"f1\": (0, 0),\n",
    "        \"mcc\": (0, 0),\n",
    "        \"auc_roc\": (0, 0),\n",
    "        \"auc_pr\": (0, 0)\n",
    "    }\n",
    "\n",
    "    return best_results \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob[:,1] >= threshold).astype(int)\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "        auc_roc = roc_auc_score(y_true, y_prob[:, 1])\n",
    "        auc_pr = average_precision_score(y_true, y_prob[:, 1])\n",
    "\n",
    "        # Update best threshold for each metric\n",
    "        if acc > best_results[\"accuracy\"][1]:\n",
    "            best_results[\"accuracy\"] = (threshold, acc)\n",
    "        if f1 > best_results[\"f1\"][1]:\n",
    "            best_results[\"f1\"] = (threshold, f1)\n",
    "        if mcc > best_results[\"mcc\"][1]:\n",
    "            best_results[\"mcc\"] = (threshold, mcc)\n",
    "        if auc_roc > best_results[\"auc_roc\"][1]:\n",
    "            best_results[\"auc_roc\"] = (threshold, auc_roc)\n",
    "        if auc_pr > best_results[\"auc_pr\"][1]:\n",
    "            best_results[\"auc_pr\"] = (threshold, auc_pr)\n",
    "    return best_results\n",
    "\n",
    "\n",
    "def learn( npd, X_train, y_train , X_test, t0):\n",
    "    '''\n",
    "    X_train n1, d \n",
    "\n",
    "    C: n * n * d \n",
    "    '''\n",
    "    t1 = time.time()\n",
    "    null_point_X = (sp(X_train).dot(sp(npd))).toarray()\n",
    "    null_point_X_test = (sp(X_test).dot(sp(npd))).toarray()  \n",
    "\n",
    "    train_score_tmp = distance_vector(null_point_X, null_point_X)\n",
    "    for i in range(len(train_score_tmp)):\n",
    "        train_score_tmp[i , i] = 1e9                                                      \n",
    "    train_score = np.amin(train_score_tmp, axis=1)\n",
    "    \n",
    "    y_score = minimum_distance(null_point_X_test, null_point_X)\n",
    "    y_proba = np.zeros((len(y_score), 2))\n",
    "    y_proba[:, 1] = np.minimum(y_score / np.max(train_score), 1)                         \n",
    "    y_proba[:, 0] = 1 - y_proba[:, 1]                                                     # Probability for class 0\n",
    "    y_proba = np.nan_to_num(y_proba, nan=1.0)\n",
    "    y_predict = (y_proba[:, 1] > 0.001).astype(int) \n",
    "    t2 = time.time()\n",
    "    print(\"...............................Timing Model................................\")\n",
    "    print(\"Time train:\", t1-t0)\n",
    "    print(\"Time test:\" , t2-t1)\n",
    "    return y_proba, t1, t2  \n",
    "    \n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "def Model_evaluating(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    Evaluate the model using threshold derived from ROC curve (Youden’s J statistic).\n",
    "    \n",
    "    Args:\n",
    "        y_true (ndarray): True labels (binary: 0 or 1).\n",
    "        y_scores (ndarray): Predicted probabilities for each class (2D array).\n",
    "        \n",
    "    Returns:\n",
    "        list: List of evaluation metrics [AUC, AUCPR, Accuracy, MCC, F1, Precision, Recall].\n",
    "    \"\"\"\n",
    "    print(\"..............................Report Parameter...............................\")\n",
    "    \n",
    "    # Lấy xác suất cho lớp dương (lớp 1)\n",
    "    y_prob = y_scores[:, 1]\n",
    "    \n",
    "    # Tính ROC và threshold tối ưu theo Youden’s J statistic\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    j_scores = tpr - fpr\n",
    "    optimal_idx = np.argmax(j_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    print(\"Optimal threshold (Youden's J):\", optimal_threshold)\n",
    "\n",
    "    # Dự đoán nhãn với threshold tối ưu\n",
    "    y_predict_optimal = (y_prob >= optimal_threshold).astype(int)\n",
    "\n",
    "    # Tính các chỉ số đánh giá\n",
    "    mcc = matthews_corrcoef(y_true, y_predict_optimal)\n",
    "    f1 = f1_score(y_true, y_predict_optimal)\n",
    "    ppv = precision_score(y_true, y_predict_optimal)\n",
    "    recall = recall_score(y_true, y_predict_optimal)\n",
    "    accuracy = accuracy_score(y_true, y_predict_optimal)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    aucpr = average_precision_score(y_true, y_prob)\n",
    "    \n",
    "    # In ra các kết quả\n",
    "    print(\"AUCROC:\", auc * 100)\n",
    "    print(\"AUCPR:\", aucpr * 100)\n",
    "    print(\"Accuracy:\", accuracy * 100)\n",
    "    print(\"MCC:\", mcc)\n",
    "    print(\"F1 score:\", f1)\n",
    "    print(\"PPV (Precision):\", ppv)\n",
    "    print(\"TPR (Recall):\", recall)\n",
    "\n",
    "    return y_predict_optimal  # [auc * 100, aucpr * 100, accuracy * 100, mcc, f1, ppv, recall]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train, X_test, y_test = preprocess_data_OC(df2, df1)\n",
    "\n",
    "# X_total = np.vstack((X_train, X_test))\n",
    "# unique_rows = np.unique(X_total, axis=0)\n",
    "\n",
    "# print(\"Số dòng trong X_total:\", X_total.shape[0])   # Output: 5\n",
    "# print(\"Số dòng khác nhau:\", unique_rows.shape[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "columns = [\"scaler\",\"nCluster\", 'noise_percentage', \"AUCROC\", \"AUCPR\", \"Accuracy\", \"MCC\", \"F1 Score\",\n",
    "           \"Precision\", \"Recall\", \"Time Train\", \"Time Test\"]\n",
    "\n",
    "import cProfile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def function(df1, df2, scaler, noise, ncluster):\n",
    "    X_train0, y_train0 = preprocess_data_noise(df1, noise)\n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"mean\") \n",
    "    X_train0[np.isinf(X_train0)] = np.nan  \n",
    "    X_train0 = imputer.fit_transform(X_train0)\n",
    "\n",
    "    X_train, y_train = cluster_kmeans(X_train0, ncluster)\n",
    "\n",
    "\n",
    "    t5 = time.time()     \n",
    "    npd = calculate_NPD(X_train, y_train)\n",
    "    t4 = time.time() \n",
    "\n",
    "    print(\" time to calculate NPD: \" , t4 - t5)  \n",
    "\n",
    "    return 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating synthetic data for 1000 samples and 10 features...\n",
      "<class 'numpy.ndarray'>\n",
      "..............................Data Overview................................\n",
      "Train Data Shape: (700, 11)\n",
      "Train Data Labels [0]: [0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Number of samples after adding noise: 346\n",
      "Number of features: 10\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "Begin calculating NPD --------------\n",
      "shape of X (10, 346)\n",
      "rank sw st : 9 9\n",
      "rank sw st : 9 9\n",
      "N = 346   d = 10    c = 1\n",
      "P_w : d x N = (10, 346)\n",
      "P_t : d x N = (10, 346)\n",
      "S_w : d x d = (10, 10)\n",
      "S_t : d x d = (10, 10)\n",
      "Q   : d x N = (10, 10)\n",
      "B   : N x L = (10, 1)\n",
      "W   : d x L = (10, 1)\n",
      " time to calculate NPD:  0.0010912418365478516\n",
      "Done!\n",
      "\n",
      "Generating synthetic data for 1000 samples and 100 features...\n",
      "<class 'numpy.ndarray'>\n",
      "..............................Data Overview................................\n",
      "Train Data Shape: (700, 101)\n",
      "Train Data Labels [0]: [0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "Number of samples after adding noise: 343\n",
      "Number of features: 100\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "Begin calculating NPD --------------\n",
      "shape of X (100, 343)\n",
      "rank sw st : 90 90\n",
      "rank sw st : 90 90\n",
      "N = 343   d = 100    c = 1\n",
      "P_w : d x N = (100, 343)\n",
      "P_t : d x N = (100, 343)\n",
      "S_w : d x d = (100, 100)\n",
      "S_t : d x d = (100, 100)\n",
      "Q   : d x N = (100, 100)\n",
      "B   : N x L = (100, 10)\n",
      "W   : d x L = (100, 10)\n",
      " time to calculate NPD:  0.07865619659423828\n",
      "Done!\n",
      "\n",
      "Generating synthetic data for 1000 samples and 1000 features...\n",
      "<class 'numpy.ndarray'>\n",
      "..............................Data Overview................................\n",
      "Train Data Shape: (700, 1001)\n",
      "Train Data Labels [0]: [0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Number of samples after adding noise: 346\n",
      "Number of features: 1000\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "Begin calculating NPD --------------\n",
      "shape of X (1000, 346)\n",
      "rank sw st : 345 345\n",
      "rank sw st : 345 345\n",
      "N = 346   d = 1000    c = 1\n",
      "P_w : d x N = (1000, 346)\n",
      "P_t : d x N = (1000, 346)\n",
      "S_w : d x d = (1000, 1000)\n",
      "S_t : d x d = (1000, 1000)\n",
      "Q   : d x N = (1000, 346)\n",
      "B   : N x L = (346, 1)\n",
      "W   : d x L = (1000, 1)\n",
      " time to calculate NPD:  0.8449790477752686\n",
      "Done!\n",
      "\n",
      "Generating synthetic data for 1000 samples and 10000 features...\n",
      "<class 'numpy.ndarray'>\n",
      "..............................Data Overview................................\n",
      "Train Data Shape: (700, 10001)\n",
      "Train Data Labels [0]: [0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Number of samples after adding noise: 358\n",
      "Number of features: 10000\n",
      "Starting K-Means clustering...\n",
      "Final number of clusters: 1\n",
      "Begin calculating NPD --------------\n",
      "shape of X (10000, 358)\n"
     ]
    }
   ],
   "source": [
    "# import cProfile\n",
    "import pstats    \n",
    "\n",
    "# dataset_prefixes =  ['N_BaIoT_dataloader.csv']\n",
    "dataset_prefixes =  [ 'BoTIoT', 'CICIoT2023','N_BaIoT', 'ToNIoT' ]\n",
    "\n",
    "sample_sizes = [10**3, 10**4, 10**5, 10**6]\n",
    "feature_sizes = [10, 100, 1000, 10000]\n",
    "\n",
    "setting = { # C means the number of cluster , Sc is scaler   \n",
    "    \"BoTIoT\": { \n",
    "        \"C\" : 1, \n",
    "        \"Sc\" : \"Normalizer\" \n",
    "    }, \n",
    "    \"CICIoT2023\": {\n",
    "        \"C\" : 1, \n",
    "        \"Sc\" : \"MinMaxScaler\" \n",
    "    }, \n",
    "    \"N_BaIoT\": {\n",
    "        \"C\": 1, \n",
    "        \"Sc\": \"MinMaxScaler\"\n",
    "    },\n",
    "    \"ToNIoT\": {\n",
    "        \"C\": 1, \n",
    "        \"Sc\": \"QuantileTransformer\" \n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "for n_samples in sample_sizes:\n",
    "    for n_features in feature_sizes:\n",
    "        print(f\"\\nGenerating synthetic data for {n_samples} samples and {n_features} features...\")\n",
    "\n",
    "        train_file = f'Datascaled/NoiseOCData/Train_Normalizer_data_BoTIoT.csv'\n",
    "        df_train = pd.read_csv(train_file).dropna()\n",
    "        \n",
    "        # Lấy tỷ lệ nhãn từ df_train\n",
    "        # label_ratio = df_train['label'].value_counts(normalize=True)\n",
    "        label_ratio = df_train.iloc[:, :-1].value_counts(normalize=True)\n",
    "        ratio_0 = label_ratio.get(0, 0.5)\n",
    "        ratio_1 = label_ratio.get(1, 0.5)\n",
    "\n",
    "        weights = [ratio_0, ratio_1]\n",
    "\n",
    "        # Sinh dữ liệu\n",
    "        X, y = make_classification(\n",
    "            n_samples=n_samples,\n",
    "            n_features=n_features,\n",
    "            n_informative=int(n_features * 0.6),  # phần đặc trưng thực sự\n",
    "            n_redundant=int(n_features * 0.1),\n",
    "            n_repeated=0,\n",
    "            n_classes=2,\n",
    "            weights=weights,\n",
    "            flip_y=0.01,\n",
    "            class_sep=1.0,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # df_synthetic = pd.DataFrame(X)\n",
    "        # df_synthetic['label'] = y\n",
    "\n",
    "        y = y.reshape(-1, 1)\n",
    "\n",
    "        df_synthetic = np.hstack((X, y))  # shape sẽ là (n_samples, n_features + 1)\n",
    "        df_train_new, df_test_new = train_test_split(df_synthetic, test_size=0.3, random_state=42)\n",
    "\n",
    "        print( type(df_train_new) ) \n",
    "        try:\n",
    "            function(df_train_new, df_test_new, \"Normalizer\" , noise=0, ncluster=1)\n",
    "            print(\"Done!\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed on this setting:\", e)\n",
    "            \n",
    "# for prefix in dataset_prefixes:\n",
    "#     print(f\"\\n{'-'*50}\\nRunning for {prefix}\\n{'-'*50}\")\n",
    "    \n",
    "#     scaler = Normalizer\n",
    "#     ncluster = 1\n",
    "    \n",
    "\n",
    "#     # Tải dữ liệu\n",
    "#     train_file = f'Datascaled/NoiseOCData/Train_{scaler}_data_{prefix}.csv'\n",
    "#     test_file = f'Datascaled/NoiseOCData/Test_{scaler}_data_{prefix}.csv'\n",
    "\n",
    "#     df_train = pd.read_csv(train_file).dropna()\n",
    "#     df_test = pd.read_csv(test_file).dropna()\n",
    "\n",
    "#     df_full = pd.concat([df_train, df_test], ignore_index=True)\n",
    "#     df_train_new, df_test_new = train_test_split(df_full, test_size=0.3, random_state=42)\n",
    "\n",
    "#     y_true, y_pred = function(df_train_new, df_test_new, scaler, noise=0, ncluster=ncluster)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
